Assessment of Students Use of Generative Artificial Intelligence: Prompting Strategies and Prompt Engineering in Chemistry Education

Abstract

The rapid integration of generative artificial intelligence (AI) into educational settings prompts an urgent examination of its efficacy and the strategies that students employ to harness its potential. This study focuses on preservice chemistry teachers use of generative AI for chemistry-specific problem-solving and task completion. We found that there is a prevalent reliance on copy-pasting tactics in initial prompting approaches, and students need guidance to improve their prompting abilities. By implementing the “Five S” prompting framework, we explore the evolution of student prompts and the resultant satisfaction with AI-generated responses. Our findings indicate that, while students initially struggle with the nuances of effective prompting, the adoption of structured frameworks significantly enhances their perceived quality of AI-generated answers. This research sheds light on the current state of AI use among students but also underscores the importance of targeted educational frameworks to refine AI interaction in academic contexts. In particular, we suggest critical engagement and methodological prompt engineering strategies to maximize the educational benefits of generative AI technologies

Keywords
what are keywords
General Public
Undergraduate/General
Internet/Web-Based Learning
Generative Artificial Intelligence

Introduction
The publics’ interest in generative artificial intelligence (AI) was sparked by the release of ChatGPT, a chatbot powered by a large language model. The terms AI and ChatGPT have grown more and more synonymous over the past year. There are various applications of AI bots in journalism and media education, (1) scientific applications like peer reviews, (2) drug discovery, (3) material design, (4) or even scientific discovery itself. (5) In education, initial research on using generative AI mainly revolved around strengths and weaknesses (6) and opportunities and challenges. (7,8) Scholars extensively tested what potential ChatGPT has in the educational context by evaluating answers given by the generative AI (9−14) and proposed a range of applications in several fields of education. (15−24) This same development was seen in chemistry education, where scientists evaluated academic answers given by AI, describing mixed performances by ChatGPT in terms of application of knowledge. (9,25) They compared AI-generated answers to student answers and found that the AI was sound in some areas but made mathematical mistakes that students would not make. (26) Further, they yielded a range of applications in chemistry education like writing lab reports and promoting metacognition by evaluation of AI-generated answers, highlighting the need for critical and reflective use of AI. (27−30) It is important to note that in the context of chemistry it has been shown that there are certain shortcomings. For example, mathematical answers are often incorrect, and hallucinations occur frequently. Furthermore, chemistry questions in multiple-choice and open format of a general chemistry exam were answered with low accuracy. (31,32) As for students, it was shown that AI tools are used by two-thirds of German university students, (33) but they have mediocre AI literacy. (34) There even are reports of unreflected acceptance of ChatGPT’s answers when solving physics tasks. (35) With broad usage among students, low accuracy of answers in the field of chemistry, and increasing public attention, it is of great interest to educate students on the reflective and informed use of generative AI.
A Lack of Research on Students Prompting Strategies
Among the most important skills to improve AI-generated answers is prompting (a prompt is the input given to a generative AI). There are several approaches, ranging from basic strategies like input–output (36−40) prompting to Chain of Thoughts (41,42) and Tree of Thoughts (43) prompting, all of which yield improved performances. There is an extensive overview of such strategies, including how and why they work. (44) However, there is little knowledge about what strategies students use in their generative AI interactions. Also, critical evaluation of the generated output is extremely important to overcome inaccuracies and hallucinations.
Prompt engineering is a rapidly developing complex field that will evolve over the next years. There are several simplifying models such as the Five S prompting framework which is specifically targeted at educators. (45) The framework guides prompters in their generative AI use and offers a concise and easy-to-remember set of principles. It could potentially be an appropriate tool for chemistry educators and teacher students. It uses similar principles proposed by Zheng et al., (46) who developed the ChemPrompt strategy. The ChemPrompt strategy reduces hallucination, implements detailed instruction, requests structured output, and promotes interactive prompt refinement, yielding remarkable results.
The Five S Prompting Framework
In this study, students use the “Five S Model” prompting framework (see Figure 1) for educators proposed by aiforeducation.io. (45) The framework uses five strategies, summarized as Five S’s, to refine prompts.
Figure 1

Figure 1. Five S prompting framework as adapted from aiforeducation.io was used in this study. Text in italics marks where prompts specifically fulfill their corresponding “S”.

“Set the Scene” improves the answer due to the context sensitivity of large language models. (44) “Be specific” is similar to what Zheng et al. propose as “Detailed Instruction”. (46) “Simplify your language” is sometimes contradicting specificity, but a consequence of how large language models work with natural language. (44) “Structure the output” again is a principle found with Zheng et al. (46) “Share Feedback” is a further strategy developed due to the high dependency of large language models on the data provided to it. While this prompting framework is sometimes contradictory and, in some ways, fuzzy, it might serve as a facilitator for reflecting on students’ own prompts. Overall, it gives a preliminary, easy-to-remember guideline on prompting principles that is based on how large language models work.
Aim of This Study
In this publication, the aim is to investigate the use of text-generative AI by undergraduate students in a chemistry context, more specifically its use to answer chemistry-related questions and to work on specific chemistry tasks. The study is designed to give insight into naïve prompting strategies. Further, we want to compare students’ satisfaction with answers generated with those preliminary strategies to more advanced prompting. Specifically, the Five S prompting framework for educators, (45) a (mostly) zero-shot input–output prompting strategy, is used. The satisfaction of students with answers generated using this prompting framework is assessed and compared to their initial prompting. This should give first insight into prompting strategies. Also, the usefulness of prompting frameworks and the students’ use of text-generative AI are described in this context. The work does not aim to verify whether the answers given by AI were correct, as this has been considered many times, (31,32,47) and it is expected that students will tend to rate the answers to be more correct than experts would. (35)
Altogether, the main aim of this study is to develop a deeper understanding of prompting skills and to investigate how to implement the Five S framework to develop those skills. With this knowledge, we want to help make possible more informed teaching of prompting skills and reflective prompting.
Setting and Methodology
This study is set in a university course “digital media for chemistry classes” for preservice chemistry teachers. The course is held as part of the curriculum in the seventh semester of an 8-semester bachelors’ degree. The syllabus of the course defines various learning outcomes, including the use and discussion of current media, and it was therefore suitable for incorporation of generative AI as a topic. In the winter semester of 2023, the course was divided into 2 parts: a general part focusing on the use of digital media in chemistry classes (4 units) and a part focusing on the use of artificial intelligence in chemistry education (4 units; see Figure 2). In this study, the findings of the second unit of this second part will be presented. For the duration of the whole course, students were given access to text-generative AI via the OpenAI-API (ChatGPT v3.5 and ChatGPT v4.0) using a discord bot acting as ChatGPT. In this study, students used only ChatGPT v3.5 (November 2023 model).
Figure 2

Figure 2. Overview of the second part of the university course. Duration of all units was 90 min. In this study, the focus is on the findings of research on the second unit of the generative AI part of the course.

Survey Method
The students (n = 23) in the course described above were surveyed for this study. Participation in the study was entirely voluntary, and it was made clear that participation in research would not affect grading; all participants approved informed consent and agreed to the use of their data for scientific purposes. In a presurvey, they described their knowledge on generative AI and its use as low to basic (see Supporting Information p. 2). In this course, they got an introduction to large language models and generative AI. Then, they did some very basic exercises using ChatGPT and got an introduction to the Five S prompting framework. To investigate their prompting strategies after these first instructions and to learn about their use of text-generative AI in a chemistry context, they were asked to work on chemistry-specific questions at three different educational levels: lower secondary level, upper secondary level, and tertiary (university) level. At lower and upper secondary level, students were asked to choose their question from a range of preselected questions from schoolbooks. It was decided to leave freedom of task choice to maybe trigger a larger variety of prompting strategies (for examples, see Supporting Information p. 3). At the university level, students had to use ChatGPT to answer an exam question taken from a bachelor’s organic chemistry exam. This decision was made to compare different prompting strategies employed to solve a specific problem. For each level, the students were first asked to specify whether they could answer the question without the use of generative AI. Then, they used ChatGPT to answer the question. They copied both their prompt and the answer to ChatGPT into a work log sheet. They rated the answer in 5 dimensions (completeness, easy to understand, ChatGPT’s understanding of chemistry, chemical correctness, and overall satisfaction) on a five-level Likert scale and gave reasons for their ratings. Then, they rated their prompts using the Five S prompting framework for educators and were asked to improve their prompt. Thereafter, they again rated the answer using the same dimensions and gave reasons for their ratings. For full insight into the log sheets, see Supporting Information p. 4.
Analysis and Interpretation
From the 23 students who answered one question per academic level, 69 log sheets were returned for data analysis. In 67 cases, data was complete; therefore, 2 log sheets were dismissed from the data set. The log sheets were analyzed using MaxQDA for evaluative qualitative content analysis, following the method of Kuckartz. (48) The categories for evaluation of the prompt were derived deductively from the Five S prompting framework. For each prompt and each of the five “S”, it was evaluated whether we found a high or low level of that “S” in the prompt. For example, prompts using “You are an undergraduate chemistry student” were categorized to a high level of the first “S”. A question where no context or role is given would be categorized to a low level of this first “S” but might be appropriate for a high level of any other “S”. The author and a second rater independently rated about one-third of the material (8 of 23 students). Interrater reliability was found to be satisfactory at κ = 0.68. The author and second rater discussed coding differences and further specified the coding scheme. In the revised coding scheme, a good inter-rater reliability of κ = 0.82 was yielded.
Furthermore, the Likert-scale ratings of the 5 dimensions (completeness, ease of understanding, understanding of chemistry, chemical correctness, and overall satisfaction) were compared for the initial prompt and the adjusted prompt using t tests. An ANOVA with Bonferroni post hoc correction was calculated to decide any differences in rating depending on the educational level of the question. Adjusted prompts were also rated by evaluative qualitative content analysis following Kuckartz. (48) For the section where students reflected on their own prompts using the Five S framework, we categorized their reflection for each prompt into whether they themselves thought they achieved a high level for each of the five “S”.
Results and Discussion
Students first had to indicate whether they could answer the question asked without the help of ChatGPT. Overall, half of the students fully agreed that they would not need ChatGPT to answer the question, 24% tended to agree, 22% indicated neither agreement nor disagreement, and 4% tended to disagree, with less agreement to the statement for university level (for further insight, see Table S1, Supporting Information p. 7). This result indicates that students are sufficiently competent to evaluate the answers given by generative AI, particularly in terms of correctness and chemical understanding. Furthermore, it is important to state that, at the point of data collection, students had one previous instruction lesson. There, they had at least their very first contact with ChatGPT and were, in short, taught about prompting and the Five S framework. As indicated before, most students reported that they did not use ChatGPT at all or had only a few contacts with it. Therefore, the following findings describe mostly novices in generative AI use.
First Prompting Often Involves Copy-Pasting Questions
As a first step toward understanding students’ use of ChatGPT in the context of chemistry tasks and questions, it is interesting to investigate the strategies they employ to solve a chemistry problem. In this study, to rule out that students copy-paste questions just out of convenience, questions were provided on paper for the secondary level and electronically for the university level. There was no noticeable difference in the results that we found between the levels. Students resorted to copy-pasting (or typing) the question into ChatGPT in approximately 60% of cases as a first prompting strategy (see Figure 3), which is comparable to other reports on problem-solving using ChatGPT. (35) It is hardly surprising that almost none of these questions and tasks are designed prompts for use with generative AI. Therefore, we only sporadically found prompts adhering to the Five S prompting framework. While chemistry questions sometimes give background, they hardly set the scene for AI. Also, a structured output is usually not specified in chemistry questions, and therefore, copy-pasted prompts lack structure. Nevertheless, we rarely found specificity and simplicity in the language. This was more expected; however, it was interesting to note that the questions and tasks designed to be specific for students often were not specific in a generative AI sense. For example, asking ChatGPT “In what way is water different from other liquids?” (log sheet 8) is not a specific prompt for the AI but is a specific question for a student with the page heading “anomaly of water”. Overall, we found that approximately 15% of copy-pasted questions were specific in a generative AI sense and that about 10% were simplified language. This is mostly due to the lower secondary school books being written in a more conversational and easy-to-understand style, as the university-level question is asked in a formal, academic style, and copy-pasting this question would therefore not result in a simple prompt.
Figure 3

Figure 3. Overview of the share of prompting strategies found for the first attempts at prompting. Copy-pasting is the most prevalent strategy.

First Prompts Sometimes Embed Copy-Pasted Questions with Context
A further 30% of prompts we analyzed copy-paste the question or parts of the question into ChatGPT but embed it in further context (see Figure 3). For example, to “Set the scene”, students added information on the topic of the question (e.g., “In a chemistry class on lipids, this question is asked...”, log sheet 34). They also added knowledge level (e.g., “answer the question at university level...”, log sheet 56) or gave the AI a specific role in a prompting approach (e.g., “answer as a chemistry professor”, log sheet 48). “Set the scene” prompting was most frequently found in those embedded-question prompts. About two-thirds of the embedded approaches set the scene for the generative AI. Further 40% specified the question (e.g., “answer this question but make it easy to understand”, log sheet 30), with just 10% simplifying the question (e.g., splitting the prompt for a longer question, log sheets 51, 55, 60, and others) and 10% requesting a specific structure (e.g., “use a reaction scheme where it enhances understanding”, log sheet 57). Comparing to copy-pasted prompts, we can see there is an increased level of Five S prompting strategies employed here, with setting the scene being a standout strategy.
First Prompts Rarely Reformulate the Question
While a majority of students’ initial use of ChatGPT for problem solving relies on replicating the exact wording of the question or adding additional context, a further 10% of initial prompts completely reformulated the question in their own words (see Figure 3). This category of prompts mostly consisted of students simplifying the question, often moving to more simple approaches using wh- or how-questions (e.g., “Why is silicon so expensive even though it is abundant?”, log sheet 12). Sometimes, students made their prompt less specific by reformulating (e.g., “What makes up hydrocarbons?”, log sheet 5; original question: “Hydrocarbons consist of two major elements. Please name both”) and leaving out details, resulting in oversimplification. Overall, Five S strategies were employed at a similar frequency to embedded prompts, with higher simplicity being the only difference.
Students’ Verdict on Their Own Prompts
Copy-pasting is observed even though students had known the Five S model previously and answered three questions in the log sheets, with no significant change in frequency of copy-pasting for their first, second, and third question. To explain this observation, a look into students’ own evaluation of their prompts might give an insight. They often acknowledge that their prompt does not use a context or scene and does not ask for a specific structure. This is something we found as well when we rated their prompts, although students felt that their prompts employed more of the Five S strategies than we did. However, students thought their own prompts were vastly more specific and simplified in language than we rated the prompts. We mentioned above that only 15% of copy-pasted prompts were specific and 10% used simple language. Students vastly overestimated this point, with 75% thinking of book questions as specific and 75% feeling the question was simplified. This indicates that students in their early stages of using AI might need further instruction on what exactly making prompts specific and simple means in a generative AI way. For example, earlier, we discussed that “In what way is water different from other liquids?” (log sheet 8) is not specific in a generative AI way, and the student reflects on this: “If I wanted it [ChatGPT] to answer I should have specifically asked for the anomaly of water” (log sheet 8). Their altered, reflective prompt was more specific and increased their satisfaction with the answer in all dimensions. In general, we found that students often argued that their copied prompt was specific “because it contained all information necessary” or “I asked for a concrete answer”. This further shows the disparity in what students seem to understand as specific and what specificity in a generative AI way means. The same can be observed for simplicity, where students argue that their prompt is “short” and “not overly complicated”, but simplicity in a generative AI sense means a conversational approach using minimal technical language.
In embedded prompts, there was less mismatch in our ratings of specificity and the students’ own assessment of their specificity. However, students overestimated their prompts in simplicity (50% of prompts) against what we found (20% of prompts). We believe that the differences discussed here cannot be attributed to a miscommunication of what the Five S framework wants to achieve. The framework as well as examples of how to specifically implement each “S” were available to the students throughout the whole session. It more so seems that the wording used in the framework itself is ambiguous and difficult to interpret for students.
Students Often Improve Their Prompts by Setting the Scene and Being More Specific
With these findings in mind, it is also interesting to investigate the strategies students employ to improve their prompts using the Five S framework. Students commonly acknowledged a lack of scene-setting, output structure, and interestingly specificity, even when they thought their prompts were specific. Two main strategies for improvement were observed: students either concentrated on one “S” (about 50% of cases) or tried to improve two of the “S” (other half of cases). Where two “S” were undertaken, often specificity and scene-setting or specificity and structure were both improved. A very common strategy to set the scene was including a level of education, for example, adding “please answer the question as an eighth-grade teacher” (log sheet 4) or “you are an intelligent university chemistry student and answer an exam question” (log sheet 61). This is in line with what was found for the first prompts that already employed the Five S framework. To structure the output, basic strategies included requesting a table or reaction scheme as part of the output. Specificity was increased by adding context to what is asked or restricting the answer to a specific topic. In other words, there was no obvious difference in Five S strategies for the first prompts or reflective prompt improvements.
Overall, 85% of the reflective prompts improved at least in one of the Five S categories (see Figure 4), while 15% of reflective prompts failed to implement at least one “S”. This was mostly the case when students aimed to be specific or simplify their language, again pointing to challenges with this point. Surprisingly, only 15% of the improved prompts built on the existing prompt and were rated “Share feedback” prompts. Examples for how students used this strategy are adding missing context (“please include the same facts for sulfur”, log sheet 2), revising parts of the answer (“please give me the same answer, but make the assignment only once”, log sheet 17), or specifically requesting more chemistry-related reasoning (“Now give me the oxidation numbers as they would normally be given”, log sheet 46). Only once a student corrected ChatGPT (“I have a source saying there are 6 oxidation levels, but you say there are 4. Which statement is legit?”, log sheet 47). This is especially interesting, as students specifically remarked that correctness was low for some cases but did not correct ChatGPT by sharing feedback. All of these findings indicate that students might find it easier to ask more specifically by adding context. They also apply setting the scene by indicating the academic level and the expected level of explanation as well as structuring the output by requesting specific parts in the answer. Simplification of prompts in a conversational approach was less abundant. Altogether, this shows that, to further improve prompting, there might be an additional need for specific learning opportunities focusing on the aspects of simplicity and feedback.
Figure 4

Figure 4. Overview of the authors’ ratings of first and improved prompts. To improve prompts, the most common strategies were to “set the scene” and “be specific”. *: “Share feedback” prompting can only be present in improved prompts, as first prompts have no context to evaluate.

Students Rating of ChatGPT’s Answers to Chemistry Questions
Students were asked to rate the answers given by ChatGPT on a five-level Likert-scale. There was no statistically significant difference in students ratings of completeness of the answer (F(2, 64) = 0.71, p = 0.494), perceived chemistry understanding of ChatGPT (F(2, 64) = 1.720, p = 0.187), and overall satisfaction with the answer (F(2, 64) = 2.00, p = 0.144) for different educational levels as determined by an ANOVA. Overall, students tended to agree that questions were answered completely (M = 4.31, SD = 0.988). They moderately tended to agree that ChatGPT understands chemistry based on the answers they got (M = 3.46, SD = 1.341) and moderately tended to agree to be satisfied with those answers (M = 3.37, SD = 1.191). The perceived easiness to understand the answer differed statistically significantly for different educational levels (F(2, 64) = 3.12, p = 0.050), as well as the perceived correctness of the answer (F(2, 64) = 9.06, p < 0.001). We found that perceived easiness to understand the answer given decreased from lower secondary level (M = 4.48, SD = 0.790) to upper secondary level (M = 4.09, SD = 1.311) to university level (M = 3.67, SD = 1.065). This same observation was made for perceived correctness, with a decrease from higher values in lower secondary level (M = 4.26, SD = 1.010) to upper secondary level (M = 3.43, SD = 1.343) to university level (M = 2.67, SD = 1.354). The last result is the only rating below the middle of the scale. From this data, it can be concluded that ChatGPT was satisfactory at providing students complete answers but lacks in perceived correctness.
This further highlights the need of critical evaluation of chemistry-related answers generated by ChatGPT. This was already pointed out in several studies evaluating the capabilities of ChatGPT in this context. (31,32) It is also interesting that perceived correctness actually decreases for higher levels of education. This may be in correlation with ChatGPT v3.5s limited chemistry capabilities. It must be noted that GPT v4.0 was not used here but is generally expected to give much more accurate answers across many fields. The main reasons given for a low rating in perceived correctness were hallucination (“One answer is correct, but it [ChatGPT] makes up facts for the second one”, log sheet 62) and lack of technical language (“It [ChatGPT] uses colloquial expressions, making the answer not accurate”, log sheet 48) as well as failure to complete chemistry-specific tasks (“For oxidation numbers, my colleagues got different answers than me [for the same substance]”, log sheet 59). Positive ratings of correctness were often reasoned as recitation of simple facts (“yes, these are facts from the periodic table”, log sheet 42) and satisfactory results for the educational level (“I would be delighted if I got this answer from a student”, log sheet 9).
More Reflective Prompts Improve Students’ Perception of AI-Generated Answers
Gaining an insight into students’ prompting strategies and their perception of AI-generated answers is of great interest to develop an understanding of their use of generative AI. However, we also want to understand whether we can influence their prompting strategies and maybe even improve their perception of the answers generated by AI. To show how reflective prompting, using the Five S framework for educators, influences students’ perception of these answers, a paired sample t test was conducted for all five dimensions that students rated in their work log sheets. Here, we report that for all five dimensions there has been a significant increase. This was observed in perceived completeness of the answer, easiness to understand the answer, chemistry understanding of ChatGPT, chemical correctness, and overall satisfaction with the answer (see Table 1).
Table 1. Mean Values and Standard Deviations for the 5-Level Likert-Scale Ranking of AI-Generated Answers as Perceived by the Studentsa
 	first prompt	reflective prompt	 	 	 
 	M	SD	M	SD	t (66)	p	Cohen’s d
Completeness	4.31	0.988	4.63	0.756	–2.484	0.016	1.033
Easiness to understand	4.09	1.111	4.49	0.911	–3.032	0.003	1.088
Chemistry understanding of ChatGPT	3.46	1.341	3.78	1.191	–2.416	0.018	1.062
Chemical correctness	3.48	1.386	3.75	1.295	–2.248	0.028	0.978
Overall satisfaction	3.37	1.191	3.72	1.216	–2.366	0.021	1.188
a1 = completely disagree, 5 = completely agree; see log sheets in the SI. There is an increase for all mean values for each dimension. For a paired sample t-test, t-values, p-values, and Cohen’s d are given.
This overall shows that the five S prompting framework as a means for prompt engineering and reflection of their own prompts might be suitable to increase students perceived overall “quality” of answers given by generative AI. However, with the insights we gained it is of particular importance to note that students overestimate the specificity and simplicity of prompts, showing a need of further clarification and learning opportunities to develop their prompting skills.
Limitations
Although the results found in this study give a compelling insight into students’ prompting strategies and gather concrete insight into where they struggle in AI use, there are limitations in what can be deduced here. First, prompting is a highly empirical field, and it is not well-known why exactly some strategies work. Sometimes, what works with one large language model might not be suitable for the next one or even for a newer version of the same model. Second, the sample size is small, this reduces generalizations that can be made, but instead, I aimed to give an insight into some individual strategies. Third, students were asked to fill the log sheets with first and improved reflective prompts, but there is no way to control whether they generated more output, further tweaking and improving prompts repeatedly and only reporting their last prompt as an improved prompt. Thus, some improved prompts might be the product of iterative improvement and reflection.
Teaching Implications
With the results presented in this publication, it is obvious that teaching must adapt to address the emergence of generative AI. We investigated naïve strategies that students use when applying generative AI. We saw a great risk of unreflected prompting or copy-pasting of textbook tasks. Further, there is a risk of unreflective acceptance of the answers given by AI. To mitigate these risks, we suggest providing students with explicit learning opportunities on prompting and the reflective use of generative AI. The Five S model seems to be a suitable framework to help students reflect on their prompting. Its use improves the perceived quality of the answers. The framework itself is not limited to use by chemistry teachers and might be beneficial to all graduating chemists and possibly other science curricula graduates. However, when implementing the framework, it is crucial for the instructor to thoroughly reflect on its integration, as we have pointed to difficulties in understanding some of the terminology of the framework. We suggest providing additional learning opportunities or scaffolds for the terms “simplicity” and “specificity” to make clear what those terms mean in a generative AI sense. For example, one could provide information about how generative AI is context-dependent and uses natural language. To add to that, we observed a relationship between self-reported content knowledge and prompting skills. Students that reported higher ability to solve a task were less likely to uncritically use the AI and more often successfully employed Five S strategies. One possible implication is to teach prompting skills when content knowledge levels are higher, for example, near the end or after a teaching sequence. This way, students are equipped with the knowledge necessary to concentrate on prompting and critical evaluation and are provided with explicit learning opportunities that possibly improve AI literacy.
Conclusion
In this study, chemistry teacher students’ use of generative AI was investigated. It was found that, when given the task to use ChatGPT to answer chemistry related questions, students largely employed a very simple prompting strategy, copy-and-pasting the question in most cases. Adding a few additional pieces of information was found in some cases, while individual approaches were rare. Students identified lack of context and structure in their prompts but thought they met specificity and simplicity requirements when they did not manage to reach them. This may be because these terms have a specific meaning in a generative AI sense. When improving their prompts, students added context by setting the scene, structuring output, and being more specific. Providing feedback and simplifying prompts were rarely employed. It was also shown that students generally tend to agree that they were satisfied with ChatGPT’s answers. However, they pointed out a decreased ease of understanding ChatGPT’s answers and a decreased level of correctness for answers specifically in higher chemistry education. Overall, with prompts improved by prompting strategies, students perceived satisfaction with answers significantly increased in all rating categories.
Considering that students seem to initially lack prompting strategies but can improve them when given a guideline like the Five S prompting framework, using such frameworks and strategies might be an important way to improve student generative AI use. It still needs to be discussed how such a framework can be implemented in education settings, as there seem to be some difficulties with terms used in the Five S framework. However, using prompting strategies can significantly improve students’ satisfaction with AI generated answers for chemistry questions.