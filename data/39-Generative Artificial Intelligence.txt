Generative Artificial Intelligence in Education: From  Deceptive to Disruptive

Abstract  Generative Artificial Intelligence (GenAI) has emerged as a promising technology that can create original content, such as text, images, and sound. The use of GenAI in educational settings is becoming increasingly popular and offers a range of opportunities and challenges. This special issue explores the management and integration of GenAI in educational settings, including the ethical considerations, best practices, and opportunities. The potential of GenAI in education is vast. By using algorithms and data, GenAI can create original content that can be used to augment traditional teaching methods, creating a more interactive and personalized learning experience. In addition, GenAI can be utilized as an assessment tool and for providing feedback to students using generated content. For instance, it can be used to create custom quizzes, generate essay prompts, or even grade essays. The use of GenAI as an assessment tool can reduce the workload of teachers and help students receive prompt feedback on their work. Incorporating GenAI in educational settings also poses challenges related to academic integrity. With availability of GenAI models, students can use them to study or complete their homework assignments, which can raise concerns about the authenticity and authorship of the delivered work. Therefore, it is important to ensure that academic standards are maintained, and the originality of the student's work is preserved. This issue highlights the need for implementing ethical practices in the use of GenAI models and ensuring that the technology is used to support and not replace the  student's learning experience.

I. Large Language Models Take Artificial Intelligence From Deceptive to Disruptive  T  ECHNOLOGY has evolved rapidly in the last few years, affecting many areas, including education. The launch of ChatGPT on November 30, 2022, was a key event in the history of the Artificial Intelligence (AI). For the first time, a technology labeled AI went mainstream, becoming the fastest-growing consumer product of all time, getting 1 million users in just five days (See Fig. 1) and reaching 100 million active users in less than two months (Fig. 2) [1]. The new chatbot has had a deep cultural impact, bringing the rapidly advancing field of AI and its societal impacts to the forefront of public attention.  The model of the 6Ds of digitized technologies introduced by Peter Diamandis and Steven Kotler [2] showcases the significance of the Generative AI (GenAI) [3] moment in 2023. The 6D model states that when something is digitized, it goes through six phases:  1. Digitized. A resource, a technology, a process, or a social or economic activity becomes digital, it will evolve at an exponential  pace following the pace of improvement described by Moore’s Law [4] and other exponential behaviors observed in digital technologies (computing, memory, digital storage, bandwidth, etc.).  2. Deceptive. In the first stages, the digitized version will be inferior to the old analog version, and its evolution will be deceptively slower than the linear, steady improvements of analog alternatives. A classic example is digital photography, invented in the 70s by Kodak, which was inferior to chemical film for over 30 years.  3. Disruptive. The exponential curve of growth kicks and the technological improvement mimics a hockey stick curve. The digitized version becomes disruptive, deeming the previous technology obsolete in a very short period of time. The following phases are observed after the disruption.  4. Demonetization. Marginal costs tend to be zero. Taking one more digital picture is close to zero, just like doing a web search, watching an online video, or making a social media post. Kodak went bankrupt in 2010, the same year that Instagram was acquired by Facebook (now Meta) for an unprecedented sum. Instagram’s business model relied on the zero marginal cost of taking and posting a picture online.  5. Democratization. Access to the digitized version becomes universal. While the paper volumes of an encyclopedia were expensive and took significant physical space, Wikipedia is open 
to anyone with access to an online device, which makes it more valuable to own such devices.  6. Dematerialization. The analog artifact is no longer required, just like the traditional encyclopedia and photo albums are prescinding, and we can take back the space they occupied on our shelves.  Leveraging foundational technologies like neural networks [5], deep learning [6], transformers [7], and quantum technologies [8], Large Language Models (LLMs) [9], [10] like ChatGPT are navigating the trajectory outlined in the 6D model. Starting from an understated impact, these models are transitioning to a phase of significant disruption. This ongoing shift is evident in their rapid adoption and increasing prominence across diverse sectors, indicating a growing and extensive impact on the economy and culture.  If the exponential trends in AI continue, we can anticipate significant performance and changes in emergent functionalities. A notable trend is the rise of Large Multimodal Models (LMMs) or Multimodal Large Language Models (MLLMs) [11], such as GPT-4V [12], trained on diverse data types like text, images, sound, video, and infrared images. These LMMs exhibit surprising capabilities such as creating stories from images or performing OCR-free math reasoning [13], suggesting a potential path to Artificial General Intelligence (AGI) [14].  The year 2023 has been remarkable for LLMs, with exponential or sigmoid growth in various dimensions: enhanced capabilities, increased model sizes, new models and projects, heightened investment, and public attention. Nevertheless, it has also led to one of the quickest government reactions to a new technology. The U.S. Federal Elections Commission is investigating misleading political ads, and Congress demands more oversight on how AI firms manage and identify their training data. Likewise, the European Union has updated its AI Act to address GenAI [15]. Additionally, it has emphasized the ethical aspects of Information and Communication Technologies (ICTs) and AI [16]. Significant debates have arisen over the impact of AI technologies on society and the job market, as well as philosophical discussions about the potential catastrophic consequences of AGI and Superintelligent Artificial Intelligence (SIA) [17] for humanity.  A. Discovering the Emergent Abilities of Large Language Models  It is important to consider that ChatGPT is just the tip of the iceberg in the innovations emerging from the GenAI sector, a field heavily reliant on transformer models [7] and diffusion techniques [18]. ChatGPT is a chatbot based on an adaptation of the GPT-3 LLM (specifically GPT-3.5-Turbo) [19] (with a 175 billion-parameter architecture capable of handling a context window of 4,096 tokens, about 2,500 words) and, on its enhanced version, the GPT-4 model [20] (with a context window of 32K tokens). Information about GPT 4.0 has not been opened to the community. It is estimated to be a model of  about 1.8 trillion parameters organized as a MoE (Mixture of Experts), with 16 experts of 111 billion parameters, plus the trunk part of 55 billion parameters, activating only two experts for each inference (280 billion parameters) [21].  LLMs are enormous neural network systems based on the transformer architecture [7], introduced by 2017 DeepMind, a company acquired by Alphabet (Google) in 2017. Since then, transformers have become the go-to architecture in AI research, serving as a kind of lingua franca among AI research subfields. Previously, these subfields had diverged so much in their theoretical approaches that innovations in one area rarely permeated others [22].  Creating an LLM involves several key steps:  1. Model architecture. This is the code and mathematical framework of the model. Most top-performing LLMs currently use variations of the “decoder-only” transformer architecture.  2. Training dataset. This includes all the examples and documents on which the model is trained, shaping its learned patterns. The content typically consists of text in natural or programming languages or structured data like tables or equations.  3. Tokenizer. It converts the text from the training dataset into numerical values, as models require numbers for processing. Text is transformed into tokens (words, sub-words, or characters) based on the tokenization method. The size of a dataset is often measured by the number of these tokens, which can range from hundreds of billions to several trillion.  4. Training hyperparameters. These define the specifics of the training process, including the rate of parameter adjustments and model updates.  5. Computing power and human oversight. Adequate computing resources and skilled personnel are essential for running and monitoring the training process. The training involves setting up the architecture on hardware and running the training algorithm with the chosen hyperparameters, resulting in a set of learned model weights.  6. Post-training. LLMs can be specialized or adapted for specific tasks through fine-tuning. This involves additional training on a more specialized dataset, optimizing the model for particular applications. Though costly in terms of computing power, this step is generally less expensive than training a model from scratch. High-quality open-source pre-trained models are valuable in this context, as they allow for community-driven development and application, even with limited computing resources [23].  Time Taken by Online Services to Reach 1 Million Users  Time to Reach 1 Million Users (Days)  ChatGPT  Spotify  Dropbox  Facebook  Twi er  Net lix  Airbnb  Kickstarter  Instagram  Foursquare  5  150  210  300  730  1277  912  912  75  390  0 200 400 600 800 1000 1200  Fig. 2. Time taken by Online Services to reach 1 million users. Source: Own production adapted from https://nerdynav.com/chatgpt-statistics/  ChatGPT User Growth Over Time  Number of Users  5 days of launch January 2023 April 2023 August 2023  1 Million  100 Million  0.00  0.25  0.50  1.00  1.50  1.75  1e8

Fig. 1. Number of users of ChatGPT during the first nine months. Source: Own production adapted from https://nerdynav.com/chatgpt-statistics/

Recent developments show that as large pre-trained models grow in size with billions of parameters, they reveal unique properties [24]. In particular, it seemed that models going above specific size thresholds jumped in capabilities, two concepts that were dubbed “emergent abilities” and “scaling laws.”  For instance, OpenAI’s GPT models display this evolution. The original GPT could manage basic text labeling but lacked coherence in text generation [25]. GPT-2 improved, offering higher-quality text and some instruction-following capabilities [26]. GPT-3, however, emerged as a versatile and practical LLM for various language tasks. The significant capability leap between these models is mainly due to scaling up computational power and data: GPT-3 required about 20,000 times more computation than the original GPT [27]. Although these models share similar designs, their advancements are largely attributed to breakthroughs in high-performance computing infrastructure rather than specific advancements in language technology model design.  As they scale up, LLMs exhibit new properties that their developers had not anticipated, and we are only now starting to discover them. Among these properties, few-shot learning and chain-of-thought reasoning stand out.  • Few-shot learning enables a sufficiently large LLM to quickly grasp new tasks from just a few examples in a single interaction [26].  • Chain-of-thought reasoning allows the model to articulate its thought process when tackling complex tasks, similar to how the students would explain their reasoning during a math test, thereby enhancing their performance [28].  These GPT-3 capabilities, particularly in few-shot learning and chain-of-thought reasoning, were identified post-training and several months after its widespread public deployment, respectively [29]-[32].  In hindsight, these characteristics are partly the consequence of the LLMs’ ability to “learn” from the information within the context of execution -all the information received from user messages and that the model has generated- like the training and fine-tuning data.  Furthermore, LLMs demonstrate unforeseen skills in programming, arithmetic, correcting misconceptions, and answering exam questions across various domains, and improve as the model size scales up [30], [33].  There is a common belief that LLMs are merely statistical predictors of the next word, limited to text-based learning and reasoning. However, recent evidence suggests that LLMs are developing internal representations of the world, enabling them to reason abstractly beyond the specific linguistic structure of texts [24]. Although this ability is currently limited and inconsistent, it is most evident in larger and newer models, indicating that it could strengthen with further scaling of these systems. Key findings supporting this include:  • LLMs’ internal representations of color words align closely with human color perception [34].  • They can infer authors’ knowledge or beliefs from a document and predict its continuation [35].  • LLMs internally represent properties and locations of objects in stories, evolving as new information is presented. This includes representing spatial layouts in story settings and real-world geography and providing instructions for drawing novel objects.  • LLMs develop internal representations of the game board’s state when trained on board games using descriptions of moves [36].  • LLMs can differentiate between misconceptions and facts, showing calibrated internal representations of truth likelihood [30].  • LLMs pass tests designed for common-sense reasoning, including  those like the Winograd Schema Challenge [37], which lack textual clues for answers.  These findings indicate a growing ability of LLMs to develop complex, abstract internal models that extend beyond simple text processing.  B. Size Matters: Openness Is the Key  In the previous section, we discussed the matter of size in LLMs. Models above specific size thresholds seemed to jump in “emergent abilities” according to certain “scaling laws.” However, in March 2022, DeepMind released a paper exploring the ideal balance between tokens and model parameters within a set compute budget for LLM training [38]. The study suggests that smaller models with significantly more data are more effective for an average budget. For instance, the Chinchilla model, which is not open source, had 70B parameters (a third the size of larger models) but was trained on 1.4T tokens of data (3 to 4 times more). This approach led to comparable or better performance than larger models, both open and closed source.  According to Clémentine Fourrier [23], “this paradigm shift, while probably already known in a closed lab, took the open science community by storm.” In 2023, we witnessed a wave of open-source releases of pre-trained LLMS released almost daily. Noteworthy releases LLaMA (by Meta) in February, Pythia (by Eleuther AI) in April, MPT (by MosaicML) in May, X-GEN (by Salesforce) and Falcon (by TIIUAE) in June, Llama 2 (by Meta) in July. Qwen (by Alibaba) and Mistral (by Mistral AI) in September, Yi (by 01-ai) in November, DeciLM (by Deci), Phi-2, and SOLAR (by Upstage) in December.  These models, with parameters ranging between 3B and 70B, have quickly gained adoption for their performance and varying open-source licenses. Most models incorporate decoder transformer architecture with modifications and varying attention functions. While performance and inference speeds differ, the primary distinctions among these publicly released architectures are their training data and licensing.  These releases of open-source LLMs, along with other notable open-source AI models in image processing like Stability.ai’s Stable Diffusion, and audio processing models, such as OpenAI’s speechto-text model “Whisper,” have sparked great excitement among the developer community worldwide. Throughout 2023, we have witnessed a surge in the number of software projects related to generative AI (Fig. 3).  0  10000  20000  30000  Number of Projects  40000  50000  60000  2010 2012 2014 2016 Year  2018 2020 2022  The global growth in generative AI projects on GitHub  Fig. 3. Global growth in Generative AI software projects. Source: Own production adapted from https://d66z.short.gy/3f10bE  Scores, perhaps hundreds of thousands of independent software developers, researchers, and entrepreneurs worldwide, have begun experimenting with these technologies. Whether working with open models or developing against the OpenAI’s Application Programming Interface (API) and other proprietary LLM providers like Google or Anthropic, this vibrant activity leads to experimentation in new use cases, applications, and technologies based on and complementary to AI models.  In early 2023, a group of Stanford students utilized OpenAI’s text_davinci-003 API to generate a fine-tuning dataset, leading to the development of Alpaca 7B [39]. This model, fine-tuned from the LLaMA 7B model [40], is designed to follow instructions based on 52K demonstrations. Alpaca exhibits similar capabilities to OpenAI’s text-davinci-003 but is notably smaller and more cost-effective to reproduce, with an estimated cost of under $600 [41].  An internal Google document, leaked in spring 2023 (https://d66z. short.gy/u7blNr), reveals insights on the competitive landscape of AI. It suggests that open-source AI is outpacing giants like Google and OpenAI, particularly in the realms of LLMs. This shift is attributed to the speed, customization, privacy, and capabilities of open-source models, even with fewer resources. The document highlights opensource models achieving remarkable feats with significantly lower budgets, challenging the traditional approach of building giant, costly models.  At the time of this writing (early 2024), the best-performing published LLM, according to the Aena ELO Rating [42], is OpenAi’s GPT-4-Turbo-1106. However, in the top, we find two open-sourced LLMs: Mixtral-8x7b-instruct from the French firm Mistral AI and Tulu-2-DPO-70B from Paul Allen’s AllenAI (https://d66z.short.gy/ A5XMno). While the final draft of this paper is being written, new models such as Gemini Pro 1.5 with 1M tokens of context [43] are being introduced in the leaderboard, still without surpassing the latest GPT-4 on overall performance.  However, just days before the publication of this special issue, the project LoRA [44] (https://d66z.short.gy/pKHBNG) has released a specialized set of fine-tuned versions of Mistral 7b, an open-source LLM by the French company Mistral AI, that can be run on a medium spec laptop, where each specialized small LoRA LLM outperforms GPT-4 significantly on a specific benchmark [45].  II. GenAI and Education  A. Towards the Young Lady’s Illustrated Primer  In Neal Stephenson’s science fiction novel “The Diamond Age” [46], one of the central pieces of educational technology is the “Young Lady’s Illustrated Primer.” This device is a highly advanced, interactive book that uses AI to tailor educational content and tutoring to the individual learner. Designed initially for an elite clientele, the Primer adapts to its users’ interests, learning pace, and developmental needs, providing personalized education. The story plot places the Illustrated Primer in the hands of a poor girl, who turns her life’s path around.  The Primer goes beyond traditional educational tools in several ways. First, it engages with the user through interactive storytelling, making learning an immersive experience. The stories it tells are not static but evolve based on the user’s interactions and choices, teaching problem-solving, critical thinking, and moral reasoning. Second, the AI in the Primer is capable of understanding and responding to the emotional and cognitive state of the user, providing support and challenges that are appropriate for the user’s current level of understanding. This aspect of the Primer reflects a deep integration of AI into the educational process, offering a vision of how technology might be used to create highly individualized learning experiences.  Just as Stephenson’s previous book “Snowcrash” [47] has inspired many modern technologies that are or might become a reality (virtual reality, augmented reality, internet of things, surveillance of workers with data analytics, cryptocurrencies, and smart contracts, networked states [48], and even a virtual librarian character that could easily be  a near future product evolved from ChatGTP), “The Diamond Age’s” Illustrated Primer is an inspiration for the next wave of educational technologies.  There is no lack of techno-optimists and capital to push a new wave of technologies that are moving from deceptive to disruptive. Diamandis and Kotler showcase in their book “The future is faster than you think” a student’s field trip to a virtual-reality Ancient Rome, accompanied by an AI instructor to illustrate the educational transformative applications of the combination of GenAI, virtual reality, and augmented reality [49].  But, paraphrasing Darth Vader in Star Wars first film (chronologically), before we get too proud of the technological monstrosity we are about to construct, let us take a step back and reconsider what we have learned about educational technologies (EdTech).  B. Education Is More Than a Marketplace  The worldwide education market was valued at approximately $6,682.46 billion in 2022 (https://d66z.short.gy/zYVVYD). This market encompasses a wide range of segments, including K-12 education, higher education, vocational education, corporate training, and various modes of delivery such as online learning, in-person learning, and blended learning.  In the last 25 years, educational technology has undergone significant transformation. The advent of the internet in the mid-1990s marked the beginning of a new era in education. Early technologies were primarily focused on computer-based learning and multimedia content in classrooms [50]. However, the early 2000s witnessed a surge in online learning platforms [51], revolutionizing access to education. This period saw the introduction of virtual classrooms, e-learning modules, and interactive educational software. The proliferation of mobile technology and tablets in the 2010s further expanded the reach of digital learning, allowing students to access educational resources anytime, anywhere [52]. More recently, advancements in AI, virtual and augmented reality, and adaptive learning systems have further personalized the learning experience, catering to individual learning styles and needs [53], [54]. This rapid evolution of technology has broadened the scope of education and brought about a paradigm shift in teaching methodologies and learning processes.  During all these years, the landscape of educational technology has been marked by a striking duality. On the one hand, there is an undeniable commercialization, with education increasingly influenced by market-driven models and private enterprises [55]. On the other hand, there is a growing movement towards open-source technologies [56] and freely accessible content repositories [57]. This contrast paints a complex picture of the current educational space, where the forces of commodification coexist with a commitment to open access and knowledge sharing.  The current landscape of educational technology, whether opensource or privately owned, demands a critical examination of its approach, implementation, and application. The following are several key issues:  1. Narrow focus on learning. Educational technology often emphasizes “learning” and “learners,” a concept termed “learnification.” This overlooks vital educational aspects like socialization, subjectification, qualification, and contextual factors [58]. Tools like Learning Management Systems (LMSs) tend to function more as management tools than learning aids, limiting the understanding of digital technology’s role in education [59].  2. Technology over pedagogy. The idea that technology should be integrated with teaching methods to enhance education truly is often overlooked. Blending technology with effective teaching strategies is crucial for real progress in education. This ensures that technology exists in the classroom and supports and improves learning outcomes [60]. This concept has been introduced previously. Back in the 1980s, Seymour Papert [61] observed similar issues within the LOGO community. He criticized the usual ways of evaluating educational technology, such as controlled experiments and product reviews. Papert argued for a more comprehensive approach considering the social and cultural aspects of using computers in education. His viewpoint challenges the common, technology-focused mindset in education. Instead of looking at how technology fits into education, he suggested a more culturally aware evaluation of its role. This approach remains relevant today as we continue to explore the most effective ways to integrate technology in learning environments.  3. Emotional and human impact. Understanding digital tools’ emotional and human impact is crucial [62]. These technologies influence students’ and staff’s emotions, values, and behaviors, and their role in learning environments should be supportive and enriching. Online learning technologies, especially LMS, inherently exhibit an “architecture of control” in their design. The user interface and design choices subtly shape users’ behavior and interactions, potentially limiting educational exploration and autonomy. Furthermore, integrating learning analytics introduces continuous monitoring and analysis of student data [63]. While aimed at personalizing and enhancing learning, this constant surveillance raises privacy and psychological concerns [64]. The educational journey can become algorithm-driven, often without transparently acknowledging underlying decision-making processes [65].  C. ChatGPT Goes to School  The domain of education has historically pioneered the assimilation of technological advancements. In the last decades, many software applications have been developed and evolved to cater to diverse educational requisites, spanning online learning, language acquisition, academic research, pedagogical support, content generation, and professional development.  The infusion of AI into education is not a recent phenomenon [66]. Despite years of dedicated research and substantial financial investments, the field has yet to yield substantial impacts beyond research and development, with only a handful of commercial products achieving limited influence.  The emergence of ChatGPT has metamorphosed AI’s role in education from a theoretical construct into an immediate reality. This paradigm shift transpired virtually overnight, organically gaining traction without advertising or marketing campaigns. Stakeholders, including students, educators, and administrators, have instinctively grasped this transformation’s significance, urgency, and potential, even though the precise course of action still needs to be discovered [67].  As compelling proof of this rapid and widespread interest, many teachers are enrolling in different courses about integrating ChatGPT and GenAI tools in their classrooms and courses. Most teachers who participated in these courses cited three primary motivations for their interest in ChatGPT’s role in education. First, they expressed concerns about the potential for increased plagiarism facilitated by the technology. Second, they were intrigued by the implications of automating academic tasks within their specific fields of expertise. Last but not least, they were interested in how ChatGPT could enhance students’ educational experiences and outcomes.  The advent of ChatGPT (as the most known GenAI tool) has further enriched the educational technological landscape, offering, among others, [68]:  • Diverse educational opportunities: ChatGPT and similar LLMs can generate instructional content, facilitate discussions on diversity and inclusion, create quizzes, evaluate assignments, and provide feedback. Their versatility extends to assisting in understanding complex concepts and offering examples of code in programming languages [69].  • Research assistance. ChatGPT can suggest research ideas and methodologies and provide examples from previous studies. It can enhance inclusivity in research, find relationships between subjects, assist in statistical analysis, and suggest further study extensions [70].  • Writing assistance. ChatGPT can offer feedback on writing, provide suggestions on organization, and help make arguments more compelling [71].  However, the integration of LLMs like ChatGPT in education presents also significant risks [68], for example:  • Quality of prompts. ChatGPT and similar models’ efficacy heavily relies on the quality of the prompts provided. The users’ ability to frame questions effectively is crucial in obtaining accurate and relevant responses [72].  • Response variability. The quality of responses can vary significantly based on the application domain. If the training dataset for a particular domain lacks depth or breadth, the responses in that domain might not meet the desired standards [73].  • Hallucinations. LLMs tend to generate content that, while appearing authoritative, might be entirely fabricated or unrelated to the query [74]. Such “hallucinations” can mislead users, especially in an educational context where accuracy is paramount.  • Over-reliance on technology. There is a risk of decreased creativity and critical thinking [75] due to over-dependence on ChatGPT.  • Inaccurate or biased Information. ChatGPT’s responses may unintentionally perpetuate biases and reinforce stereotypes in its training data [76].  • Lack of human interaction: While ChatGPT can assist, it cannot replace the value of human interaction, which is essential for students’ social and emotional development [77].  • Ethical concerns. Issues related to data ownership [78], control, consent, and plagiarism [79] may arise.  • Security concerns [80]. Storing sensitive data on ChatGPT could pose a security risk due to OpenAI has openly stated that conversations with ChatGPT are going to be included in datasets for training future models [81].  Moreover, the use of ChatGPT in education brings forth a set of ethical and societal challenges, especially for educational institutions and decision-makers [68], for example:  • Integrating the GenAI into the educational institutions’ Information Technology (IT) government policies. Glitches, server downtime, or compatibility issues can disrupt the teaching and research process. Thus, the institutions must redefine their IT government strategies to integrate AI advances into their technological ecosystems [82].  • Development of ethical codes and the establishment of general guidelines regarding generative AI. Ensuring responsible and ethical practices in its implementation [83].  • Compliance with data regulations. Due to the geographical location of OpenAI’s servers, compliance with specific data privacy regulations (for example, in the European Union) is compromised [84], [85].  • Limit the educational institution’s dependency on third-party enterprises. Universities should not rely solely on third-party

solutions. They should encourage a collaborative approach, promoting development and adopting open-source, ethical, and secure LLMs [82].  To address these challenges, educators must emphasize critical thinking, promote collaboration, establish clear guidelines for using AI technology, and have backup plans [86].  D. AI Plagiarism, the Elephant in the Room  The evolving landscape of academic integrity is increasingly challenged by the use of writing essays, documentation analysis and research, and even solving math problems, presenting educators with dilemmas over distinguishing genuine student work from AI-generated content. The core of this issue lies in the sophisticated capabilities of AI, which enable the production of text indistinguishable from humanwritten essays at minimal cost and effort [87]. This accessibility has magnified concerns over academic dishonesty, previously exacerbated by the internet and platforms facilitating the sharing of completed assignments.  However, ways to circumvent system controls have always been used, for example, by inserting Cyrillic characters that look like letters of the Latin alphabet (see the table of confusing characters at https://d66z.short.gy/qLwNBx) and easily circumvent anti-plagiarism systems.  Detection tools like Turnitin, designed to identify plagiarism, are now grappling with the nuances of AI-generated texts, often leading to false positives and negatives. The efficacy of these tools diminishes as AI technology advances, a point underscored by research from the University of Maryland [88], which suggests that detecting AIgenerated text reliably may be impossible.  An illustrative case discussed by Robert Topinka [89], a Birkbeck, University of London professor, highlights these challenges. He recounts an instance where a top-performing student contested an accusation of submitting an AI-generated essay, underscoring the limitations of current detection methods and the potential for unjust accusations. The lecturers seem to wish for the easy solution, the infallible judgment of the AI tool that will tell whether the student has cheated with AI. They also seem to lose the irony of it and the fact that research indicates that this infallible judgment is not infallible or even capable of outperforming random classifiers [88].  The situation calls for a fundamental reevaluation of academic assessment methods. Alternatives that prioritize critical thinking and creativity, such as presentations and podcasts, are proposed to adapt to the AI era [90]. These methods ensure fairness and encourage genuine student engagement, moving away from traditional essays vulnerable to AI assistance.  This shift also prompts a broader discussion on the ethical responsibilities of educators in deploying AI detection tools. The reliance on imperfect technology risks harming students’ academic careers and reflects a deeper issue of educational values. The drive towards easy solutions for maintaining academic integrity may overshadow the essential goal of education: to cultivate understanding, critical thinking, and innovation among students.  E. Safe AI in Education  In 2023, we have repeatedly heard and read the words “AI Ethics” [91] and “AI Safety” [92]. We have reached a point where we do not have a common definition, and most people using the terms align it with their agenda. We propose a simple definition of “Safe AI in Education,” which is an AI system that is used by students that:  1. Provides a guarantee of privacy of the students’ data and interactions with it. All the information about the students, their identity, roles, academic records, and interactions with the system  are to be secure and used only to provide the service. We also need guarantees that the information is deleted after the academic course is over;  2. Is aligned with the teaching strategy. ChatGPT and other GenAI tools are multi-purpose. It can allow a student to learn, create content, and research, but also to cheat and avoid doing the hard work and learning. Students can ask the system for solutions to their assignments or to paraphrase essays to evade proctoring and anti-plagiarism software;  3. Provides answers and interactions aligned with a didactic purpose. If an LLM-powered application is used within the context of a learning activity, we need to be able to bind it under certain parameters. For example, Salman Kahn presented at TED 2023 Kahn Amigo [93], an AI system based on GPT-4 that adapted its behavior to a certain study plan, could act as a Socratic teacher, and was able to present relevant questions to the students to help them progress, instead of providing straightforward answers;  4. Minimizes the risk of hallucinations or incorrect information. LLMs are trained with vast amounts of information, and the best ones, like GPT-4 in early 2024, are often correct. However, there is no guarantee that the output is correct and relevant. And there is always the possibility of an AI hallucination. It is a tall order, but a safe AI system needs to maximize the relevance of its answers and minimize its mishaps. The intuition is that this task is much simpler when the application context is smaller than when a chatbot like ChatGPT is open to any conceivable task.  5. Presents a behavior, values, and usefulness that students and teachers understand. The user experience must clarify what the tool is and is not for.  F. Smart Learning Applications, a Technological Approach to Safe AI in Education  The idea of a “Smart Learning Application” emerges as a pivotal innovation rooted in the principles of AI safety and educational integrity. This stems from discussions at the 2023 TEEM conference in Bragança, Portugal, particularly during the Managing Generative AI in Educational Settings session. This idea is conceptualized as an advanced AI educational tool that goes beyond traditional learning applications by integrating with an LMS, such as Moodle, where they are appropriately termed “activities” [94]. In contrast to general educational apps like Kahoot, which do not integrate with the LMSs and therefore fall short of our criteria due to their disconnection from the educational framework, Smart Learning Applications are crafted to function within the specific boundaries of a course. These applications stand out for their capacity to:  • Ensure a secure access. Utilizing the LMS for authentication and authorization, they restrict access to legitimate users.  • Adapt to user roles. The LMS customizes the application’s features to match the user’s role: teacher, student, or administrator.  • Provide course-specific context. Each application instance is directly associated with a course, enabling a customized educational journey. Smart Learning Applications leverage LLMs via APIs to facilitate features such as on-the-fly content creation and personalized learning trajectories. This strategy boosts interactivity and customization, tackling challenges like guaranteeing content accuracy and adhering to data privacy laws. The goal is to present an educational technology that is more closely aligned with educational objectives, capable of upholding academic integrity, and offering a tailored user experience
II. A Final Reflection  There are reasons for excitement and concern with applying GenAI in education. Yet, we must prevent one from overshadowing the other about the leap in AI, and potentially in its educational application, with ChatGPT as the flagship, necessitates relentless study, design, experimentation, and evaluation. This should be done with caution yet boldness, embracing the new possibilities. Let us discard the notion that technology, being material and mercenary, will ruin an education that is spiritual and selfless [95]. Many of the issues and dangers identified in the educational context have yet to arise due to the emergence of ChatGPT or other similar applications. They already existed, have been approached from various perspectives, and have remained unresolved. However, the potential of these technologies and the effect of their rapid penetration in all realms of society are magnifying some of these issues more than ever before [68]. AI, especially with its ability to create content indistinguishable from human production and interact with users through natural language, represents one of our most socially disruptive technological means. We are just beginning to imagine the possibilities, risks, and challenges that this technology opens up. However, it is essential to recognize that the future we may build on this foundation must be in more than just the hands of technologists. There must be spaces for inter- and transdisciplinary co-creation that ensure the ethical, safe, and inclusive development of a technology that, not so long ago, we would have considered science fiction.  IV. Monograph Contents  This International Journal of Interactive Multimedia and Artificial Intelligence monograph about Generative Artificial Intelligence in Education comprises seven research papers. The first paper is entitled “A cybernetic perspective on generative AI in education: From transmission to coordination” by Dai Griffiths, Enrique Frías-Martínez, Ahmed Tlili and, Daniel Burgos. This work examines the impact of LLMs and GenAI on education, highlighting a lack of clarity in human-machine communication within educational models. It introduces two paradigms: the transmission paradigm, which aligns with traditional educational methods and communication models, and the coordination paradigm, which combines constructivist learning models with a coordination communication model. The authors argue that LLMs disrupt the existing balance between these paradigms by creating a simulacrum of intelligence, challenging the transmission paradigm’s validity. They suggest that adopting the coordination paradigm can help educational institutions understand and utilize GenAI more effectively, urging a shift in educational practices to leverage AI’s capabilities fully.  Lin Tang and Yu-Sheng Su, in their work “Ethical implications and principles of using artificial intelligence models in the classroom: A systematic literature review,” conduct a systematic literature review [96], [97] on the ethical implications and principles of using AI models in classrooms, addressing the need for an ethical framework amidst AI’s growing educational application. By analyzing 32 out of 1,445 publications from 2013 to 2023, the authors identified five main ethical concerns: algorithmic bias, data privacy breaches, opacity, diminished autonomy, and academic dishonesty, with algorithmic bias and privacy issues being the most prevalent. They also outline six ethical principles: fairness, privacy, transparency, accountability, autonomy, and beneficence, emphasizing fairness and privacy as critical. The paper highlights the under-researched areas of autonomy and academic misconduct, urging more in-depth discussions and solutions to ethical issues, clarity on implementing ethical principles, and accurate assessment of AI’s ethical implications in education.  The next paper, “A trustworthy automated short-answer scoring system using a new dataset and hybrid transfer learning method,” by Martinus Maslim, Hei-Chia Wang, Cendra Devayana Putra, and Yulius Denny Prabowo, introduces HTL-ASAS, an advanced automated system for scoring short answers, addressing inconsistencies in manual grading by teachers due to various challenges. Utilizing a hybrid transfer learning approach and a new dataset of student answers (QA-CS), the system demonstrates remarkably high accuracy (99.6%) in evaluating responses from introductory IT courses. This high level of precision suggests HTL-ASAS’s potential as a reliable tool in educational settings, promising to reduce teacher workload and improve assessment consistency.  Juan Izquierdo-Domenech, Jordi Linares-Pellicer, and Isabel FerriMolla are the authors of the paper “Virtual reality and language models, a new frontier in learning.” They introduce an innovative learning architecture that combines virtual reality and LLMs with RetrievalAugmented Generation (RAG) to enhance educational experiences across various settings. This approach integrates immersive virtual reality applications with LLMs, allowing students to interactively engage with learning materials through questions and receive answers with textual and visual hints within a virtual reality environment. The paper addresses the challenge of integrating diverse data sources by utilizing RAG to structure information from APIs, PDFs, Structured Query Language (SQL) databases, and more into formats that are easily processed by LLMs. An empirical study involving twenty participants compared the effectiveness of this virtual reality and LLM architecture against traditional learning methods showed significant improvements in learning outcomes for the group using the immersive virtual reality application. This research highlights the potential of combining virtual reality and LLMs to create dynamic, engaging, and effective learning experiences.  The paper “Generative Artificial Intelligence in product design education: Navigating concerns of originality and ethics,” by Kristin A. Bartlett and Jorge D. Camba, explores the integration of image-generative AI in product design education, addressing the technological advancements and their potential future applications. It critically examines the legal and ethical challenges posed by such technology, including issues of bias, exploitation of hidden labor, intellectual property theft, lack of originality, and inadequate copyright protection. The authors offer recommendations for design educators on incorporating AI responsibly into the curriculum. They advocate for AI to be presented as one of many tools available to designers, emphasizing its role in the creative process rather than as a means to produce final designs. The paper also suggests strategies for fostering meaningful discussions about AI among students, aiming to enrich their understanding and ethical use of AI in design.  Verónica Parra, Patricia Sureda, Ana Corica, Silvia Schiaffino, and Daniela Godoy investigate in their work, “Can generative AI solve geometry problems? Strengths and weaknesses of LLMs for geometric reasoning in Spanish” the potential of GenAI, specifically LLMs like ChatGPT, Bard, and others, in solving geometry problems, a key area in high-school curricula. It highlights the growing interest in using LLMs for educational purposes, especially math problem-solving, and notes the usual focus on English language benchmarks. This study differentiates itself by concentrating on Spanish, a comparatively less-resourced language, to explore LLMs’ capabilities in geometric reasoning. By analyzing the performance of chatbots powered by various LLMs, the study assesses their accuracy in solving geometry problems and categorizes errors in their reasoning processes. The findings aim to understand LLMs’ strengths and weaknesses in geometry, paving the way for better classroom integration strategies and developing more advanced generative AI tools for educational support.
The last paper, entitled “Evaluating ChatGPT-generated linear algebra formative assessments, by Nelly Rigaud Téllez, Patricia Rayón Villela, and Roberto Blanco Bautista, delves into the utilization of LLMs, specifically ChatGPT, for creating formative assessments in linear algebra, focusing on the mathematical problem-solving process. It assesses ChatGPT’s performance in generating feedback on linear algebra problems, highlighting deficiencies in reasoning, proofs, and model construction. By comparing feedback from both instructors and ChatGPT against detailed formative feedback criteria, including affective aspects, the study aims to enhance the feedback quality from both sources. A novel framework for formative assessment using LLMs was developed to generate prompts based on common linear algebra errors, facilitating concept development and problemsolving strategies. This approach encourages a dynamic learning cycle where instructors validate tasks. ChatGPT supports query-based learning, revealing insights into improving feedback for advanced math problems and suggesting adaptations in teaching and learning strategies for educators and students.