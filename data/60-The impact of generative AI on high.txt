The impact of generative AI on higher education learning and teaching: A study of educators’ perspectives

Abstract
In recent months, Artificial Intelligence (AI) has had, and will continue to have, a dramatic impact on Higher Education (HE). A study conducted by researchers at a leading university in Australia surveyed 30 of their teaching staff, drawn predominantly from their teaching academy, and interviewed eight of them regarding the impact of AI on HE. Data were analyzed using the procedures of Inductive Thematic Analysis and revealed a lack of any homogenous sentiment around AI in HE and much ambiguity regarding best practice regarding recent technological developments. The results indicate concerns exist around concepts relating to academic integrity, however, these concerns may be exaggerated. Almost half of the participants indicated they were using AI within their teaching roles with the most common design change being modifications to assessments. Less than a quarter of staff agreed the university has adequately equipped them for AI, and more than three quarters indicated they would like support. They unanimously assumed the technology will improve. Keeping in mind universities’ obligation to serve students by preparing them for industry, it is vitally important that the HE sector stays informed of developments in AI and commit to ongoing research and discussions regarding best practice in response to AI. However, anything regarding AI and future developments will be extremely difficult to predict.

Keywords
Artificial intelligenceGenerative artificial intelligenceHigher educationChatGPTLearning and teaching


1. Introduction
Artificial Intelligence (AI) is becoming a ubiquitous entity in modern developed societies. It is a significant factor in marketing, design and entertainment, and is also increasingly present in Higher Education (HE). Many AI systems exist and are operating behind the scenes in large numbers effecting a variety of aspects in current civilization. However, a significant development in AI occurred in November 2022 when the San Francisco based company OpenAI released to the public their Chat Generative Pre-trained Transformer: ChatGPT. ChatGPT is a Large Language Model (LLM) chatbot that uses Natural Language Processing (NLP) to create human-like responses to users’ prompts. This new development has already had a considerable impact on the education industry and has prompted the release of numerous other generative AI tools.
ChatGPT was rapidly adopted by the general public and reached 100 million users in the first two months after its public release, making it the fastest-growing consumer application in history (Hu, 2023). Other significant recent developments in generative AI which are influencing HE include Midjourney, released in July 2022, Microsoft's Bing AI Chat, which was released in February 2023, Google’s chatbot BARD, released in February 2023, and Dall-E, released in January 2021. Some more specific generative AI tools having an impact on HE include Synthesia, a video generation tool, and Amper Music, a cloud-based platform that quickly and easily generates soundtracks for films or digital games. The ultimate impact of AI, or what it might become, is as yet unknown. However, its potential to “trigger transformative change is undeniable” (Bozkurt, 2023, p. 199). Commentators have made strong claims regarding the impact of AI including claiming it is “poised to have a more substantial impact than the introduction of electricity” (Thurzo, Strunga, Urban, Surovková, & Afrashtehfar, 2023, p. 3).
Within the field of education, AI, and in particular generative Artificial Intelligence (GAI), has recently caused industry wide concern. For example, ChatGPT has been observed to be able to pass exams in medicine (Fijačko, Gosak, Štiglic, Picard, & Douma, 2023; Kung et al., 2023; Morreel; Mathysen; Verhoeven, 2023; Newton & Xiromeriti, 2023), law (Choi, Hickman, Monahan, & Schwarcz, 2023; Ryznar, 2023) and English comprehension (de Winter 2023) as well as many other disciplines. However, AI is not without fault and typically produces “hallucinations”: errors with deceptive plausibility and misalignment with truthful information (Walczak & Cellary, 2023, p. 80). In a comparative study comparing the reliability of three LLMs in rhinoplasty presurgical planning, decision making, and patient education, Seth et al. (2023) found: “BARD delivered the most succinct and comprehensible information, followed by ChatGPT and Bing AI” (p. 1).
When the education sector was only just beginning to grapple with the implications of AI, researchers at a leading university in Australia conducted studies to ascertain the impact of engaging with generative Artificial Intelligence (GAI) on Higher Education (HE) Learning and Teaching (L&T). The motivation for this study was to help understand the current perspectives of educators, help inform policy design and begin to identify what best practice might look like when engaging with these emerging technologies. Staff and students were surveyed regarding their experiences. This paper will report on the perspectives observed in the staff survey and follow-up interviews and address the following research question: “What are the perspectives of educators at the University of Adelaide on the impact of generative Artificial Intelligence on Higher Education learning and teaching?
2. Literature review
Recent rapid developments regarding AI in HE have been influenced by two major factors, the release of Open AI’s ChatGPT and the increased virtual learning, including adoption of AI, brought about by COVID-19 (AlDhaen, 2022; Bearman, Ryan, & Ajjawi, 2023; Nawaz, Gomes, & Saldeen, 2020). This was, and continues to be, more prominent in developing nations (Al Dhaen, Stone, & Mahmood, 2022; Shaikh et al., 2022).
Universities have a responsibility to provide their students with up-to-date efficient and relevant learning tools. Universities also have a responsibility to respond to the rapidly changing technological landscape, which includes AI as an increasingly prominent feature (Bearman et al., 2023). To prepare students for industry, which will inevitably include AI, “it shall be compulsory to have students understand at least basic principles and terminology regarding AI” (Thurzo et al., 2023, p. 3). HE institutions also have an obligation to provide equitable education to all students and AI is seen as being able to assist in such provision (McGrath, Pargman, Juth, & Palmgren, 2023). Curriculum content and pedagogical practices at universities need to be revised and updated in light of the recent “AI paradigm shift” (Thurzo et al., 2023, p. 3).
University educators are typically slow to adapt to new technologies due to resistance to changing teaching practices (McGrath et al., 2023). AI is seen as both a threat and an opportunity by university educators. A common theme among the conversations around AI in education is a fear that AI could, or even will, replace teachers (Al Dhaen et al., 2022; McGrath, 2023; Schiff, 2021; Zhai et al., 2021). Bearman et al. (2023, p. 376) describe this as just one aspect of a “dystopia-is-now” perspective on AI in HE. A major concern regarding AI in HE is the potential for students to plagiarize its products (Alhaidry, Fatani. Alrayes, Almana & Alfhaed, 2023; Dien, 2023; King, 2023; Peres, Schreier, Schweidel, & Sorescu, 2023; Shoja, Van de Ridder, & Rajput, 2023), this process even has a new name: “AIgiarism” (Murugesan & Cherukuri, 2023, p. 119). Further dystopian predictions include undermining scientific pursuits (Bozkurt, 2023; Dergaa, Chamari, Zmijewski, & Ben Saad, 2023), privacy and ethics concerns (Crawford, Cowling, & Allen, 2023; Lodge, Thompson, & Corrin, 2023), perpetuating biases (Cooper, 2023) and spreading false information (Murugesan & Cherukuri, 2023).
Many educators are unaware of the use of AI as a teaching tool or are hesitant to implement it in their classrooms (Chiu & Chai, 2020). One common reason is the need for more time to dedicate to learning how to implement AI effectively (Crawford et al., 2023; McGrath et al., 2023). Educators typically rely more on external motivations or incentives (recognition, promotion, and monetary rewards), than internal motivation (interesting teaching and learning, improved learning outcomes, and improved teaching quality) and they therefore often do not see any personal benefit in adopting AI-based learning and teaching pedagogies (Al Dhaen et al., 2022). Furthermore, many educators feel AI tools are still too early in their development and not accurate enough in their information retrieval or delivery to be used in learning and teaching contexts (McGrath et al., 2023). There are also various divides among user types regarding the adoption of AI tools. For example, females self-report having less knowledge about AI than males (McGrath et al., 2023). Students who are not using their first language, or even students with particularly strong accents may find themselves at a disadvantage in some situations using AI-powered voice recognition and dictation tools, as these applications may struggle with accents and dialects, potentially leading to misunderstandings or miscommunications (Wang et al., 2023b).
However, it is not all bad news. Among the contrasting utopian perspectives is AI’s ability to positively influence students’ creativity and self-efficacy (Wang, Sun, & Chen, 2023), provide inclusive curriculum (Bozkurt, 2023), and personalize learning (Lodge et al., 2023; Mogali, 2023). Personalized content has been demonstrated to enhance the learning experience and lead to more successful outcomes for international students (Wang, Sun, & Chen, 2023). It has also been found to be an effective tool in improving post-secondary teachers’ professional knowledge (Chaipidech, Srisawasdi, Kajornmanee, & Chaipah, 2022). ChatGPT was also observed to assist in medical training by “surfacing novel and nonobvious concepts that may not be in learners’ sphere of awareness” (Kung et al., 2023, p. 9). Survey respondents in McGrath et al.’s (2023) study believed that students from vulnerable groups, including those with learning disabilities, could benefit from AI assistance. Furthermore, they agreed that there were equity-based reasons for universities to engage with relevant AI tools.
AI challenges what it means to be a teacher (Bearman et al., 2023; Firat, 2023) and “redefine[s] the roles of human educators” (Bozkurt, 2023, p. 199). Educators should adjust both teaching and modes of assessments to enhance the students' outcomes while simultaneously safeguarding against unethical use of generative AI (Alhaidry, Fatani, Alrayes, Almana, & Alfhaed, 2023; Pearce & Chiavaroli, 2023). For this to occur, universities need to set strong policies, and continuous research agendas, that address AI concerns including ethical implications (Bearman et al., 2023). Now, and during any future periods of rapid developments in technology, constant effort must be placed on “future proofing subjects and degrees” (Crawford et al., 2023, p. 11).
3. Method
A cross-faculty Community of Practice (CoP) was formed among the university staff and students to collectively discuss issues surrounding AI. A team of researchers from the CoP invited members of the university’s Education Academy to participate in a study designed to understand the impact of AI on learning and teaching. Members of the Academy are staff of the university who have demonstrated outstanding university teaching and actively participate on multiple advisory boards and committees supporting and promoting excellence in teaching and learning and the development of educational skills of colleagues. The cohort of participants represent a diverse range of disciplines and have no specific background regarding their previous engagement with technology or AI in education.
The initial mode of data collection was an online survey which attracted 30 useable contributions. The survey was conducted via the university’s account on the Qualtrics platform and comprised seven metadata questions and a further 20 questions regarding the impact of AI on HE including yes/no questions, Likert scales, drop down selection boxes, and open questions with the opportunity for short written answers. This survey design was intended to give a mix of both qualitative and quantitative data. Surveys were circulated amongst, and modified by all authors to ensure face validity. No questions were compulsory, and there was an average response rate of 78.6%. All survey participants will be referred to as respondents when discussing questions they answered, and all statistics will refer to response rates for each question.
The survey was followed with interviews with self-selected members from the survey respondents. Eight members accepted the invitation to be interviewed. Data were analyzed using the established procedures of Inductive Thematic Analysis (Braun & Clarke, 2006, 2022) employing the following series of processes: data collection, coding, generating themes, and analyzing themes. Inductive analysis was employed as it is a method of approaching the data without trying to fit it into a preexisting frame or the researchers’ preconceptions (Nowell, Norris, White, & Moules, 2017). The data corpus was coded manually from Excel spreadsheets and prominent themes identified.
4. Data
Survey data were collected during the first academic semester of 2023 between May and July. The interviews were conducted early in the second semester, in August. Participants in the survey comprised staff from all three faculties at the university. Fig. 1 shows the balance between faculties, demonstrating a strong representation from the Faculty of Health and Medical Sciences in this study.
Fig. 1
Download: Download high-res image (254KB)
Download: Download full-size image
Fig. 1. Faculties of survey respondents.

Two participants, survey respondents one and twenty-six, did not indicate a faculty, one was a teacher of a pre-enrolment course and the other indicated their current roles as a research designer. Table 1 shows the respondents, allocated with pseudonyms SR1 to SR30 and their faculties.
Table 1. Survey Respondent’s faculties.

Pseudonym	Faculty	Pseudonym	Faculty
SR1	Not indicated	SR16	Health & Medical Sciences
SR2	Health & Medical Sciences	SR17	Health & Medical Sciences
SR3	Science, Engineering & Tech	SR18	Health & Medical Sciences
SR4	Health & Medical Sciences	SR19	Arts, Business, Law & Economics
SR5	Health & Medical Sciences	SR20	Arts, Business, Law & Economics
SR6	Health & Medical Sciences	SR21	Health & Medical Sciences
SR7	Science, Engineering & Tech	SR22	Science, Engineering & Tech
SR8	Arts, Business, Law & Economics	SR23	Health & Medical Sciences
SR9	Health & Medical Sciences	SR24	Health & Medical Sciences
SR10	Arts, Business, Law & Economics	SR25	Health & Medical Sciences
SR11	Health & Medical Sciences	SR26	Not Indicated
SR12	Health & Medical Sciences	SR27	Arts, Business, Law & Economics
SR13	Health & Medical Sciences	SR28	Arts, Business, Law & Economics
SR14	Health & Medical Sciences	SR29	Health & Medical Sciences
SR15	Science, Engineering & Tech	SR30	Arts, Business, Law & Economics
The majority (76.7%, n = 23) of survey participants in this study had been employed by the university for more than 10 years, indicating a stable workforce with long-term experience in higher education. Fig. 2 shows the duration of employment of the participants.
Fig. 2
Download: Download high-res image (224KB)
Download: Download full-size image
Fig. 2. Duration of employment of participants.

The majority of participants (73.3%, n = 22) were tenured staff. One was an adjunct senior lecturer and the remaining seven were employed on fixed term or continuous contracts.
Interviews were conducted with eight participants who self-selected to take part in the next stage of the project. Interviewees have been allocated pseudonyms indicating their faculties and their schools, as shown in Table 2.
Table 2. Interviewees’ pseudonyms, faculties and schools.

Pseudonym	Faculty	School
SM	Science, Engineering & Tech	Molecular and Bio-medical science
AH	Arts, Business, Law and Economics	Humanities
SP	Science, Engineering & Tech	Physics, Chemistry and Earth sciences
AJ	Arts, Business, Law and Economics	Journalism
HN	Health & Medical Sciences	Nursing
SA	Science, Engineering & Tech	Agriculture, Food and Wine
SV	Science, Engineering & Tech	Veterinary Science
AS	Arts, Business, Law and Economics	Social Sciences
5. Findings
Key themes were identified from the data. 1) The dominance of ChatGPT, 2) The need for institutional support, and 3) The importance of engaging with students. These themes will be presented and discussed in section 6. A further minor theme regarding positive future outlooks about the progressive improvement of the quality of AI is discussed in section 5.7. Following is a topical presentation of the findings.
5.1. The impact of AI on work domains
Survey participants were asked if they were using, or were planning to use, generative AI in three areas of their work; teaching, research and administration. The most common of these three areas was teaching, in which 48.3% (n = 14) of participants indicated they were using AI. Administration tasks were less frequent (33.3%, n = 9) and research tasks were the most infrequent (28.6%, n = 8). None of the interviewees stated they had used AI in any significant mode in research. Most of them had tested AI’s ability to complete assessment tasks.
Survey participants were asked to indicate, via a 5-step Likert scale, how frequently they use AI to support their work. Only one respondent indicated they had never used generative AI. Two respondents indicated they used AI daily. The most typical response was “Very Infrequently (a few times a semester/year, or less)” with 38.5% (n = 5) of respondents selecting this measure.
When asked to list the AI tools they have used in the last 6 months, all respondents (n = 12) listed ChatGPT. Other AI applications listed by respondents include Bing AI, Midjourney, Dall-E, Grammarly, Research Rabbit, Otter and Elicit. Fig. 3 indicates the number of respondents using each tool listed.
Fig. 3
Download: Download high-res image (238KB)
Download: Download full-size image
Fig. 3. AI tools used by respondents.

This dominance of ChatGPT as the primary generative AI tool is supported by the interview data. When asked how they have engaged with AI, interviewee AS stated “It has always been ChatGPT” and SP made the observation: “When people say AI, everyone just immediately assumes it’s ChatGPT”. Two interviewees stated they had also experimented with Bing AI.
Demonstrating a strong feeling of comfort regarding the use of AI, 69% (n = 9) of survey respondents said they were either somewhat, or extremely comfortable using generative AI-based tools to support their work. No respondents were extremely uncomfortable and only two (15.4%) indicated they were somewhat uncomfortable. No interviewees expressed discomfort regarding engaging with AI. They typically had positive experiences regarding the impact of using AI, often around the concept of saving time or mental effort: “I think we should engage with it as much as possible, primarily as a time saving tool” (SM), and “It decreased my cognitive load” (SP).
The participants were instructed to indicate tasks where they had used AI assistance. The most common response (n = 10) was to test AI’s capabilities to complete their assessment tasks. This was echoed in the interviewee responses, eg: “I actually put all my essay questions through ChatGPT” (AS). The next most common responses were to assist in administration tasks (n = 6) and generate new assessment tasks (n = 5). The participants were invited to list other activities. One respondent stated they use AI to format reference lists, however they did not indicate which application they used for this task. ChatGPT is known to perform poorly at this task (Alkaissi & McFarlane, 2023). Other tasks mentioned by interviewees included “I have used it to help me generate code” (SM), and “I’ve also generated a few paragraphs for bureaucratic reports” (AS). These data suggest the impact of AI is broad, adding further challenges to policy design.
5.2. Skills development
Two related questions asked the participants to what extent they feel the university has equipped them with the necessary knowledge and skills to effectively use AI, and if they would like support in learning how to use generative AI. Only 6 respondents (23%) agreed the university has adequately equipped them, and the majority of participants (78%, n = 21) indicated they would like support. The answers regarding the kind of support staff would like were varied and included: “ethical ways to use AI to design, conduct and write up research” (SR24), “how to do so appropriately and efficiently” (SR14), and “getting more of a sense of the breadth of (particularly teaching) activities that AI can support” (SR8, parenthesis theirs). There was a general sense of unknown among the answers with no specific detailed explanation of the type of support they felt they needed. Indicating a gap in the training and support currently offered by the university, 78% (n = 21) of respondents indicated they believe, or strongly believe, generative AI-based tools can enhance their work. Fig. 4 illustrates this gap.
Fig. 4
Download: Download high-res image (68KB)
Download: Download full-size image
Fig. 4. Gap between adequate training and desired support.

Eighteen respondents (66.6%) indicated they have undertaken some professional development regarding AI during Semester 1 (March to July), 2023. However, when asked to what extent they feel they have the necessary knowledge and skills to effectively discuss the use of AI-based tools and technologies with their students, only 12 (57.7%) selected either ‘agree’ or ‘strongly agree’ from the 5-point Likert scale. Only 55.6% (n = 10) of the respondents who indicated they have undertaken professional development also selected either ‘somewhat agree’ or ‘strongly agree’ that they have the knowledge and skills. This may indicate the professional development was not highly successful. Fig. 5 illustrates this gap in the professional development and its success.
Fig. 5
Download: Download high-res image (682KB)
Download: Download full-size image
Fig. 5. Professional development in AI

This was also a common thread among the interviewees who typically felt they needed more training and to spend more time engaging with AI to get used to it: “It’s something where I’m aware there is a lot I don’t know” (SP), and “We need more professional development … there’s just not enough information getting out there to academics about what they can do with it and what is safe” (SV).
5.3. Student use of AI
The survey participants were asked to indicate, via a 4-point Likert scale, how often they believed students were using AI in their learning. No respondents selected ‘never’. The majority (78%, n = 21) indicated they believed students were using AI either at least once a month or once a week. Fig. 6 shows the frequency educators believe students to be using AI.
Fig. 6
Download: Download high-res image (245KB)
Download: Download full-size image
Fig. 6. Frequency educators believe students are using AI.

Interviewee SV, a lecturer in Veterinary Science, expressed multiple concerns regarding student use of AI: “this semester it’s pretty bad … they use Grammarly so much that in the end it’s almost not their work”. They also noted that they had a couple of students inform them that other students were using it during tutorial activities, with these students expressing they were uncomfortable that other students were doing so.
However, data suggests educators may have been overestimating the student use of AI. In the parallel study, 31% (n = 109) of students surveyed stated they had never used AI and 25.5% (n = 90) had used it very infrequently (a few times a semester/year, or less). Interviewee AJ, a lecturer in Journalism, polled their students and was surprised to find more students than they expected indicated they had not used it. SA, a lecturer in Agricultural Studies, recalled a seminar they recently attended where students expressed they are not game to use it, or they do not want to use it, as they feel it may diminish the value of their degrees. AS, a lecturer in Political Science, observed among their cohort “students which were already in their 2nd, 3rd, 4th year of university already had the skills to write essays, they didn’t really need to access this technology, they didn’t want to”. SM, a lecturer in Molecular Biology, said they have advised their students that it is “in various contexts … OK to use” and suggested there needs to be a focused part of the inductions in first-year to inform the students of benefits and drawbacks as well as correct use and management of AI. These data suggest the impact of AI on student outputs may in fact be smaller than feared.
A theme among the university staff interviewees when discussing student use of generative AI was one of transparency. SA told their students: “If you’re going to use it, that’s fine, but make sure I know you’re using it, and also to demonstrate how you’ve used it.” Similarly, AS told their students as part of the rules of engagement of using ChatGPT in their course, that they wanted the students to be totally transparent. AJ noted if educators are setting up scenarios where they are pretending students are not engaging with AI, and thereby, in effect not being transparent, then that “seems like a very slippery slope to be going down.”
5.4. Impact of AI on curriculum design
A near 50% split (Yes: 48.1%, n = 13; No: 51.9%, N = 14) was observed when the survey participants were asked if they had modified their course design in Semester 1 in light of generative AI. A common element was modifications to assessments: “using a variety of assessment methods” (SR24), “new assessment questions” (SR23), and “undertake application of knowledge assessment tasks” (SR14). Other insightful responses included changing essay topics to ones that rely more heavily on fieldwork and using AI to generate poor quality written drafts with students then tasked to improve the content. However, nearly two thirds of the respondents (63%, n = 17) indicated they had not provided information to students about how to use generative AI. All the respondents stated they had talked to their students about appropriate use, including caution and integrity, of AI and its capabilities.
In response to this survey data, interviewees were asked if they have modified assessments to increase resilience to AI. AJ stated for one course: “I’m actually not as bothered because it’s more about the broad process”, yet for another course: “the assessment for the course does not work anymore and that is something that presents quite a significant issue”. Forecasting a positive engagement with AI, they concluded “I am going to have to think about how I re-focus the assignments much more significantly to more explicitly enable them and support them to engage with AI.” AS stated they are “in two minds about it” and will wait to see what happens with their second semester first-year students, observing “they’re probably a lot more adept and in touch with these technologies”. Other impact on assessment designs include minor developments: “being much clearer about the types of references that are required”, “reduce the percentage of that assessment and bump up others” (AS), and “more focus on the value ad of what it is that human-generated content can add” (AJ). Some educators have either made more dramatic changes or intend to in the near future: “we decided to make the test now closed book instead, there will be no devices” (SV), and “take it back to face-to-face, do it in the classroom again” (AS).
While discussing assessments, the topic of AI detection tools was often raised by interviewees. The common thread was they were inadequate. Half of the interviewees expressed little trust in Turnitin as an AI detection tool and none gave positive feedback: “Turnitin is not super helpful and doesn’t give you any opportunity to actually track it” (AJ), and “we’ve gotta get a better tool than Turnitin” (SA). SV claimed “the technology is changing so rapidly that any detection tool that is developed would be out dated within months.” This is a challenge that is clearly inadequately addressed and will undoubtably impact curriculum and assessment design in the future as educators employ practical responses that circumvent the problem.
5.5. Concerns regarding the impact of AI in HE
The most notable response to any question in the survey was when they were asked if they have any concerns with the use of generative AI in teaching and research? The overwhelming majority (89%, n = 24) of respondents selected ‘yes’ which then allowed them to explain their concerns. The most prominent topics among the responses were concerns around plagiarism and accuracy of AI produced content. Other concerns include the capacity to limit or substitute student learning, avoiding the experiential process of learning, and “implicit bias in data sources used to train GenAI” (SR7). SR22 stated “It will change authentic practice in almost every discipline.”
Worries and concerns about AI was the most frequently populated code from the interview data. However, there was no homogeneous theme among the concerns with the interviewees expressing a variety of issues including copyright and ownership, student use of AI, lack of regulation, inaccuracy of AI, and literacy abilities. SM claims people only get better at things the more they do them and stated; “if we all start using AI to help us with our writing, then our general ability to write as a population will start to drop.” AS observed a notable increase in the average grade of a multiple-choice test in 2023 stating: “the reason why I am worried is because the average mark for that test has been very consistent over the years. It was roughly between 70 and 75, now the average mark this time around is 84 percent.” They believe this indicates their students used AI to assist them with this assessment.
Interviewee SA is concerned because they do not think governments and other policy makers are keeping up with how rapidly AI is developing. Starting with “It scares me heaps”, they expressed a concern that they do not know what happens to their data which they feed into AI tools. However, they then demonstrated another common issue, that of not knowing enough about AI, following with “this might be an unfounded fear”. This unknown element is echoed in SV’s comment: “we’re graduating students and sending them into industry with a parchment that says the university … thinks this student is competent in this, and has deep discipline knowledge in this field. But, do they really?” SM feels generative AI is too easily accessed and predicts over the next two to three years students will become increasingly adept at the use of AI, “beyond even what we are capable of.” This concern is shared by AJ who stated “at some point we will get overtaken well and truly by students who have got a bit more time to do that kind of stuff”. AS is concerned for the students that will get left behind because they have not caught up on the technology as much as others or are afraid to engage with it. AS’s other concerns were that a gradual creep of AI into academia will “remove a little bit of autonomy without us really realizing in a sense” and in the broader social and political contexts feels AI “can turbocharge a lot of the problems that are already created by social media.”
5.6. Perceived impacts of AI in HE
Survey participants were asked to explain their perceived potential benefits in the use of generative AI tools. Twelve respondents (50%) mentioned concepts around efficiency, time saving, and reducing their workload. Other responses included marking, kick-starting ideas, and to “develop potentially re-searchable questions” (SR2). SR22 suggested AI offers “the opportunity to work at higher levels of abstraction”. The use of AI as a writing tool was also prominent in the survey data. The time-saving and tool concepts were strongly echoed in the interview data. SA expressed “if it is used wisely, great, and it could be a great time saver”. However, they also noted “it could potentially lead to people being sloppy”. AS was quite enthusiastic: “this will be fantastic because there’s so much bullshit we have to write sometimes, just kind of filling in reports.” Other potential benefits listed by the interviewees include: “speed up the processing of data” (SM), “quickly get up to speed on something” (SP), and designing multiple choice questions to “free us up for doing more of the higher order stuff” (AJ). SA recalled occasions where they might ask a colleague to proof-read work, or examine a rubric design, because they might look at it slightly differently. They suggested “AI could stand in for that colleague”. Regarding perceived potential benefits of students engaging with AI, AH, a lecturer in Modern History, stated: “Students who learn to use this technology as a genuine tool for learning will fantastically expand their horizons and their potential.” However, they also warned that students who use AI to “dumb themselves down” will succeed in doing so.
Survey questions about the future impact of generative AI in HE returned mixed responses with both positive and negative connotations. Sixty three percent (n = 17) of respondents indicated they do not think the use of generative AI in teaching and learning could lead to a dehumanization of education. However, 70% (n = 19) indicated they were concerned that its use in learning and teaching could lead to students becoming overly reliant on technology. The survey participants were then asked how they think the increasing use of generative AI in HE will impact the future of work and employment within universities. Indicating a complete lack of any coherent consensus on the topic, answers were as varied as “No changes” (SR17) to “change it, and us completely” (SR28), and “Enormous impact on content, assessment, process, jobs, academic roles and equity” (SR10). This is perhaps best summarized by SR9’s answer: “Not sure anyone can really foresee this truly”.
Some of the more insightful responses regarding the future impact of AI in HE included; “it seems that easy/rapid access to information has led to a lack of engagement with that information … AI is likely to exacerbate this” (SR7), and “What we currently have in place now in terms of generative AI tools may be redundant in as little as a few months’ time. Or alternatively, might collapse and rescind somewhat before climbing ahead in a different format again one day” (SR2). SR20 believes it will enhance HE if students are taught to use it properly. However, SR14 believes universities will struggle to ensure academic standards. Another common feeling was AI will be much like other technological advances in the past including calculators, laptop computers and the internet. It will “become another tool to extend thinking and development” (SR16) and “need to be fit in with what is being done” (SR13) however it will need to be managed by humans, which will lead to new employment roles, including handling increased academic integrity issues. Perhaps the most useful response was SR2’s comment that it is dangerous to make predictions based on “current conditioned thought in relation to what we currently know and/or what we already have at our disposal.”
When asked about the future impact of AI in HE, and in their disciplines, interviewees returned a range of both optimistic and pessimistic outlooks: “It might allow us to think slightly differently about problems” (SA), and “I am hesitant because with all technologies you always end up with the sense that it’s going to bring some good and some bad” (AS). SM stated; “I do wonder whether sufficient research might help us identify ways in which we use it that we haven’t thought about yet, that would then lead to new innovations.” However, they also observed it is already being used in the bio-medical field for tasks including experiment design, speeding up repetitive tasks in analysis, and looking for patterns in data samples. AH stated “There are real dangers there and we have to be aware of them, but on the whole this is positive.” However, they went on to say that institutional governance will be too slow to respond to the AI phenomenon and concluded “we have enormous potential now to actually just get on with stuff and to take the initiative. It’s down to us.”
The final question in the survey was an open opportunity for participants to discuss anything else they would like to add. SR20 stated “most academics are not ready to embrace AI in their teaching and suggested “more mandatory PD [Professional Development] should be provided”. SR10 suggested the majority of discourse contains mostly negative connotations and feels the need for more positive discussion. SR7 suggested that if AI can make parts of their job easier or less time consuming, then maybe more time can be dedicated to working on strategies to engage students. A neat summary of the sentiments of the survey was provided by SR5; “As with all new technologies, there will be a balance between what can be gained and what might be lost, and this will need both careful thought and surveillance.” Other concepts raised by the interviewees include exploring AI together with the students: “That is the stuff that we can explore with students in the courses” (AS), and “I don’t think we are doing anything like enough to incorporate the students in this conversation” (AH).
A few of the interviewees had, quite independently, developed a common approach to engaging the students with AI. They were all a little different in detail, however, there was a common pattern of either allowing the students to draft an essay using ChatGPT, or supplying students with one already drafted by AI. The students’ first task, formatively assessed, was to critique the draft, this could include grading and commenting. Then the students were to use that draft, respond to their own critique, and create a final essay. This is a mode the educators see as a way of positively, productively, and appropriately engaging students with AI with full transparency. Methods of ensuring sufficiency of students’ own work in the final product include appendices with prompts and responses from interactions with ChatGPT, drafts with Track Changes, reflective summaries of use of AI, and making the word count dramatically different.
5.7. Future predictions regarding the impact of AI in HE
The data demonstrates there is little homogeneity within each sub-topic around the subject of AI in HE. However, there was one prominent emergent theme; everyone seems to assume the technology will improve. This theme will now be examined and discussed.
Both the data and the literature support issues of inaccuracy with ChatGPT outputs. Surameery and Shakor (2023) observed the accuracy of ChatGPT in reasoning categories was unreliable, more so in inductive reasoning than deductive, and it is prone to “hallucination problems” (p. 19). Biswas (2023) warns the results of ChatGPT analysis should be interpreted with caution. Interviewee AS observed “a lot of what is coming through is just bullshit” and warned “if you cannot tell the difference, so if you don’t know your facts, or you don’t know your content, this is going to destroy you.” However, the accuracy of ChatGPT was often observed to be passable or potentially in the range of scoring a Credit (a grade between 65 and 74 percent): “it would be probably something in the region of a low credit” (AJ), “it’s quite good at solving bio-chemical calculations … if the questions were out of ten to score maybe 6 or 7” (SV). The general feeling was it was not deep enough for high scores or advanced courses: “it can’t provide it at the level of scientific knowledge that I require from my students … not the sort of depth that I was looking for from a third-year student” (SA). SV also noted AI could not deliver detail specific to the individual course content.
There is a common belief that as generative AI tools continue to develop, they will improve in quality and accuracy of output and thereby have ever increasing impact. SR3 stated “The key for me is to stay abreast of new developments in AI, accept that technology is going to continue to advance and be involved with education.” SR24 observed “It is not good enough yet”, implying it will be in the future, and SR7 suspects “that there will be improvements in efficiencies”. This theme was also prevalent among the interview data: “Well, it’s only going to get better” (SP). HN, a lecturer in Nursing, stated: “I’m really curious as to how fast it’s going to evolve to be that more authoritative instrument”. AJ tested ChatGPT against an assessment at the beginning of semesters one and two, and observed the later outcome was “significantly more nuanced.” These sentiments are supported by the literature. Lewandowski, Łukowicz, Świetlik, and Barańska-Rybak (2023) observed the accuracy of ChatGPT-4 in answering questions in Dermatology tests had a “significantly better performance compared with ChatGPT-3.5” (p. 5). Ram and Verma (2023) predict “ChatGPT and other language models will continue to improve in terms of accuracy and responsiveness (p. 259). Stokel-Walker and Van Noorden (2023) also predict this will happen rapidly: “The computer science behind generative AI is moving so fast that innovations emerge every month” (p. 216).
However, there is also discourse, based on recent research, suggesting the opposite. Chen, Zaharia, and Zou (2023) conducted benchmark tests comparing the accuracy of GPT-4 and GPT-3.5 across 7 diverse tasks between March and June 2023. They found a significant variety of performance across the four months. For example, in a pattern recognition task GPT-4 made a mistake on a query that it was correct for in March. It also reduced its accuracy in identifying prime vs composite numbers, from 84% to 51% accuracy. Southern (2023) also observed ChatGPT users are complaining that it has become “prone to more mistakes or nonsensical answers”. He states that traffic on Open AI’s website reduced by 10% between May and June concluding: “The decline in usage hints that the initial enthusiasm for AI chatbots might wane as their limitations become more apparent”. Ortiz (2023) describes this phenomenon as “AI Drift” in an article where she claims: “the rumors are true, ChatGPT is getting progressively dumber.” She suggests the reason for AI Drift is “attempts to improve parts of complicated AI models cause other parts to perform worse.” This is clearly an area for more research and for interested parties to keep a close watch on developments.
6. Discussion
The impact recent developments in AI have had on HE is unquestionably quite dramatic. The findings suggest the greatest concern is around students using AI to generate assessed material and teachers are facing new challenges including not being able to rely on current assessment strategies and not knowing how to create new reliable assessment regimes. As is often the case with new technologies, there are concerns about understanding what technology to use, how it should best be used and what training and support will be provided to ensure competent responses to AI use. A variety of responses among university staff exist, from kneejerk reactions leading to attempts to ban it, through to well thought out modes of transparently engaging with AI. Most staff feel transparency is very important in this regard and are keen to engage with students in such manners. However, although there are a few ideas, there is little consensus regarding what that would look like. From the themes identified in Findings we find some commonalities.
6.1. The dominance of ChatGPT
In 21st century education, the presence of AI is inevitable. Although some staff mentioned using other AI tools in the surveys including Dall-E, Midjourney, Bing AI, and Grammarly, ChatGPT dominated the discourse and was the most discussed AI application across the data corpus. Only three AI tools were mentioned by name by the interviewees. Fig. 7 illustrates the dominance of ChatGPT in the interview data indicating the number of times each tool was mentioned.
Fig. 7
Download: Download high-res image (158KB)
Download: Download full-size image
Fig. 7. Number of times each AI tool was mentioned by name by the interviewees.

ChatGPT can be used as a very productive tool allowing for great time-saving measures for both staff and students. Staff are also using it to improve efficiency in data analysis, especially with Big Data. It is also being used to draft essays and reports, within ethical constraints. However, there is little discussion around what can then be done with the time saved by using AI. The impact of ChatGPT is an industry wide flurry of development in generative AI, and the impact particularly on education has been a variety of knee-jerk reactions to the unknown. Allowing, or even encouraging, the use of AI with the inclusion of transparent records in assessment appendices is a common practical response.
6.2. The need for institutional support
Staff feel there is pressing need for adequate training, however, they also typically feel the university is too slow and bogged down with corporate control to be able to respond to the phenomenon adequately. This sentiment extends to industry, particularly around concepts of intellectual property. The concept of AI as a tool extended beyond simple time saving measures to identifying patterns in data, improving modes of expression, and adding perspectives on a variety of types of content. The data reveal a general feeling of uncertainty regarding the impact of AI in HE in the future with mixtures of optimism and pessimism, often simultaneously within each staff member. Further discussion, and perhaps professional development is needed in relevant and appropriate modes of engaging with AI, for both staff and students. Further discussion is also needed in ethical considerations, and limitations, of engaging with AI. Practical responses to these challenges range from avoiding AI to “tak[ing] the initiative” (AH) and thoroughly engaging with it. Yet, across this range of responses, adherence to academic integrity is a prominent and common concern.
6.3. The importance of engaging with students
A very important concept highlighted by the participants in this study is engaging with the students in conversations around AI. The focus should be about the students’ education after all, as they are the ones the university exists to serve. Looking beyond education and forward to the industries in which the students will be working, observing how AI is being engaged would be an important informative process for educators. A practical response to the challenge of engaging with students has been involving student AI ambassadors in the AI CoP. Interviewees discussed how AI is being used in fields as diverse as journalism, bio-medicine and politics. It is the duty of HE institutions to ensure students are prepared for 21st century industries which will inevitably be impacted by AI in ways yet unpredictable. Universities are obligated to best prepare students to critically, and ethically, engage with industry practice. To this end, educators must stay informed, and keep up to date with theory, pedagogy and practical applications of AI in both education and industry.
7. Conclusion
This study, conducted by researchers at a leading university in Australia, surveyed and interviewed staff from the university’s Education Academy regarding the impact of Artificial Intelligence (AI) on Higher Education (HE). Data revealed there is no homogenous sentiment within the university’s staff around AI in HE and a great deal of ambiguity regarding best practice in response to these recent technological developments. The greatest concerns among staff were around concepts relating to AI’s potential impact on academic integrity, however these concerns may be exaggerated.
AI has had, in recent months, and will continue to have, a dramatic impact on HE. It is imperative that educators, and management, stay abreast of developments and continue to commit to ongoing research and discourse regarding responses to AI, keeping in mind the university’s obligation to serve the students in preparing them for industry. However, if 2023, and this study in particular, has demonstrated anything regarding AI and future developments, particularly in relation to HE, any accurate predictions will be extremely difficult. Perhaps it is best to let SR9 conclude: “Not sure anyone can really foresee this truly”.
8. Implications for theory and practice
Theoretically we see strong connections from the findings of these work to ideas such as connectivism. AI is likely to see a resurgence in interest in this theory and we expect to see strong frameworks relating connectivism to student interaction with these tools for learning in coming years. For educators it adds yet another aspect of technology to consider. Practically we see significant training issues for both staff and students in the use of AI and significant confusion in what tools are appropriate to be used. Lack of training leads to the potential for communication issues and misunderstanding about what is and isn’t acceptable in individual assessment tasks, which has significant implications around academic integrity. We see this as a key area to explore in future research.
The study was conducted at a single university in Australia employing an internal cohort of 30 participants. Whilst the authors feel the results of this study are reflective of the national mood in Australia as indicated by a series of workshops and presentations at conferences and a national statement on the use of AI in education, this study offers a detailed snapshot of one specific case study where most users were mainly aware of ChatGPT. Whilst ChatGPT has a strong current hold, funding models, privacy concerns and copyright issues will provide important issues for institutions and governments to contend with, which could halt adoption. The timeline to respond to these challenges are short as students will not be waiting for policy revisions before engaging with generative AI tools. Policy designs will need to be flexible and adaptable to accommodate rapid developments in AI technologies which may be problematic in large, slow-moving institutions.
The findings from this study may be specific to the local environment and therefore may not be indicative of the general global perspective. It does, however, provide a context for future research and provides a detailed, specific example relevant to the overall discourse on the impact of generative AI on Higher Education.