Generative Artificial Intelligence: Implications and Considerations for Higher Education Practice

Abstract
Generative Artificial Intelligence (GAI) has emerged as a transformative force in higher education, offering both challenges and opportunities. This paper explores the multifaceted impact of GAI on academic work, with a focus on student life and, in particular, the implications for international students. While GAI, exemplified by models like ChatGPT, has the potential to revolutionize education, concerns about academic integrity have arisen, leading to debates on the use of AI detection tools. This essay highlights the difficulties in reliably detecting AI-generated content, raising concerns about potential false accusations against students. It also discusses biases within AI models, emphasizing the need for fairness and equity in AI-based assessments with a particular emphasis on the disproportionate impact of GAI on international students, who already face biases and discrimination. It also highlights the potential for AI to mitigate some of these challenges by providing language support and accessibility features. Finally, this essay acknowledges the disruptive potential of GAI in higher education and calls for a balanced approach that addresses both the challenges and opportunities it presents by emphasizing the importance of AI literacy and ethical considerations in adopting AI technologies to ensure equitable access and positive outcomes for all students. We offer a coda to Ng et al.’s AI competency framework, mapped to the Revised Bloom’s Taxonomy, through a lens of cultural competence with AI as a means of supporting educators to use these tools equitably in their teaching.
Keywords: Generative Artificial Intelligence; international students; ChatGPT; academic integrity; AI literacy; AI literacy framework
1. Introduction
When a development such as Generative Artificial Intelligence (GAI) comes along, with the potential to disrupt almost every aspect of academic work, it is very easy to fall into the trap of using superlatives such as ‘game-changing’ and ‘seismic’. While ChatGPT has come to overwhelmingly dominate the early narrative about GAI, it is important to acknowledge that it is essentially the most popular brand of a particular approach to GAI, but there are many other equally, if not more, capable models (Perplexity.ai, BLOOM, ChatSonic, Claude, Bard, Whisper, Jasper Chat to name just a few). Such is the potential impact that GAI can have, and indeed is already having, that one can be swept along at times alternating between hand-wringing portents of doom and the joyous embrace of potentialities. The truth (if indeed that is an appropriate term) is that the lines between artificial and reality are becoming increasingly blurred and we as individuals and as a collective may well need to adopt a new pragmatic approach to how we live, work, teach, and study that recognizes the very real likelihood that GAI will become as ubiquitous as the internet itself in our everyday lives.
While there are many in higher education that welcome and embrace the opportunities presented by AI, we are at the same time being presented with warnings that academic integrity and contemporary understanding of disciplinary knowledge are under attack [1]. From some quarters, there is vociferous saber-rattling by those who claim that this is yet another existential threat to the institution of higher education and that the academic enterprise will surely be irreparably damaged by this new threat. We hear the call to arms (or, perhaps, a dejected surrender!) that written assessments as we know them are, or will become, redundant and that a return to traditional invigilated pen and paper exams is the only way to ensure a return to order and integrity in higher education assessments. This type of moral panic is common and understandable with a disruption to institutional norms of this magnitude, but we argue that returning to past practices that were more secure but less reliable or valid as an assessment strategy would ultimately serve students poorly in preparation for a world where Generative AI tools will be woven through all aspects of their life and work. This paper explores the question of the potential impact of Generative AI on international students and students for whom English is an additional language in Western higher education institutions with the view to offering one possible strategy for enhancing support to and with international students. In describing our approach for this paper, it is apt to characterize it as a commentary, with its emphasis on a less expansive strategy than a systematic review and one in which “it is expected that the author possesses expertise in the content area of the commentary” (p. 103, [2]). Adopting a similar approach to Grassini [3] in terms of our search strategy, the literature was screened non-systematically using Google Scholar, EBSCO, and Omni-linked databases for articles and books published between 2019 (when GPT-2 was released) and September 2023. Search terms included “AI OR Generative AI AND Higher Education”; “AI OR Generative AI AND International Students”; “AI Literacy AND Higher Education”; and “ChatGPT AND International Students”. Articles perceived as particularly pertinent were used as starting points for the snowball method, with subsequent articles added to the study as a result. Recognizing that this is a rapidly emerging field, this article seeks to inform the conversation about the intersection of AI and educators’ cultural competence, including unique risks and challenges that marginalized students potentially face with these tools.
2. What Is Generative Artificial Intelligence?
Generative AI has become so ubiquitous in such a relatively short period of time that it may seem a little unnecessary to explain what is meant by the term. However, it is that level of ubiquity, and indeed pervasiveness, as well as the still emerging nature of the field, that makes it all the more important to clarify exactly what we are discussing in this paper, and how it works, as some of the concerns that arise from the use of GAI emanate from the way the models that underlie these tools are trained and subsequently operate. Generative AI refers to a class of artificial intelligence systems designed to generate content or data, such as text, images, video, music, computer code, or even complex combinations of these media, that closely resemble human-created content. These systems use machine learning techniques, particularly deep learning, to identify and mimic patterns, styles, and structures found in the input data they are trained on [4,5,6]. Therein lies one of the issues with AI; it is not a neutral and objective ‘entity’; it depends on the quality of the data used in training, and the method of refining or tuning the model, which can be subject to the human biases of their creators [7]. And while these biases can be at least partially accounted for, the process uses human intervention and exposure to the worst elements of the internet, often using poorly paid and vulnerable people to do this work that is ultimately harmful to them [5,8].
Generative Pre-trained Transformer (GPT) models, which underlie the popular ChatGPT tools, are based on the use of “publicly available digital content data (natural language processing [NLP]) to read and produce human-like text in several languages and can exhibit creativity in writing” (p. 53, [4]). Since the release of ChatGPT and its open API in November 2022, this model and several similar rivals have been used to create literally thousands of AI-powered tools that are able to produce graphics and digital artworks; music; identify and describe images; produce and describe video content; and many more emerging applications. But arguably, it is the text generating capacity of GPT models that has attracted the most attention so far for their ability to mimic human writing. In academia, this attention has caused considerable anxiety, concern, and prognostication about the potential, often negatively conceived, impacts of these tools on knowledge creation, academic work, and the integrity of the academic enterprise [9]. These fears make the sector vulnerable to the claims of the technology snake oil industry, which offers techno-solutionism targeted at those fears, but which often does not live up to its promises.
3. Generative AI and Academic Integrity
The arrival of ChatGPT, and other GAI models close on its heels that are rapidly increasing in specificity, accuracy and efficacy, is fueling significant concerns about how these tools may be used to ‘cheat’ in academic programming to circumvent academic norms around originality of thought and written text that are foundational to most understandings of the academic endeavor. While this concern is understandable in consideration of the long-held traditions of higher education, it reinforces the existing culture of privileging one form of knowledge representation—written text, and particularly that written in English—over others. Valuing writing above other creative works makes this form of communication a target for techno-solutionism claiming to protect the integrity of the writing process [10]. In relation to this emerging threat to academic integrity, the responses from the higher education sector “have been varied and fragmented, ranging from those that have rushed to implement full bans on the use of ChatGPT to others who have started to embrace it by publishing student guidance on how to engage with AI effectively and ethically” (pp. 1–2, [11]).
As a result, a growing number of ed tech startups, as well as long-established technology companies, have suddenly turned their attention to developing tools that claim to be able to detect text generated by AI models, using the language of ‘protecting academic integrity’. In response to demands from the sector, OpenAI, the company behind ChatGPT, created a tool in January 2023 that was intended to detect content created by their own model, but the company warned early on that the nature of Generative AI was such that it would be almost impossible to reliably detect AI-generated writing. In late July 2023, they quietly sunsetted the tool, acknowledging that the efficacy was too low to use for most desired applications (e.g., to support accusations of cheating or integrity breaches and subsequent disciplinary proceedings), and there was potential to do harm by leaving the tool running. Even the academic integrity tech giant Turnitin, who released their own AI text detection tool in April 2023 [12] to much fanfare and claims of impossibly high accuracy, had to walk back those claims a few months later when the evidence showed that the false negative, and more concerningly, false positive rates were both much higher than originally reported.
Research on the efficacy of a wide range of GAI detection tools is now beginning to become available (although most are still in pre-print or as yet not peer-reviewed at the time of writing, e.g., [13,14,15,16,17,18]). A universal theme throughout these studies is the conclusion that, at present, it is impossible to reliably detect content generated with the assistance of AI, especially if it is adapted from the AI output. False positive and negative rates of these tools are unacceptably high for use as evidence of academic misconduct in situations where the use of such assistance is not approved [19]. Even Originality.ai, a tool that currently has the highest success rate at detecting AI-generated text content, does not recommend the use of their tool and its outputs for academic disciplinary proceedings [20].
What does this mean for students, then? Unfortunately, while we now have evidence that the difficulty of using another technology in detecting AI-generated text means that these tools should not be relied upon as sole evidence of academic misconduct, it is inevitable that some students will be accused on the basis of reports from these systems that are either institutionally procured (as in the case of the popular Turnitin platform already used by many institutions to detect writing similarity) or through the ethically questionable practice of submitting student work, without consent, to the multitude of freely available platforms that instructors could find with a simple online search. However, institutional faith in proprietary ‘plagiarism’ detection software may be misplaced if the study by Khali and Er [14] is indicative of their effectiveness. Submitting 50 short essays generated by ChatGPT through Turnitin (n = 25) and iThenticate (n = 25), respectively, they reported that 40 out of the 50 returned scores indicating high levels of originality. It is to be expected that Turnitin’s similarity checking tools would identify AI-generated content as original simply because, rather than copying text from other sources, it is generating original text based on the prompt and therefore would have no basis for a comparison to existing text.
Interestingly, when Khali and Er [14] reverse engineered the process and asked, “is this text generated by a chatbot?” (p. 9), ChatGPT identified 46/50 or 92% had been written by itself. This result is also not surprising, given that ChatGPT is trained to provide answers based on a prompt, not necessarily accurate answers (sometimes referred to as ‘hallucinating’), so the framing of the prompt significantly impacts the directionality of the answer. In fact, this last point is of particular significance for those attempting to use ChatGPT to detect Generative AI text. The reality is that “inputting a prompt and a string of text, ChatGPT will confidently state that most original texts are its own work, even excerpts from famous novels” [21]. The danger of an instructor relying on ChatGPT to ‘test’ student papers for AI plagiarism is well illustrated by the experiences of the Texas A & M instructor Dr. Jared Mumm who failed his entire class on the basis of ChatGPT, (incorrectly) claiming that every essay written by his class had been written by ChatGPT [21].
The uncritical reliance on reports from technology solutions, which are often difficult to interpret meaningfully, has a long history of leading to inequitable and unfair treatment of students who may be accused of breaching integrity rules when they have in fact done nothing wrong. Often, it is marginalized students, such as first-in-family learners, learners of color, or international students who become the target of accusations of academic integrity breaches. The consequences for these groups can range from premature decisions to leave an unwelcoming higher education landscape, to dire consequences for international students whose families have invested heavily to ensure they have a better life.
4. Implications of Generative AI for International Students: From Academic Integrity Concerns to Personal Tutor
The international education market has become critical to the financial sustainability of higher education sectors in several parts of the Western world, with Australia, the US, the UK, and Canada the most popular destinations globally for students seeking an international education experience. These countries’ higher education systems, particularly Canada and Australia, are heavily reliant on revenue from international students as public investment in the sector has become unsustainably low, thus forcing many institutions to seek alternative revenues to fund their operations. This situation can sometimes lead to the demonization of international students—resentment for the places they take in higher education, inflated concerns about their contribution to the cost and scarcity of housing in university towns, and the fact that they are needed to prop up these under-funded education systems at all [22].
There is also sometimes suspicion and assumptions about the motives of international students as seeing education as a pathway to immigration, and a belief that certain cultural groups are somehow trying to ‘game’ the system or ‘cheat’ their way to a valuable credential [23]. Such beliefs, which are usually held without sound evidence, dehumanize international students which, ironically, the academic integrity literature suggests is one of the conditions that increases the likelihood of students choosing to breach academic integrity rules. Leask (p. 183, [24]) notes that “Despite the difficulties associated with defining and detecting plagiarism, it is said to be on the increase, and students from ‘other cultures’ are frequently highlighted as being perpetrators of this crime against the academic community of enlightened Western scholars”. This discourse of othering of students who come from beyond the dominant white Western culture is pervasive in the literature, with international students being portrayed as inferior, unable to think critically or learn in the superior Western culture, or as desperate inferior learners whose only hope of success is to plagiarize [24].
There are undertones of bias and racism in the way that academic integrity charges are brought against international students, with the term ‘international students’ often being used pejoratively, and as a euphemism for non-white or of non-European background [23]. In her review of the University of Windsor’s (Ontario, Canada) academic integrity processes and procedures that showed a disproportionate number of students of color being charged with academic integrity violations, Christensen Hughes [25] noted that some faculty members appeared to be deliberately targeting students from visible minorities. This is likely related to the unfounded belief that students from these minority groups are more likely to cheat than their domestic counterparts.
Of similar concern, Liang et al. [26] found that several of the most widely used AI detectors consistently misclassified non-native English writers’ content as being generated by AI, while native English writing samples were more likely to be accurately identified, suggesting a significant bias against certain linguistic patterns. This is an artefact of the extant biases in the data that most foundation Large Language Models are trained on but, to an instructor who already believes or suspects that international students are pre-disposed to cheating, it would confirm the pre-existing bias. In turn, this would likely lead to international students being falsely accused of misuse of GAI tools at a higher rate than their domestic student counterparts, posing obvious concerns for equity and fairness.
Another potential concern for equity, especially in institutions that condone or encourage the use of AI detection tools, is that there will always be a lag between the latest AI models and any potential tools designed to detect their outputs, creating a self-reinforcing digital equity challenge where those who have access to the latest models have an advantage over those who do not [27]. Access to the latest models will inevitably come at a cost, so only those who can afford that cost will be able to use them. What that means practically is that students with the resources to buy access to the best AI tools will have an advantage over those who cannot, not just because the model may produce better outputs, but because they are also less likely to get caught by any detector trained to look for outputs from earlier models [27]. This reinforces existing inequities, potentially unfairly targeting learners with the least access to resources, and suggests that institutionally procured or developed and supported AI systems may be necessary to ensure equitable access for all students.
5. Potential Positives for Students
While there are many concerns about the potential for GAI to be used inequitably or harmfully against students, and particularly international students or those for whom English is an additional language [28], we acknowledge the more hopeful view that these tools also have the potential to provide a number of benefits for diverse student groups. The first and most obvious is the potential for AI to act as a personal language tutor. Thousands of international students coming into Western education systems enter English language preparation programs every year to prepare them for university-level communication. The conversational nature of AI bots based on Large Language Models (LLMs) makes them a good fit to provide a low-stakes, personal, adaptive language tutor that international students could use to improve their written or spoken English. Models could even be trained in local terminology and idioms so that they could explain these to learners in context. To be clear, we see these developments as augmenting the experience of intensive English language learning, rather than replacing the existing approaches, but they would provide some flexibility and reinforcement that is often not available in traditional in-person models that are currently common in English language preparation programs [29,30].
While the advantages of extensive personalized learning and feedback are well known, reconciling how that might be achieved in practice has until very recently been less evident, all the more so in the context of increasing teaching loads and casualization of the academic workforce. Educational chatbots have been available since the early 2010s, but the rapidly increasing availability of more and more powerful AI software and more comprehensive datasets on which to train them means that reliable and affordable alternative (non-human) real-time support is now an achievable reality [31]. Kaplan-Rakowski et al. (p. 316, [6]) argue that chatbots “can engage students in interactive, conversational ways and provide immediate answers to questions, which minimizes delays in learners’ progress due to waiting for teachers’ responses”. Given the multifaceted and increasingly complex nature of the student experience, we need to grasp any tool or strategy that can have a positive impact on student wellbeing. For example, a study by Wu et al. (p. 77798, [32]) reported that student engagement with a responsive chatbot that acted as both a teaching assistant and companion “could reduce feelings of isolation and detachment with greater effect than teacher counselling services”.
Beyond general chatbot tutors, some of the most significant capabilities lie in the specific rather than the general. By that, we mean that we see real potential to impact learning with the development of AI tools that are trained specifically on the information corpus of a course (such as the ProfBot application from Toronto Metropolitan University’s Dr. Sean Wise that is currently in beta [33], or an individual textbook (such as MathGPT, which was trained as a tutor for open access math texts, but can be expanded to any textbook) [34]. With tools like this, the AI acts as a personal study buddy, capable of explaining concepts in multiple ways, responding to questions, creating bespoke questions/quizzes based on the content of the course or textbook, and providing feedback on student work. These tools will feasibly be able to translate and explain concepts written in English to multiple languages, which may significantly impact learning of students in real time by helping them make the connection between the English version and their preferred language. One can easily imagine the potential reduction in cognitive load this could bring about for newcomers if they can expend less cognitive capacity on trying to grasp concepts only previously available in English.
With the increasing globalization of higher education and international students coming to English-speaking education from significantly more diverse linguistic backgrounds, AI supports based on polyglot models such as BLOOM [35] have the potential to ease the transition for many of them by offering contextual, real-time translation into multiple languages. These translations could be based on audio, text, or even images and video. It is likely that students will use these tools to improve their writing in English, especially if they are trained and encouraged to use them ethically and appropriately.
A further potential positive impact of AI for international students lies in the possibilities these tools offer to students with disabilities. International students with disabilities face all the same discrimination and challenges that local students with disabilities do, but they have to do so on top of trying to become culturally and linguistically fluent, learning about systems and practices that are often extremely foreign to them [36,37]. International students are often unaware of the supports available to them in their new institution, struggle to navigate the formal accommodation process, and can also experience personal cultural barriers to self-identifying as having a disability or requiring additional support if their previous cultural experience does not recognize disability in the same way as in the Western educational context [38]. This can be especially challenging for students with invisible or hidden disabilities [36]. Approaching these challenges through a Universal Design for Learning (UDL) approach would require a shift in curriculum development and support to reduce or remove barriers to students, without the need for one-off accommodations for many students [39].
One way to achieve at least some of the goals of UDL [40] would be to offer AI as an assistive tool for all learners, which would help to level the playing field for students with disabilities. For example, learners who need a note taker to create notes for them could use a tool like Otter.ai to capture and summarize the class, providing a searchable digital starting point for the learner to work from. Bespoke solutions may even be able to be trained to provide content or support in the format best suited to the learner’s needs. For example, a learner with a particular need for a visual representation such as a concept map could generate that from their class notes and readings as a starting point for study. Many institutions already allow or encourage students to use assistive AI technologies such as Grammarly to check their written work, and GAI tools are very similar in functionality. They are also rapidly becoming incorporated into office productivity suites (such as Office 365 and Google Suite), so will become a normal part of the accessibility workflow for many people. It is important that a lack of AI literacy does not lead to unfair or unrealistic restrictions on tools that have the potential to improve the lives of many people who currently face barriers to full participation in society.
6. The Need for AI Literacy in Higher Education
Despite the inevitable hand-wringing and moral panic from certain elements of the education world, the reality is that GAI is with us now (even in instances where you may not be aware of it) and will only continue to become a normalized part of our lives. This new reality is resulting in a shift in terms of what is considered to be part of the modern individual’s essential ‘toolkit’ of competencies; “just as traditional literacy skills have been associated with individual rights, understanding AI is essential in the AI era” (p. 354, [41]). The increasingly widespread use of AI across all aspects of society has “created a need for us to not only be able to use AI (even when we do not realize it) but to also have a greater understanding of its broad applications, usefulness, limitations, and biases” (p. 3, [42]). In fact, Ng. et al. (p. 2, [43]) argue that, in an evolving landscape, “AI potentially becomes one of the most important technology skills in the twenty-first century”. Therefore, if our aim is to equip our students with skills, knowledge, and competencies that will allow them to thrive in the 21st century, we need to rapidly adapt our programming to include AI literacy and competency across the disciplines. Academics and staff in universities need to develop literacy in the applications of AI to their own disciplines.
AI literacy can be thought of as an extension of digital or ICT literacy, but the different affordances, risks, and ethical questions that arise from its use require a specific set of skills and critical faculties that go beyond general technology literacy, incorporating elements of functional and social literacy as well [44]. Kong et al. (p. 1, [45]) argue that “the AI revolution is no longer in its infancy” and “therefore, it is time for our society to educate their citizens regarding AI” (pp. 1/2, [45]). They propose that AI literacy comprises three components. The first component is to acquire an understanding of the basic concepts of AI; examples of which include “machine learning, classifiers, decision trees, reasoning and prediction” (p. 2, [45]). The second component builds on this knowledge acquisition with learners applying AI concepts to “make judgement concerning AI independently, which is using AI concepts for evaluation”, and thirdly, to use AI concepts “for understanding the real world through problem solving” (p. 2, [45]). In this manner AI literacy is conceived as existing on a continuum, starting from basic knowledge acquisition and becoming progressively more sophisticated in terms of insight and application. Several potential frameworks for AI literacy and competency in education are beginning to emerge [46,47,48,49]. For something as dynamic as AI, we are mindful that any framework/s we suggest can necessarily be somewhat limited and timebound. Nonetheless, we feel that the works of Ng. et al. [43] and Hillier [46] provide a useful way to reflect on where your current knowledge and practice of AI is situated and identify perhaps where you wish to progress it too.
6.1. Ng et al.’s Framework
Ng et. al.’s [43] exploratory review of AI literacies provides a granular and graduated way of conceptualizing AI literacy. From their review, they identified four aspects of AI literacy. The most ‘basic’ level of AI literacy begins with knowing and understanding AI, where users need training in the acquisition of “fundamental concepts, skills, knowledge and attitudes that require no prior knowledge” (p. 4, [43]). The second aspect is an ability to use and apply AI in a manner that goes beyond acquisition of ‘know how’ knowledge. This level seeks to “to educate citizens to understand AI applications and how it can affect our lives, as well as knowing the ethical issues regarding AI technologies” (p. 4, [43]). The third aspect of AI literacy that they identify moves the dial on from being simply users of AI to a capacity of being able to evaluate and create AI. Finally, as a way of mapping these literacies, they “proposed to assign these three aspects (i.e., know and understand, use, and evaluate and create AI) into the cognitive levels of Bloom’s Taxonomy” (p. 4, [43]). Thus, they assigned “Know and Understand AI” to the bottom two levels of the Taxonomy and “Use and Apply” to the middle ‘apply’ level; with “Evaluate and Create AI” assigned to the three higher cognitive levels of ‘analyze’, ‘evaluate’, and ‘create’.
6.2. Hillier’s Framework
Matthew Hillier’s [46] proposed AI literacy framework offers another useful starting point for academics, staff, and students who need to become proficient and comfortable with the use of AI tools. From our perspective, we believe that it is a particularly useful framework as it “is focused on a user perspective rather than a developer perspective because the majority of students will sit in the former category” [42]. The framework includes five basic elements:
6.2.1. Ethical Use of AI Tools
While he acknowledges that several of the legal and ethical issues around AI are still to be resolved, he proposes that users should familiarize themselves with issues regarding algorithm transparency, data ownership, privacy, hidden labor, embedded bias, and undisclosed plagiarism.
6.2.2. Knowledge of AI Affordances
Given the sheer number of AI tools currently available, it is practically impossible to have a comprehensive knowledge of all tools. Nevertheless, users should familiarize themselves with the capabilities and limitations, including the key risks and benefits of the tools that they are using.
6.2.3. Working Effectively with AI Tools
This element suggests a move from being a more passive user, to the ‘user-as-developer’ approach, and encourages educators to “leverage free, open access online resources…then work with students to develop unit specific examples and lead discussion on the effective use of tools relevant to the discipline, unit or assessment task context” [42].
6.2.4. Evaluation of AI Output
Developing the ability to interrogate and critically analyze content is a key 21st century graduate attribute. As previously noted, “Generative AI is known to hallucinate to produce plausible, but false information in its output (such as fake references) and so being able to evaluate the output for its quality is a key capability in making use of AI tools” [46].
6.2.5. Use and Integration into Practice
Given the implications represented by AI in the workplace, it is vital that “educators can collaborate with students to explore how industry is adopting Generative AI tools, how it is impacting workflows and productivity as well as how the industry can navigate the emergent issues that these tools bring” [46]. Quite simply, students will be entering a workforce where the ability to use AI will be an integral element of many of their roles; consequently, they need to be capable of embracing this new reality.
6.3. Adapting an AI Framework
Proposing a framework for a landscape as complex and rapidly changing as AI in education may seem like a fraught exercise given that it is almost impossible to predict where the evolution of these tools will lead. Literacy frameworks such as those discussed above should be considered more like a compass than a map in that they indicate a general direction we should strive to head in, rather than an exact path to get there. Universities should be critical places helping to shape the ethical and equitable development and use of AI, and university staff will need to be AI literate to avoid perpetuating or creating inequities for diverse learners. As the two frameworks illustrate, an understanding of what is meant by AI literacies is beginning to emerge, ranging from the functionalist level of skills and knowledge acquisition up to the level where you are encouraged to adopt a critical examination of the implications of AI use.
Incorporating a Cultural Context
As Section 4 highlighted, while Generative AI does offer several positive opportunities, there are a number of limitations, not least because “AI-based English language learning applications may not offer a broad enough view of language and culture, limiting students’ exposure to diverse perspectives” (p. 2, [25]). Ng et. al.’s [43] and Hillier’s [46] AI literacy frameworks are helpful starting points for understanding the elements of AI literacy necessary for the general population to function in a society that has AI deeply enmeshed with work and everyday life.
We expect, and we are already seeing, that while the core elements of these frameworks are useful and valid, they may need to be adapted or augmented for different disciplines and contexts. For example, UNESCO is currently developing a framework for AI competency for teachers and students [47], Santana and Diaz-Fernandez [48] identified AI competencies for Human Resource Management professionals, and Bruneault, Sabourin Laflamme, and Mondoux [49] identified AI competencies for ethicists. Given that academics and staff in higher education work in highly specialized environments and interact with both an exceptionally diverse workforce and student body, they are likely to require AI literacies and competencies that recognize the ways in which these tools intersect in their worlds. In particular, the intersection of intercultural competence and AI has the potential to significantly impact the experience of international students and others from marginalized backgrounds through their interactions with university educators and staff.
In this paper, we have not gone as far as to propose another complete AI literacy framework; rather, by specifically focusing on the issues faced by international students and those for whom English is an additional language, what we propose is effectively a coda to Ng et al.’s [43] framework to help inform the AI literacy of educators who support these students. We acknowledge that this is a relatively modest focus. However, as we previously highlighted, this is a group of students for whom interacting with AI may prove to be a greater challenge with additional pitfalls in comparison to their peers and, as such, even relatively small initiatives can have a meaningful impact. As Table 1 indicates, we have incorporated Ng et al.’s [43] three-factor framework, which they mapped against Bloom’s [50] taxonomy. In our proposal, we have introduced modifications and enhancements. Specifically, instead of aligning with the original 1956 Taxonomy, we suggest that Bloom’s Revised Taxonomy [51] is a more suitable match in this instance. Salas-Pilco et al’s [52] (p. 13) systematic review of AI and its implications for inclusive education for minority students argue that “AI and new technologies for inclusive education must consider the situation of the minority groups that need access to quality education”. The main supplementary feature we have put forward is the integration of cultural context into AI literacy at each of Ng et al’s [43] three levels. We offer a total of 14 competencies across the three levels, with each competency aligned with a corresponding potential application for educators to better support their students.
Table 1. Cultural Context and AI Literacy.

Whichever way we choose to address AI literacy, it is clear that we cannot ignore these skills as a requirement to fully participate in an AI-infused society and need to begin building those skills into our programs immediately. You are of course free to choose how you access and interact with AI systems, but perhaps you will consider “the need to recognize the importance of establishing an educational environment that places paramount value on fostering critical thinking, encouraging self-reflection and empowering students to actively engage in the learning process” (p. 4, [53]).
7. Conclusions
Even though AI has been around for a long time in various forms, the capabilities of the current crop of AI technologies using Large Language Models has the potential to be vastly more disruptive to higher education and society at large than previous iterations. We are already seeing that minority and marginalized students, especially international students, are being accused of breaching academic integrity rules by using AI assistance in their assessment at higher rates than their domestic colleagues, perpetuating an existing inequitable pattern.
Correspondingly, these technologies have the potential for positive impacts, with international students and students with disabilities likely to have access to assistive technologies that could significantly help them learn and succeed. As educators, students, institutions, and policymakers grapple with the implications of GAI, it is imperative that we prioritize ethical AI usage, cultivate AI literacy, and develop frameworks that empower students and educators to safely harness the full potential of these technologies.
