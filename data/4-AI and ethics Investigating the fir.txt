AI and ethics: Investigating the first policy responses of higher education institutions to the challenge of generative AI

Abstract
This article addresses the ethical challenges posed by generative artificial intelligence (AI) tools in higher education and explores the first responses of universities to these challenges globally. Drawing on five key international documents from the UN, EU, and OECD, the study used content analysis to identify key ethical dimensions related to the use of generative AI in academia, such as accountability, human oversight, transparency, or inclusiveness. Empirical evidence was compiled from 30 leading universities ranked among the top 500 in the Shanghai Ranking list from May to July 2023, covering those institutions that already had publicly available responses to these dimensions in the form of policy documents or guidelines. The paper identifies the central ethical imperative that student assignments must reflect individual knowledge acquired during their education, with human individuals retaining moral and legal responsibility for AI-related wrongdoings. This top-down requirement aligns with a bottom-up approach, allowing instructors flexibility in determining how they utilize generative AI especially large language models in their own courses. Regarding human oversight, the typical response identified by the study involves a blend of preventive measures (e.g., course assessment modifications) and soft, dialogue-based sanctioning procedures. The challenge of transparency induced the good practice of clear communication of AI use in course syllabi in the first university responses examined by this study.



Introduction
The competition in generative artificial intelligence (AI) ignited by the arrival of ChatGPT, the conversational platform based on a large language model (LLM) in late November 2022 (OpenAI, 2022) had a shocking effect even on those who are not involved in the industry (Rudolph et al. 2023). Within four months, on 22 March 2023, an open letter was signed by several hundred IT professionals, corporate stakeholders, and academics calling on all AI labs to immediately pause the training of AI systems more powerful than GPT-4 (i.e., those that may trick a human being into believing it is conversing with a peer rather than a machine) for at least six months (Future of Life Institute, 2023).

Despite these concerns, competition in generative AI and LLMs does not seem to lose momentum, forcing various social systems to overcome the existential distress they might feel about the changes and the uncertainty of what the future may bring (Roose, 2023). Organisations and individuals from different sectors of the economy and various industries are looking for adaptive strategies to accommodate the emerging new normal. This includes lawmakers, international organisations, employers, and employees, as well as academic and higher education institutions (Ray, 2023; Wach et al. 2023). This fierce competition generates gaps in real-time in everyday and academic life, the latter of which is also trying to make sense of the rapid technological advancement and its effects on university-level education (Perkins, 2023). Naturally, these gaps can only be filled, and relevant questions answered much slower by academia, making AI-related research topics timely.

This article aims to reduce the magnitude of these gaps and is intended to help leaders, administrators, teachers, and students better understand the ramifications of AI tools on higher education institutions. It will do so by providing a non-exhaustive snapshot of how various universities around the world responded to generative AI-induced ethical challenges in their everyday academic lives within six-eights months after the arrival of ChatGPT. Thus, the research had asked what expectations and guidelines the first policies introduced into existing academic structures to ensure the informed, transparent, responsible and ethical use of the new tools of generative AI (henceforth GAI) by students and teachers. Through reviewing and evaluating first responses and related difficulties the paper helps institutional decision-makers to create better policies to address AI issues specific to academia. The research reported here thus addressed actual answers to the question of what happened at the institutional (policy) level as opposed to what should happen with the use of AI in classrooms. Based on such a descriptive overview, one may contemplate normative recommendations and their realistic implementability.

Given the global nature of the study’s subject matter, the paper presents examples from various continents. Even though it was not yet a widespread practice to adopt separate, AI-related guidelines, the research focused on universities that had already done so quite early. Furthermore, as best practices most often accrue from the highest-ranking universities, the analysis only considered higher education institutions that were represented among the top 500 universities in the Shanghai Ranking list (containing 3041 Universities at the time), a commonly used source to rank academic excellence.Footnote1 The main sources of this content analysis are internal documents (such as Codes of Ethics, Academic Regulations, Codes of Practice and Procedure, Guidelines for Students and Teachers or similar policy documents) from those institutions whose response to the GAI challenge was publicly accessible.

The investigation is organised around AI-related ethical dilemmas as concluded from relevant international documents, such as the instruments published by the UN, the EU, and the OECD (often considered soft law material). Through these sources, the study inductively identifies the primary aspects that these AI guidelines mention and can be connected to higher education. Thus it only contains concise references to the main ethical implications of the manifold pedagogical practices in which AI tools can be utilised in the classroom. The paper starts with a review of the challenges posed by AI technology to higher education with special focus on ethical dilemmas. Section 3 covers the research objective and the methodology followed. Section 4 presents the analysis of the selected international documents and establishes a list of key ethical principles relevant in HE contexts and in parallel presents the analysis of the examples distilled from the institutional policy documents and guidelines along that dimension. The paper closes with drawing key conclusions as well as listing limitations and ideas for future research.

Generative AI and higher education: Developments in the literature
General AI-related challenges in the classroom from a historical perspective
Jacque Ellul fatalistically wrote already in 1954 that the “infusion of some more or less vague sentiment of human welfare” cannot fundamentally alter technology’s “rigorous autonomy”, bringing him to the conclusion that “technology never observes the distinction between moral and immoral use” (Ellul, 1964, p. 97).Footnote2 Jumping ahead nearly six decades, the above quote comes to the fore, among others, when evaluating the moral and ethical aspects of the services offered by specific software programs, like ChatGPT. While they might be trained to give ethical answers, these moral barriers can be circumvented by prompt injection (Blalock, 2022), or manipulated with tricks (Alberti, 2022), so generative AI platforms can hardly be held accountable for the inaccuracy of their responsesFootnote3 or how the physical user who inserted a prompt will make use of the output. Indeed, the AI chatbot is now considered to be a potentially disruptive technology in higher education practices (Farazouli et al. 2024).

Educators and educational institution leaders have from the beginning sought solutions on how “to use a variety of the strategies and technologies of the day to help their institutions adapt to dramatically changing social needs” (Miller, 2023, p. 3). Education in the past had always had high hopes for applying the latest technological advances (Reiser, 2001; Howard and Mozejko, 2015), including the promise of providing personalised learning or using the latest tools to create and manage courses (Crompton and Burke, 2023).

The most basic (and original) educational settings include three components: the blackboard with chalk, the instructor, and textbooks as elementary “educational technologies” at any level (Reiser, 2001). Beyond these, one may talk about “educational media” which, once digital technology had entered the picture, have progressed from Computer Based Learning to Learning Management Systems to the use of the Internet, and lately to online shared learning environments with various stages in between including intelligent tutoring system, Dialogue-based Tutoring System, and Exploratory Learning Environment and Artificial Intelligence (Paek and Kim, 2021). And now the latest craze is about the generative form of AI often called conversational chatbot (Rudolph et al. 2023).

The above-mentioned promises appear to be no different in the case of using generative AI tools in education (Baskara, 2023a; Mhlanga, 2023; Yan et al. 2023). The general claim is that GAI chatbots have transformative potential in HE (Mollick and Mollick, 2022; Ilieva et al. 2023). It is further alleged, that feedback mechanisms supposedly provided by GAI can be used to provide personalised guidance to students (Baskara, 2023b). Some argue, that “AI education should be expanded and improved, especially by presenting realistic use cases and the real limitations of the technology, so that students are able to use AI confidently and responsibly in their professional future” (Almaraz-López et al. 2023, p. 1). It is still debated whether the hype is justified, yet the question still remains, how to address the issues arising in the wake of the educational application of GAI tools (Ivanov, 2023; Memarian and Doleck, 2023).

Generative AI tools, such as their most-known representative, ChatGPT impact several areas of learning and teaching. From the point of view of students, chatbots may help with so-called Self-Regulated or Self-Determined Learning (Nicol and Macfarlane‐Dick, 2006; Baskara, 2023b), where students either dialogue with chatbots or AI help with reviewing student work, even correcting it and giving feedback (Uchiyama et al. 2023). There are innovative ideas on how to use AI to support peer feedback (Bauer et al. 2023). Some consider that GAI can provide adaptive and personalised environments (Qadir, 2023) and may offer personalised tutoring (see, for example, Limo et al. (2023) on ChatGPT as a virtual tutor for personalized learning experiences). Furthermore, Yan et al. (2023) lists nine different categories of educational tasks that prior studies have attempted to automate using LLMs: Profiling and labelling (various educational or related content), Detection, Assessment and grading, Teaching support (in various educational and communication activities), Prediction, Knowledge representation, Feedback, Content generation (outline, questions, cases, etc.), Recommendation.

From the lecturers’ point of view, one of the most argued impacts is that assessment practices need to be revisited (Chaudhry et al. 2023; Gamage et al. 2023; Lim et al. 2023). For example, ChatGPT-written responses to exam questions may not be distinguished from student-written answers (Rudolph et al. 2023; Farazouli et al. 2024). Furthermore, essay-type works are facing special challenges (Sweeney, 2023). On the other hand, AI may be utilised to automate a range of educational tasks, such as test question generation, including open-ended questions, test correction, or even essay grading, feedback provision, analysing student feedback surveys, and so on (Mollick and Mollick, 2022; Rasul et al. 2023; Gimpel et al. 2023).

There is no convincing evidence, however, that either lecturers or dedicated tools are able to distinguish AI-written and student-written text with high enough accuracy that can be used to prove unethical behaviour in all cases (Akram, 2023). This led to concerns regarding the practicality and ethicality of such innovations (Yan et al. 2023). Indeed, the appearance of ChatGPT in higher education has reignited the (inconclusive) debate on the potential and risks associated with AI technologies (Ray, 2023; Rudolph et al. 2023).

When new technologies appear in or are considered for higher education, debates about their claimed advantages and potential drawbacks heat up as they are expected to disrupt traditional practices and require teachers to adapt to their potential benefits and drawbacks (as collected by Farrokhnia et al. 2023). One key area of such debates is the ethical issues raised by the growing accessibility of generative AI and discursive chatbots.

Key ethical challenges posed by AI in higher education
Yan et al. (2023), while investigating the practicality of AI in education in general, also consider ethicality in the context of educational technology and point out that related debates over the last decade (pre-ChatGPT, so to say), mostly focused on algorithmic ethics, i.e. concerns related to data mining and using AI in learning analytics. At the same time, the use of AI by teachers or, especially, by students has received less attention (or only under the scope or traditional human ethics). However, with the arrival of generative AI chatbots (such as ChatGPT), the number of publications about their use in higher education grew rapidly (Rasul et al. 2023; Yan et al. 2023).

The study by Chan (2023) offers a (general) policy framework for higher education institutions, although it focuses on one location and is based on the perceptions of students and teachers. While there are studies that collect factors to be considered for the ethical use of AI in HE, they appear to be restricted to ChatGPT (see, for example, Mhlanga (2023)). Mhlanga (2023) presents six factors: respect for privacy, fairness, and non-discrimination, transparency in the use of ChatGPT, responsible use of AI (including clarifying its limitations), ChatGPT is not a substitute for human teachers, and accuracy of information. The framework by Chan (2023) is aimed at creating policies to teach students about GAI and considers three dimensions: pedagogical, governance, and operational. Within those dimensions, ten key areas identified covering ethical concerns such as academic integrity versus academic misconduct and related ethical dilemmas (e.g. cheating or plagiarism), data privacy, transparency, accountability and security, equity in access to AI technologies, critical AI literacy, over-reliance on AI technologies (not directly ethical), responsible use of AI (in general), competencies impeded by AI (such as leadership and teamwork). Baskara (2023b), while also looking at ChatGPT only, considers the following likely danger areas: privacy, algorithmic bias issues, data security, and the potential negative impact of ChatGPT on learners’ autonomy and agency, The paper also questions the possible negative impact of GAI on social interaction and collaboration among learners. Although Yan et al. (2023) considers education in general (not HE in particular) during its review of 118 papers published since 2017 on the topic of AI ethics in education, its list of areas to look at is still relevant: transparency (of the models used), privacy (related to data collection and use by AI tools), equality (such as availability of AI tools in different languages), and beneficence (e.g. avoiding bias and avoiding biased and toxic knowledge from training data). While systematically reviewing recent publications about AI’s “morality footprint” in higher education, Memarian and Doleck (2023) consider the Fairness, Accountability, Transparency, and Ethics (FATE) approach as their framework of analyses. They note that “Ethics” appears to be the most used term as it serves as a general descriptor, while the other terms are typically only used in their descriptive sense, and their operationalisation is often lacking in related literature.

Regarding education-related data analytics, Khosravi et al. (2022) argue that educational technology that involves AI should consider accountability, explainability, fairness, interpretability and safety as key ethical concerns. Ferguson et al. (2016) also looked at learning analytics solutions using AI and warned of potential issues related to privacy, beneficence, and equality. M.A. Chaudhry et al. (2022) emphasise that enhancing the comprehension of stakeholders of a new educational AI system is the most important task, which requires making all information and decision processes available to those affected, therefore the key concern is related to transparency according to their arguments.

As such debates continue, it is difficult to identify an established definition of ethical AI in HE. It is clear, however, that the focus should not be on detecting academic misconduct (Rudolph et al. 2023). Instead, practical recommendations are required. This is especially true as even the latest studies focus mostly on issues related to assessment practices (Chan, 2023; Farazouli et al. 2024) and often limit their scope to ChatGPT (Cotton et al. 2024) (this specific tool still dominates discourses of LLMs despite the availability of many other solutions since its arrival). At the same time, the list of issues addressed appears to be arbitrary, and most publications do not look at actual practices on a global scale. Indeed, reviews of actual current practices of higher education institutions are rare, and this aspect is not yet the focus of recent HE AI ethics research reports.

As follows from the growing literature and the debate shaping up about the implications of using GAI tools in HE, there was a clear need for a systematic review of how first responses in actual academic policies and guidelines in practice have represented and addressed known ethical principles.

Research objective and methodology
In order to contribute to the debate on the impact of GAI on HE, this study aimed to review how leading institutions had reacted to the arrival of generative AI (such as ChatGPT) and what policies or institutional guidelines they have put in place shortly after. The research intended to understand whether key ethical principles were reflected in the first policy responses of HE institutions and, if yes, how they were handled.

As potential principles can diverge and could be numerous, as well as early guidelines may cover wide areas, the investigation is intended to be based on a few broad categories instead of trying to manage a large set of ideals and goals. To achieve this objective, the research was executed in three steps:

1.
It was started with identifying and collecting general ethical ideals, which were then translated and structured for the context of higher education. A thorough content analysis was performed with the intention to put emphasis on positive values instead of simply focusing on issues or risks and their mitigation.

2.
Given those positive ideals, this research collected actual examples of university policies and guidelines already available: this step was executed from May to July 2023 to find early responses addressing such norms and principles developed by leading HE institutions.

3.
The documents identified were then analysed to understand how such norms and principles had been addressed by leading HE institutions.

As a result, this research managed to highlight and contrast differing practical views, and the findings raise awareness about the difficulties of creating relevant institutional policies. The research considered the ethics of using GAI and not expectations towards their development. The next two sections provide details of the two steps.

Establishing ethical principles for higher education
While the review of relevant ethical and HE literature (as presented above) was not fully conclusive, it highlighted the importance and need for some ideals specific to HE. Therefore, as a first step, this study sought to find highly respected sources of such ethical dimensions by executing a directed content analysis of relevant international regulatory and policy recommendations.

In order to establish what key values and ideas drive the formation of future AI regulations in general, Corrêa et al. (2023) investigated 200 publications discussing governance policies and ethical guidelines for using AI as proposed by various organisations (including national governments and institutions, civil society and academic organisations, private companies, as well as international bodies). The authors were also interested in whether there are common patterns or missing ideals and norms in this extensive set of proposals and recommendations. As the research was looking for key principles and normative attributes that could form a common ground for the comparison of HE policies, this vast set of documents was used to identify internationally recognised bodies that have potential real influence in this arena and decided to consider the guidelines and recommendations they have put forward for the ethical governance of AI. Therefore, for the purpose of this study, the following sources were selected (some organisations, such as the EU were represented by several bodies):

European Commission (2021): Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act) and Amending Certain Union Legislative Acts (2021/0106 (COD)).Footnote4

European Parliament Committee on Culture and Education (2021): Report on artificial intelligence in education, culture and the audiovisual sector (2020/2017(INI)).Footnote5

High-Level Expert Group on Artificial Intelligence (EUHLEX) (2019): Ethics Guidelines for Trustworthy AI.Footnote6

UNESCO (2022): Recommendation on the Ethics of Artificial Intelligence (SHS/BIO/PI/2021/1).Footnote7

OECD (2019): Recommendation of the Council on Artificial Intelligence (OECD/LEGAL/0449).Footnote8

The ethical dilemmas established by these international documents (most of which is considered soft law material) were then used to inductively identify the primary aspects around which the investigation of educational AI principles may be organised.

Among the above documents, the EUHLEX material is the salient one as it contains a Glossary that defines and explains, among others, the two primary concepts that will be used in this paper: “artificial intelligence” and “ethics”. As this paper is, to a large extent, based on the deducted categorisation embedded in these international documents, it will follow suit in using the above terms as EUHLEX did, supporting it with the definitions contained in the other four referenced international documents. Consequently, artificial intelligence (AI) systems are referred to in this paper as software and hardware systems designed by humans that “act in the physical or digital dimension by perceiving their environment through data acquisition, interpreting the collected structured or unstructured data, reasoning on the knowledge, or processing the information, derived from this data and deciding the best action(s) to take to achieve the given goal” (EUHLEX, 2019). With regards to ethics, the EUHLEX group defines this term, in general as an academic discipline which is a subfield of philosophy, dealing with questions like “What is a good action?”, “What is the value of a human life?”, “What is justice?”, or “What is the good life?”. It also mentions that academia distinguishes four major fields: (i) Meta-ethics, (ii) normative ethics, (iii) descriptive ethics, and (iv) applied ethics ” (EUHLEX, 2019, p. 37). Within these, AI ethics belongs to the latter group of applied ethics that focuses on the practical issues raised by the design, development, implementation, and use of AI systems. By extension, the application of AI systems in higher education also falls under the domain of applied ethics.

The selection of sample universities
The collection of cases started with the AI guidelines compiled by the authors as members of the AI Committee at their university from May to July 2023. The AI Committee consisted of 12 members and investigated over 150 cases to gauge international best practices of GAI use in higher education when formulating a policy recommendation for their own university leadership. Given the global nature of the subject matter, examples from various continents were collected. From this initial pool authors narrowed the scope to the Top 500 higher education institutions of the Shanghai Ranking list for this study, as best practices most often accrue from the highest-ranking universities. Finally, only those institutions were included which, at the time of data collection, have indeed had publicly available policy documents or guidelines with clearly identifiable ethical considerations (such as relevant internal documents, Codes of Ethics, Academic Regulations, Codes of Practice and Procedure, or Guidelines for Students and Teachers). By the end of this selection process, 30 samples proved to be substantiated enough to be included in this study (presented in Table 1).

All documents were contextually analysed and annotated by both authors individually looking for references or mentions of ideas, actions or recommendations related to the ethical principles identified during the first step of the research. These comments were then compared and commonalities analysed regarding the nature and goal of the ethical recommendation.

Principles and practices of responsible use of AI in higher education
AI-related ethical codes forming the base of this investigation
A common feature of the selected AI ethics documents issued by international organisations is that they enumerate a set of ethical principles based on fundamental human values. The referenced international documents have different geographical- and policy scopes, yet they overlap in their categorisation of the ethical dimensions relevant to this research, even though they might use discrepant language to describe the same phenomenon (a factor we took into account when establishing key categories). For example, what EUHLEX dubs as “Human agency and oversight” is addressed by UNESCO under the section called “Human oversight and determination”, yet they essentially cover the same issues and recommended requirements. Among the many principles enshrined in these documents, the research focuses on those that can be directly linked to the everyday education practices of universities in relation to AI tools, omitting those that, within this context, are less situation-dependent and should normally form the overarching basis of the functioning of universities at all times, such as: respecting human rights and fundamental freedoms, refraining from all forms of discrimination, the right to privacy and data protection, or being aware of environmental concerns and responsibilities regarding sustainable development. As pointed out by Nikolinakos (2023), such principles and values provide essential guidance not only for development but also during the deployment and use of AI systems. Synthesising the common ethical codes in these instruments has led to the following cluster of ethical principles that are directly linked to AI-related higher education practices:

Accountability and responsibility;

Human agency and oversight;

Transparency and explainability

Inclusiveness and diversity.

The following subsections will give a comprehensive definition of these ethical areas and relate them to higher education expectations. Each subsection will first explain the corresponding ethical cluster, then present the specific university examples, concluding with a summary of the identified best practice under that particular cluster.

Accountability and responsibility
Definition in ethical codes and relevance
The most fundamental requirements, appearing in almost all relevant documents, bring forward the necessity that mechanisms should be implemented to ensure responsibility and accountability for AI systems and their outcomes. These cover expectations both before and after their deployment, including development and use. They entail the basic requirements of auditability (i.e. the enablement of the assessment of algorithms), clear roles in the management of data and design processes (as a means for contributing to the trustworthiness of AI technology), the minimalisation and reporting of negative impacts (focusing on the possibility of identifying, assessing, documenting and reporting on the potential negative impacts of AI systems), as well as the ability of redress (understood as the capability to utilise mechanisms that offer legal and practical remedy when unjust adverse impact occurs) (EUHLEX, 2019, pp. 19–20).

Additionally, Points 35–36 of the UNESCO recommendations remind us that it is imperative to “attribute ethical and legal responsibility for any stage of the life cycle of AI systems, as well as in cases of remedy related to AI systems, to physical persons or to existing legal entities. AI system can never replace ultimate human responsibility and accountability” (UNESCO, 2022, p. 22).

The fulfilment of this fundamental principle is also expected from academic authors, as per the announcements of some of the largest publishing houses in the world. Accordingly, AI is not an author or co-author,Footnote9 and AI-assisted technologies should not be cited as authors either,Footnote10 given that AI-generated content cannot be considered capable of initiating an original piece of research without direction from human authors. The ethical guidelines of Wiley (2023) stated that ”[AI tools] also cannot be accountable for a published work or for research design, which is a generally held requirement of authorship, nor do they have legal standing or the ability to hold or assign copyright.”Footnote11 This research angle carries over to teaching as well since students are also expected to produce outputs that are the results of their own work. Furthermore, they also often do their own research (such as literature search and review) in support of their projects, homework, thesis, and other forms of performance evaluation.

Accountability and responsibility in university first responses
The rapidly changing nature of the subject matter poses a significant challenge for scholars to assess the state of play of human responsibility. This is well exemplified by the reversal of hearts by some Australian universities (see Rudolph et al. (2023) quoting newspaper articles) who first disallowed the use of AI by students while doing assignments, just to reverse that decision a few months later and replace it by a requirement of disclosing the use of AI in homeworks. Similarly, Indian governments have been oscillating between a non-regulatory approach to foster an “innovation-friendly environment” for their universities in the summer of 2023 (Liu, 2023), only to roll back on this pledge a few months later (Dhaor, 2023).

Beyond this regulatory entropy, a fundamental principle enshrined in university codes of ethics across the globe is that students need to meet existing rules of scientific referencing and authorship.Footnote12 In other words, they should refrain from any form of plagiarism in all their written work (including essays, theses, term papers, or in-class presentations). Submitting any work and assessments created by someone or something else (including AI-generated content) as if it was their own usually amounts to either a violation of scientific referencing, plagiarism or is considered to be a form of cheating (or a combination of these), depending on the terminology used by the respective higher education institution.

As a course description of Johns Hopkins puts it, “academic honesty is required in all work you submit to be graded …., you must solve all homework and programming assignments without the help of outside sources (e.g., GAI tools)” (Johns Hopkins University, 2023).

The Tokyo Institute of Technology applies a more flexible approach, as they “trust the independence of the students and expect the best use” of AI systems from them based on good sense and ethical standards. They add, however, that submitting reports that rely almost entirely on the output of GenAI is “highly improper, and its continued use is equivalent to one’s enslavement to the technology” (Tokyo Institute of Technology, 2023).

In the case of York University, the Senate’s Academic Standards, Curriculum, and Pedagogy Committee clarified in February 2023 that students are not authorised to use “text-, image-, code-, or video-generating AI tools when completing their academic work unless explicitly permitted by a specific instructor in a particular course” (York University Senate, 2023).

In the same time frame (6 February 2023), the University of Oxford stated in a guidance material for staff members that “the unauthorised use of AI tools in exams and other assessed work is a serious disciplinary offence” not permitted for students (University of Oxford, 2023b).

Main message and best practice: honesty and mutual trust
In essence, students are not allowed to present AI-generated content as their own,Footnote13 and they should have full responsibility and accountability for their own papers.Footnote14 This is in line with the most ubiquitous principle enshrined in almost all university guidelines, irrespective of AI, that students are expected to complete their tasks based on their own knowledge and skills obtained throughout their education.

Given that the main challenge here is unauthorised use and overreliance on GAI platforms, the best practice answer is for students to adhere to academic honesty and integrity, scientific referencing standards, existing anti-plagiarism rules, and complete university assignments without fully relying on GAI tools, using, first and foremost, their own skills. The only exception is when instructed otherwise by their professors. By extension, preventing overuse and unauthorised use of AI assists students in avoiding undermining their own academic capacity-building efforts.

Human agency and oversight
Definition in ethical codes and relevance
AI systems have the potential to manipulate and influence human behaviour in ways that are not easily detectable. AI systems must, therefore, follow human-centric design principles and leave meaningful opportunities for human choice and intervention. Such systems should not be able to unjustifiably subordinate, coerce, deceive, manipulate, condition or herd humans (EUHLEX, 2019, p. 16).

Human oversight thus refers to the capability for human intervention in every decision cycle of the AI system and the ability of users to make informed, autonomous decisions regarding AI systems. This encompasses the ability to choose not to use an AI system in a particular situation or to halt AI-related operations via a “stop” button or a comparable procedure in case the user detects anomalies, dysfunctions and unexpected performance from AI tools (European Commission, 2021, Art. 14).

The sheer capability of active oversight and intervention vis-á-vis GAI systems is strongly linked to ethical responsibility and legal accountability. As Liao puts it, “the sufficient condition for human beings being rightsholders is that they have a physical basis for moral agency.” (Liao, 2020, pp. 496–497). Wagner complemented this with the essential point that entity status for non-human actors would help to shield other parties from liability, i.e., primarily manufacturers and users (Wagner, 2018). This, in turn, would result in risk externalisation, which serves to minimise or relativise a person’s moral accountability and legal liability associated with wrongful or unethical acts.

Users, in our case, are primarily students who, at times, might be tempted to make use of AI tools in an unethical way, hoping to fulfil their university tasks faster and more efficiently than they could without these.

Human agency and oversight in university first responses
The crucial aspect of this ethical issue is the presence of a “stop” button or a similar regulatory procedure to streamline the operation of GAI tools. Existing university guidelines in this question point clearly in the direction of soft sanctions, if any, given the fact that there is a lack of evidence that AI detection platforms are effective and reliable tools to tell apart human work from AI-generated ones. Additionally, these tools raise some significant implications for privacy and data security issues, which is why university guidelines are particularly cautious when referring to these. Accordingly, the National Taiwan University, the University of Toronto, the University of Waterloo, the University of Miami, the National Autonomous University of Mexico, and Yale, among others, do not recommend the use of AI detection platforms in university assessments. The University of Zürich further added the moral perspective in a guidance note from 13 July 2023, that “forbidding the use of undetectable tools on unsupervised assignments or demanding some sort of honour code likely ends up punishing the honest students” (University of Zürich, 2023). Apart from unreliability, the University of Cape Town also drew attention in its guide for staff that AI detection tools may “disproportionately flag text written by non-first language speakers as AI-generated” (University of Cape Town, 2023, p. 8).

Macquarie University took a slightly more ambiguous stance when they informed their staff that, while it is not “proof” for anything, an AI writing detection feature was launched within Turnitin as of 5 April 2023 (Hillier, 2023), claiming that the software has a 97% detection rate with a 1% false positive rate in the tests that they had conducted (Turnitin, 2023). Apart from these, Boston University is among the few examples that recommend employing AI detection tools, but only in a restricted manner to ”evaluate the degree to which AI tools have likely been employed” and not as a source for any punitive measures against students (University of Boston, 2023). Remarkably, they complement the above with suggestions for a merit-based scoring system, whereby instructors shall treat work by students who declare no use of AI tools as the baseline for grading. A lower baseline is suggested for students who declare the use of AI tools (depending on how extensive the usage was), and for the bottom of this spectrum, the university suggests imposing a significant penalty for low-energy or unreflective reuse of material generated by AI tools and assigning zero points for merely reproducing the output from AI platforms.

A discrepant approach was adopted at the University of Toronto. Here, if an instructor indicates that the use of AI tools is not permitted on an assessment, and a student is later found to have used such a tool nevertheless, then the instructor should consider meeting with the student as the first step of a dialogue-based process under the Code of Behaviour on Academic Matters (the same Code, which categorises the use of ChatGPT and other such tools as “unauthorised aid” or as “any other form of cheating” in case, an instructor specified that no outside assistance was permitted on an assignment) (University of Toronto, 2019).

More specifically, Imperial College London’s Guidance on the Use of Generative AI tools envisages the possibility of inviting a random selection of students to a so-called “authenticity interview” on their submitted assignments (Imperial College London, 2023b). This entails requiring students to attend an oral examination of their submitted work to ensure its authenticity, which includes questions about the subject or how they approached their assignment.

As a rare exception, the University of Helsinki represents one of the more rigorous examples. The “Guidelines for the Use of AI in Teaching at the University of Helsinki” does not lay down any specific procedures for AI-related ethical offences. On the contrary, as para. 7 stipulates the unauthorised use of GAI in any course examination “constitutes cheating and will be treated in the same way as other cases of cheating” (University of Helsinki, 2023).Footnote15

Those teachers who are reluctant to make AI tools a big part of their courses should rather aim to develop course assessment methods that can plausibly prevent the use of AI tools instead of attempting to filter these afterwards.Footnote16 For example, the Humboldt-Universität zu Berlin instructs that, if possible, oral or practical examinations or written examinations performed on-site are recommended as alternatives to “classical” written home assignments (Humboldt-Universität zu Berlin, 2023a).

Monash University also mentions some examples in this regard (Monash University, 2023a), such as: asking students to create oral presentations, videos, and multimedia resources; asking them to incorporate more personal reflections tied to the concepts studied; implementing programmatic assessment that focuses on assessing broader attributes of students, using multiple methods rather than focusing on assessing individual kinds of knowledge or skills using a single assessment method (e.g., writing an essay).

Similarly, the University of Toronto suggest instructors to: ask students to respond to a specific reading that is very new and thus has a limited online footprint; assign group work to be completed in class, with each member contributing; or ask students to create a first draft of an assignment by hand, which could be complemented by a call to explain or justify certain elements of their work (University of Toronto, 2023).

Main message and best practice: Avoiding overreaction
In summary, the best practice that can be identified under this ethical dilemma is to secure human oversight through a blend of preventive measures (e.g. a shift in assessment methods) and soft sanctions. Given that AI detectors are unreliable and can cause a series of data privacy issues, the sanctioning of unauthorised AI use should happen on a “soft basis”, as part of a dialogue with the student concerned. Additionally, universities need to be aware and pay due attention to potentially unwanted rebound effects of bona fide measures, such as the merit-based scoring system of the University of Boston. In that case, using different scoring baselines based on the self-declared use of AI could, in practice, generate incentives for not declaring any use of AI at all, thereby producing counter-effective results.

Transparency and explainability
Definition in ethical codes and relevance
While explainability refers to providing intelligible insight into the functioning of AI tools with a special focus on the interplay between the user’s input and the received output, transparency alludes to the requirement of providing unambiguous communication in the framework of system use.

As the European Commission’s Regulation proposal (2021) puts it under subchapter 5.2.4., transparency obligations should apply for systems that „(i) interact with humans, (ii) are used to detect emotions or determine association with (social) categories based on biometric data, or (iii) generate or manipulate content (‘deep fakes’). When persons interact with an AI system or their emotions or characteristics are recognised through automated means, people must be informed of that circumstance. If an AI system is used to generate or manipulate image, audio or video content that appreciably resembles authentic content, there should be an obligation to disclose that the content is generated through automated means, subject to exceptions for legitimate purposes (law enforcement, freedom of expression). This allows persons to make informed choices or step back from a given situation.”

People (in our case, university students and teachers) should, therefore, be fully informed when a decision is influenced by or relies on AI algorithms. In such instances, individuals should be able to ask for further explanation from the decision-maker using AI (e.g., a university body). Furthermore, individuals should be afforded the choice to present their case to a dedicated representative of the organisation in question who should have the power to reviset the decision and make corrections if necessary (UNESCO, 2022, p. 22). Therefore, in the context of courses and other related education events, teachers should be clear about their utilisation of AI during the preparation of the material. Furthermore, instructors must unambiguously clarify ethical AI use in the classroom. Clear communication is essential about whether students have permission to utilise AI tools during assignments and how to report actual use.

As both UN and EU sources point out, raising awareness about and promoting basic AI literacy should be fostered as a means to empower people and reduce the digital divides and digital access inequalities resulting from the broad adoption of AI systems (EUHLEX, 2019, p. 23; UNESCO, 2022, p. 34).

Transparency and explainability in university first responses
The implementation of this principle seems to revolve around the challenge of decentralisation of university work, including the respect for teachers’ autonomy.

Teachers’ autonomy entails that teachers can decide if and to what extent they will allow their students to use AI platforms as part of their respective courses. This, however, comes with the essential corollary, that they must clearly communicate their decision to both students and university management in the course syllabus. To support transparency in this respect, many universities decided to establish 3-level- or 4-level admissibility frameworks (and even those who did not establish such multi-level systems, e.g., the University of Toronto, urge instructors to explicitly indicate in the course syllabus the expected use of AI) (University of Toronto, 2023).

The University of Auckland is among the universities that apply a fully laissez passer laissez-faire approach in this respect, meaning that there is a lack of centralised guidance or recommendations on this subject. They rather confer all practical decision-making of GAI use on course directors, adding that it is ultimately the student’s responsibility to correctly acknowledge the use of Gen-AI software (University of Auckland, 2023). Similarly, the University of Helsinki gives as much manoeuvring space to their staff as to allow them to change the course of action during the semester. As para 1 of their earlier quoted Guidelines stipulates, teachers are responsible for deciding how GAI can be used on a given course and are free to fully prohibit their use if they think it impedes the achievement of the learning objectives.

Colorado State University, for example, provides its teachers with 3 types of syllabus statement options (Colorado State University, 2023): (a) the prohibitive statement: whereby any work created, or inspired by AI agents is considered plagiarism and will not be tolerated; (b) the use-with-permission statement: whereby generative AI can be used but only as an exception and in line with the teachers further instruction, and (c) the abdication statement: where the teacher acknowledges that the course grade will also be a reflection of the students ability to harness AI technologies as part of their preparation for their future in a workforce that will increasingly require AI-literacy.

Macquarie University applies a similar system and provides it’s professors with an Assessment Checklist in which AI use can be either “Not permitted” or “Some use permitted” (meaning that the scope of use is limited while the majority of the work should be written or made by the student.), or “Full use permitted (with attribution)”, alluding to the adaptive use of AI tools, where the generated content is edited, mixed, adapted and integrated into the student’s final submission – with attribution of the source (Macquarie University, 2023).

The same approach is used at Monash University where generative AI tools can be: (a) used for all assessments in a specific unit; (b) cannot be used for any assessments; (c) some AI tools may be used selectively (Monash University, 2023b).

The University of Cape Town (UCT) applies a 3-tier system not just in terms of the overall approach to the use or banning of GAI, but also with regard to specific assessment approaches recommended to teachers. As far as the former is concerned, they differentiate between the strategies of: (a) Avoiding (reverting to in-person assessment, where the use of AI isn’t possible); (b) Outrunning (devising an assessment that AI cannot produce); and (c) Embracing (discussing the appropriate use of AI with students and its ethical use to create the circumstances for authentic assessment outputs). The assessment possibilities, in turn, are categorised into easy, medium, and hard levels. Easy tasks include, e.g., generic short written assignments. Medium level might include examples such as personalised or context-based assessments (e.g. asking students to write to a particular audience whose knowledge and values must be considered or asking questions that would require them to give a response that draws from concepts that were learnt in class, in a lab, field trip…etc). In contrast, hard assessments include projects involving real-world applications, synchronous oral assessments, or panel assessments (University of Cape Town, 2023).

4-tier-systems are analogues. The only difference is that they break down the “middle ground”. Accordingly, the Chinese University of Hong Kong clarifies that Approach 1 (by default) means the prohibition of all use of AI tools; Approach 2 entails using AI tools only with prior permission; Approach 3 means using AI tools only with explicit acknowledgement; and Approach 4 is reserved for courses in which the use of AI tools is freely permitted with no acknowledgement needed (Chinese University of Hong Kong, 2023).

Similarly, the University of Delaware provides course syllabus statement examples for teachers including: (1) Prohibiting all use of AI tools; (2) Allowing their use only with prior permission; (3) Allow their use only with explicit acknowledgement; (4) Freely allow their use (University of Delaware, 2023).

The Technical University of Berlin also proposes a 4-tier system but uses a very different logic based on the practical knowledge one can obtain by using GAI. Accordingly, they divide AI tools as used to: (a) acquire professional competence; (b) learn to write scientifically; (c) be able to assess AI tools and compare them with scientific methods; d) professional use of AI tools in scientific work. Their corresponding guideline even quotes Art. 5 of the German Constitution referencing the freedom of teaching (Freiheit der Lehre), entailing that teachers should have the ability to decide for themselves which teaching aids they allow or prohibit.Footnote17

This detailed approach, however, is rather the exception. According to the compilation on 6 May 2023 by Solis (2023), among the 100 largest German universities, 2% applied a general prohibition on the use of ChatGPT, 23% granted partial permission, 12% generally permitted its use, while 63% of the universities had none or only vague guidelines in this respect.

Main message and best practice: raising awareness
Overall, the best practice answer to the dilemma of transparency is the internal decentralisation of university work and the application of a “bottom-up” approach that respects the autonomy of university professors. Notwithstanding the potential existence of regulatory frameworks that set out binding rules for all citizens of an HE institution, this means providing university instructors with proper manoeuvring space to decide on their own how they would like to make AI use permissible in their courses, insofar as they communicate their decision openly.

Inclusiveness and diversity
Definition in ethical codes and relevance
Para. 34 of the Report by the European Parliament Committee on Culture and Education (2021) highlights that inclusive education can only be reached with the proactive presence of teachers and stresses that “AI technologies cannot be used to the detriment or at the expense of in-person education, as teachers must not be replaced by any AI or AI-related technologies”. Additionally, para. 20 of the same document highlights the need to create diverse teams of developers and engineers to work alongside the main actors in the educational, cultural, and audiovisual sectors in order to prevent gender or social bias from being inadvertently included in AI algorithms, systems, and applications.

This approach also underlines the need to consider the variety of different theories through which AI has been developed as a precursor to ensuring the application of the principle of diversity (UNESCO, 2022, pp. 33–35), and it also recognises that a nuanced answer to AI-related challenges is only possible if affected stakeholders have an equal say in regulatory and design processes. An idea closely linked to the principle of fairness and the pledge to leave no one behind who might be affected by the outcome of using AI systems (EUHLEX, 2019, pp. 18–19).

Therefore, in the context of higher education, the principle of inclusiveness aims to ensure that an institution provides the same opportunities to access the benefits of AI technologies for all its students, irrespective of their background, while also considering the particular needs of various vulnerable groups potentially marginalised based on age, gender, culture, religion, language, or disabilities.Footnote18 Inclusiveness also alludes to stakeholder participation in internal university dialogues on the use and impact of AI systems (including students, teachers, administration and leadership) as well as in the constant evaluation of how these systems evolve. On a broader scale, it implies communication with policymakers on how higher education should accommodate itself to this rapidly changing environment (EUHLEX, 2019, p. 23; UNESCO, 2022, p. 35).

Inclusiveness and diversity in university first responses
Universities appear to be aware of the potential disadvantages for students who are either unfamiliar with GAI or who choose not to use it or use it in an unethical manner. As a result, many universities thought that the best way to foster inclusive GAI use was to offer specific examples of how teachers could constructively incorporate these tools into their courses.

The University of Waterloo, for example, recommends various methods that instructors can apply on sight, with the same set of tools for all students during their courses, which in itself mitigates the effects of any discrepancies in varying student backgrounds (University of Waterloo, 2023): (a) Give students a prompt during class, and the resulting text and ask them to critique and improve it using track changes; (b) Create two distinct texts and have students explain the flaws of each or combine them in some way using track changes; (c) Test code and documentation accuracy with a peer; or (d) Use ChatGPT to provide a preliminary summary of an issue as a jumping-off point for further research and discussion.

The University of Pittsburgh (2023) and Monash added similar recommendations to their AI guidelines (Monash University, 2023c).

The University of Cambridge mentions under its AI-deas initiative a series of projects aimed to develop new AI methods to understand and address sensory, neural or linguistic challenges such as hearing loss, brain injury or language barriers to support people who find communicating a daily challenge in order to improve equity and inclusion. As they put it, “with AI we can assess and diagnose common language and communication conditions at scale, and develop technologies such as intelligent hearing aids, real-time machine translation, or other language aids to support affected individuals at home, work or school.” (University of Cambridge, 2023).

The homepage of the Technical University of Berlin (Technische Universität Berlin) displays ample and diverse materials, including videosFootnote19 and other documents, as a source of inspiration for teachers on how to provide an equitable share of AI knowledge for their students (Glathe et al. 2023). More progressively, the university’s Institute of Psychology offers a learning modul called “Inclusive Digitalisation”, available for students enrolled in various degree programmes to understand inclusion and exclusion mechanisms in digitalisation. This modul touches upon topics such as barrier-free software design, mechanisms and reasons for digitalised discrimination or biases in corporate practices (their homepage specifically alludes to the fact that input and output devices, such as VR glasses, have exclusively undergone testing with male test subjects and that the development of digital products and services is predominantly carried out by men. The practical ramifications of such a bias result in input and output devices that are less appropriate for women and children) (Technische Universität Berlin, 2023).

Columbia recommends the practice of “scaffolding”, which is the process of breaking down a larger assignment into subtasks (Columbia University, 2023). In their understanding, this method facilitates regular check-ins and enables students to receive timely feedback throughout the learning process. Simultaneously, the implementation of scaffolding helps instructors become more familiar with students and their work as the semester progresses, allowing them to take additional steps in the case of students who might need more attention due to their vulnerable backgrounds or disabilities to complete the same tasks.

The Humboldt-Universität zu Berlin, in its Recommendations, clearly links the permission of GAI use with the requirement of equal accessibility. They remind that if examiners require students to use AI for an examination, “students must be provided with access to these technologies free of charge and in compliance with data protection regulations” (Humboldt-Universität zu Berlin, 2023b).

Concurringly, the University of Cape Town also links inclusivity to accessibility. As they put it, “there is a risk that those with poorer access to connectivity, devices, data and literacies will get unequal access to the opportunities being provided by AI”, leading to the conclusion that the planning of the admissible use of GAI on campus should be cognizant of access inequalities (University of Cape Town, 2023). They also draw their staff’s attention to a UNESCO guide material containing useful methods to incorporate ChatGPT into the course, including methods such as the “Socratic opponent” (AI acts as an opponent to develop an argument), the “study buddy” (AI helps the student reflect on learning material) or the “dynamic assessor” (AI provides educators with a profile of each student’s current knowledge based on their interactions with ChatGPT) (UNESCO International Institute for Higher Education in Latin America and the Caribbean, 2023).

Finally, the National Autonomous University of Mexico’s Recommendations suggest using GAI tools, among others, for the purposes of community development. They suggest that such community-building activities, whether online or in live groups, kill two birds with one stone. On the one hand, they assist individuals in keeping their knowledge up to date with a topic that is constantly evolving, while it offers people from various backgrounds the opportunity to become part of communities in the process where they can share their experiences and build new relations (National Autonomous University of Mexico, 2023).

Main message and best practice: Proactive central support and the pledge to leave no one behind
To conclude, AI-related inclusivity for students is best fostered if the university does not leave its professors solely to their own resources to come up with diverging initiatives. The best practice example for this dilemma thus lies in a proactive approach that results in the elaboration of concrete teaching materials (e.g., subscriptions to AI tools to ensure equal accessibility for all students, templates, video tutorials, open-access answers to FAQs…etc.), specific ideas, recommendations and to support specialised programmes and collaborations with an inclusion-generating edge. With centrally offered resources and tools institutions seem to be able to ensure accessability irrespective of students’ background and financial abilities.

Discussion of the First Responses
While artificial intelligence and even its generative form has been around for a while, the arrival of application-ready LLMs – most notably ChatGPT has changed the game when it comes to grammatically correct large-scale and content-specific text generation. This has invoked an immediate reaction from the higher education community as the question arose as to how it may affect various forms of student performance evaluation (such as essay and thesis writing) (Chaudhry et al. 2023; Yu, 2023; Farazouli et al. 2024).

Often the very first reaction (a few months after the announcement of the availability of ChatGPT) was a ban on these tools and a potential return to hand-written evaluation and oral exams. In the institutions investigated under this research, notable examples may be most Australian universities (such as Monash) or even Oxford. On the other hand, even leading institutions have immediately embraced this new tool as a great potential helper of lecturers – the top name here being Harvard. Very early responses thus ranged widely – and have changed fast over the first six-eight months “post-ChatGPT”.

Over time responses from the institutions investigated started to put out clear guidelines and even created dedicated policies or modified existing ones to ensure a framework of acceptable use. The inspiration leading these early regulatory efforts was influenced by the international ethics documents reviewed in this paper. Institutions were aware of and relied on those guidelines. The main goal of this research was to shed light on the questions of how much and in what ways they took them on board regarding first responses. Most first reactions were based on “traditional” AI ethics and understanding of AI before LLMs and the generative revolution. First responses by institutions were not based on scientific literature or arguments from journal publications. Instead, as our results demonstrated it was based on publicly available ethical norms and guidelines published by well-known international organizations and professional bodies.

Conclusions, limitations and future research
Ethical dilemmas discussed in this paper were based on the conceptualisation embedded in relevant documents of various international fora. Each ethical dimension, while multifaceted in itself, forms a complex set of challenges that are inextricably intertwined with one another. Browsing university materials, the overall impression is that Universities primarily aim to explore and harness the potential benefits of generative AI but not with an uncritical mindset. They are focusing on the opportunities while simultaneously trying to address the emerging challenges in the field.

Accordingly, the main ethical imperative is that students must complete university assignments based on the knowledge and skills they acquired during their university education unless their instructors determine otherwise. Moral and legal responsibility in this regard always rests with human individuals. AI agents possess neither the legal standing nor the physical basis for moral agency, which makes them incapable of assuming such responsibilities. This “top-down” requirement is most often complemented by the “bottom-up” approach of providing instructors with proper maneuvering space to decide how they would like to make AI use permissible in their courses.

Good practice in human oversight could thus be achieved through a combination of preventive measures and soft, dialogue-based procedures. This latter category includes the simple act of teachers providing clear, written communications in their syllabi and engaging in a dialogue with their students to provide unambiguous and transparent instructions on the use of generative AI tools within their courses. Additionally, to prevent the unauthorised use of AI tools, changing course assessment methods by default is more effective than engaging in post-assessment review due to the unreliability of AI detection tools.

Among the many ethical dilemmas that generative AI tools pose to social systems, this paper focused on those pertaining to the pedagogical aspects of higher education. Due to this limitation, related fields, such as university research, were excluded from the scope of the analysis. However, research-related activities are certainly ripe for scientific scrutiny along the lines indicated in this study. Furthermore, only a limited set of institutions could be investigated, those who were the ”first respondents” to the set of issues covered by this study. Hereby, this paper hopes to inspire further research on the impact of AI tools on higher education. Such research could cover more institutions, but it would also be interesting to revisit the same institutions again to see how their stance and approach might have changed over time considering how fast this technology evolves and how much we learn about its capabilities and shortcomings.

