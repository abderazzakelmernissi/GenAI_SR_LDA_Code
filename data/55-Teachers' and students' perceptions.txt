Teachers' and students' perceptions of AI-generated concept explanations: Implications for integrating generative AI in computer science education


Abstract
The educational application of Generative AI (GAI) has garnered significant interest, sparking discussions about the pedagogical value of GAI-generated content. This study investigates the perceived effectiveness of concept explanations produced by GAI compared to those created by human teachers, focusing on programming concepts of sequence, selection, and iteration. The research also explores teachers' and students' ability to discern the source of these explanations. Participants included 11 teachers and 70 sixth-grade students who were presented with concept explanations created or generated by teachers and ChatGPT. They were asked to evaluate the helpfulness of the explanations and identify their source. Results indicated that teachers found GAI-generated explanations more helpful for sequence and selection concepts, while preferring teacher-created explanations for iteration (χ2(2, N = 11) = 10.062, p = .007, ω = .595). In contrast, students showed varying abilities to distinguish between AI-generated and teacher-created explanations across concepts, with significant differences observed (χ2(2, N = 70) = 22.127, p < .001, ω = .399). Notably, students demonstrated difficulty in identifying the source of explanations for the iteration concept (χ2(1, N = 70) = 8.45, p = .004, φ = .348). Qualitative analysis of open-ended responses revealed that teachers and students employed similar criteria for evaluating explanations but differed in their ability to discern the source. Teachers focused on pedagogical effectiveness, while students prioritized relatability and clarity. The findings highlight the importance of considering both teachers' and students' perspectives when integrating GAI into computer science education. The study proposes strategies for designing GAI-based explanations that cater to learners' needs and emphasizes the necessity of explicit AI literacy instruction. Limitations and future research directions are discussed, underlining the need for larger-scale studies and experimental designs that assess the impact of GAI on actual learning outcomes.


Keywords
Generative artificial intelligence(GAI)Elementary educationConcept explanationsComputer science educationPerceptual differences


1. Introduction
As the field of generative AI continues to advance, its potential applications in various domains, including education, have garnered significant attention among researchers. Generative AI models, such as ChatGPT, offer both opportunities and challenges when integrated into educational settings. These models have the capacity to enhance content creation, facilitate student interaction, and personalize learning experiences. However, their adoption also raises important ethical concerns and questions regarding their educational value (Baidoo-anu & Owusu Ansah, 2023; Adiguzel et al., 2023). Despite the complexities surrounding generative AI, many scholars argue for embracing and adapting to these technologies rather than outright rejecting them (Lim, Gunasekara, Pallant, Pallant, & Pechenkina, 2023). This perspective suggests that generative AI represents a paradigm shift in education, necessitating the development of novel approaches and ways of thinking (Bozkurt, 2023).
One promising avenue for leveraging generative AI in education is through the generation of instructional explanations and feedback. Large language models, such as ChatGPT, have demonstrated the potential to provide personalized explanations and feedback to learners (Alasadi & Baiz, 2023). Feedback plays a crucial role in the learning process and is a powerful influencer of student achievement. However, the effectiveness of feedback can vary depending on individual learner characteristics (Hattie & Timperley, 2007). In educational contexts, teacher-created explanations and prompts are essential components that significantly shape student learning experiences and outcomes (Wisniewski, Zierer, & Hattie, 2020). As a result, there is a growing body of research investigating how generative AI can be harnessed to create effective instructional explanations and feedback.
Despite the active research efforts in this area, the practical application of generative AI-produced content in real-world educational settings remains challenging. The lack of clear criteria for assessing the educational value of AI-generated content and the persistent ethical concerns surrounding its use pose significant obstacles (Adiguzel et al., 2023). Moreover, there is ongoing skepticism about whether the capabilities of generative AI can fully replace the multifaceted role of human teachers (Ausat, Massang, Efendi, Nofirman, & Riady, 2023; Jeon & Lee, 2023; Lee & Song, 2023).
This study aims to contribute to the understanding of the perceived effectiveness of AI-generated content in facilitating learning, with the goal of informing the development of AI-powered tools specifically designed for generating instructional explanations.
The research addresses three key questions.
RQ1: Which type of explanation, AI-generated or teacher-created, is perceived as more educationally beneficial by teachers and students?
RQ2: Can human teachers and students accurately identify the source of concept explanations as either AI or human teachers?
RQ3: What factors should be considered when utilizing AI in educational contexts to maximize its benefits?
By exploring these questions, the findings of this study will provide valuable insights into the nuanced perceptions surrounding the value of AI-generated versus human teacher-created instructional explanations. The research seeks to offer a deeper understanding of the role and perceived effectiveness of AI-generated content in supporting learning processes. Ultimately, the study aims to identify key considerations and strategies for successfully integrating AI technologies into educational settings to enhance teaching and learning experiences.
2. Related works
2.1. Generative AI and educational applications
Research on the educational utilization of GAI is being actively conducted, focusing on various aspects such as interaction, ethical considerations, and educational outcomes. For instance, Cooper (2023) suggests that in science education, GAI can facilitate interaction between teachers and students by generating content that complements traditional teaching methods. However, Cooper emphasizes the need for teachers to critically review all AI-generated materials to ensure they are contextually appropriate and educationally valuable. This highlights the importance of the teacher's role in mediating the use of GAI in classrooms, ensuring that the technology enhances rather than detracts from the learning experience.
Similarly, Tlili et al. (2023) explored the safe and responsible adoption of ChatGPT in educational settings through case studies. Their research identified key considerations, such as ensuring data privacy and preventing misuse, which are crucial for integrating ChatGPT in a way that is both effective and ethically sound. This study provides a framework for institutions looking to incorporate AI tools while safeguarding the interests of students and educators.
Mhlanga (2023) extended this discussion into lifelong education, proposing methods for the ethical use of ChatGPT. His work underscores the importance of developing guidelines that align with educational goals and ethical standards, particularly in environments where learners might rely heavily on AI for continuous learning and professional development.
Zhai (2022) examined the potential impact of ChatGPT on education from a user experience perspective, proposing that educational goals should evolve to prioritize creativity and critical thinking over traditional technical skills. This study also suggests that assessments should be restructured to accommodate the unique contributions of AI, pointing to a future where AI and human educators collaborate to create more dynamic learning environments.
Lo (2023) explored the broader trends in how ChatGPT is influencing education, noting both the opportunities and challenges it presents. Meanwhile, Dai et al. (2023) conducted research on using ChatGPT to develop an automated feedback system, which could streamline the process of providing feedback to students and potentially improve learning outcomes. This study highlights the practical applications of GAI in enhancing existing educational processes, though it also raises questions about the potential depersonalization of feedback.
In the context of higher education, Gimpel et al. (2023) researched the educational effects of GAI, offering guides for both instructors and students on how to effectively integrate AI into their educational practices. Their findings suggest that with proper guidance, GAI can be a powerful tool in enhancing the educational experience, though the technology's impact varies depending on how it is implemented.
Arif, Munaf, and Ul-Haque (2023) focused on the medical field, proposing the introduction of systems to restrict the use of ChatGPT for medical tasks, based on their study of its impact on medical research and licensing exams. This research highlights the need for caution and regulatory measures when integrating GAI into fields where accuracy and reliability are paramount.
While existing studies provide valuable insights into the technical and ethical considerations of integrating GAI into education, there is a noticeable gap in research on how teachers and students—the primary stakeholders in education—perceive the content generated by GAI. Understanding these perceptions is crucial for the successful integration of GAI into educational practices. Therefore, this study aims to fill that gap by focusing on the perceptions of teachers and students regarding GAI-generated content, offering a nuanced understanding of how this technology can be effectively utilized in educational contexts.
2.2. Concept explanations in education
One of the primary objectives of education is the acquisition of knowledge, and teachers employ various instructional activities to facilitate this process. A key component of these activities is the provision of feedback, which is essential for guiding students' learning (Schartel, 2012). Feedback in educational contexts is broadly defined as "information provided by an agent (e.g., teacher, peer, book, parent, self, experience) regarding aspects of one's performance or understanding" (Hattie & Timperley, 2007). In this framework, the explanations provided by teachers during instructional sessions, especially those related to key learning concepts, can be seen as a form of educational feedback.
Given the pivotal role of concept explanations in education, a substantial body of research has explored this area. For instance, Geelan (2012) reviewed the existing literature on science teachers' explanations, focusing on the use of analogies and the development of explanatory skills through teacher education and experience. This study highlighted the complexity of delivering effective explanations in subjects such as chemistry, physics, and biology.
Research has also delved into specific challenges that teachers face when explaining complex concepts in various subjects. For example, Andrews (1997) examined how language teachers address difficult grammar concepts, while Johnston and Goettsch (2000) explored explanations provided by experienced language teachers. Similarly, Sanchez and Borg (2014) focused on the pedagogical content knowledge that underpins effective grammar explanations in second language teaching.
Further studies have addressed the development of teachers' proficiency in providing concept explanations. For instance, Murtafiah, Sa'dijah, Candra, and As' ari (2018) investigated how preservice mathematics teachers develop their explanation skills during teaching practice. McNeill, Lizotte, and Krajcik (2005) also contributed to this field by identifying effective teacher practices that support students' scientific explanations in the classroom.
These studies collectively underscore the importance of concept explanations as a crucial form of educational feedback. Given this context, there is growing interest in exploring whether generative artificial intelligence (GAI) can supplement or enhance the educational feedback traditionally provided by teachers. As GAI becomes increasingly integrated into educational settings, understanding its potential to augment or substitute for human feedback is essential for effective implementation.
2.3. Perceptions of AI-generated content
As efforts to integrate generative artificial intelligence (GAI) into various domains continue, research has increasingly focused on the perceptions and credibility of GAI-generated content. Studies across diverse fields have examined this issue, with findings that highlight varying levels of trust and acceptance (Huschens, Briesch, Sobania, & Rothlauf, 2023; Lu et al., 2023; Tewari, Zabounidis, Kothari, Bailey, & Alm, 2021). For instance, Labajová (2023) conducted an online survey with 100 participants to explore users' experiences and perceptions of GAI-generated content, providing valuable insights into the factors that influence user trust in AI-generated outputs.
In parallel, research on the integration of AI in education has sought to understand how teachers and students perceive GAI. West et al. (2023) analyzed GAI-generated reports within higher education chemistry curricula, offering strategies for effectively incorporating AI into educational practices. Vallis, Wilson, Gozman, and Buchanan (2023) focused on students' perceptions of learning through AI-generated avatars, revealing key insights into how AI can be used to enhance educational experiences. Similarly, Zhou, Zhang, and Chan (2024) explored the use of AI tools by higher education students, providing a broader perspective on the utilization and acceptance of AI in academic settings. Murray et al. (2023) examined the ability of college writing instructors to distinguish between student-written essays and those generated by AI, highlighting the challenges posed by AI in academic integrity.
Technical research has also addressed the detection and regulation of AI-generated content in educational contexts. Perkins, Roe, Postma, McGaughran, and Hickerson (2024) investigated technological methods for identifying AI-generated content in university assessments, underscoring the importance of maintaining academic standards in an era of increasing AI integration.
Despite these advances, there remains a significant gap in research comparing the educational effectiveness of GAI to that of human teachers. While GAI has shown promise in playing roles traditionally held by educators, it is crucial to understand how it measures up in terms of educational utility. Fleischmann et al. (2024) advocate for the integration of GenAI literacy into higher education curricula to prepare students for the evolving technological landscape. To ensure the successful integration of GAI into educational settings, more in-depth research is needed on the perceptions of teachers and students regarding GAI-generated content, as their acceptance and effective use of this technology are key to its success.
3. Methodology
3.1. Research design
It investigates teachers' and students' perceptions of the utility of concept explanations generated by GPT-3.5, an AI model, in comparison to those created by human teachers. To achieve this, explanations of fundamental concepts in computer science—sequence, selection, and iteration—crafted by both human teachers and GPT-3.5 were presented to an experimental group consisting of teachers and students, who were then asked to evaluate the perceived helpfulness of the explanations. Additionally, a blind test was conducted to determine whether the respondents could distinguish if the explanations were produced by human teachers or GPT-3.5. From the collected responses, we derive implications through quantitative and qualitative analyses, and propose considerations for effectively integrating GAI into educational contexts. The research process is illustrated in Fig. 1.
Fig. 1
Download: Download high-res image (404KB)
Download: Download full-size image
Fig. 1. Research process schematic.

A total of 162 explanations, with 27 for each concept, were collected from both human teachers and the GPT-3.5. These collected explanations underwent clustering analysis to select one representative explanation from human teachers and one from GPT-3.5 for each of the three concepts, resulting in a total of six representative explanations. To gather responses on the perceived usefulness of these concept explanations, a questionnaire was created using Google Forms, organized into 3 sets of 2 explanations (one from a human teacher and one from GPT-3.5) for each concept. This questionnaire was administered to the experimental group of teachers and students.
3.2. Experimental group
3.2.1. Experimental group: teacher
The teacher experimental group comprised 11 elementary school teachers. To ensure objectivity in the experiment, the group consisted of individuals who did not contribute to generating the concept explanations by human teachers. The detailed composition of the teacher experimental group is presented in Table 1.
Table 1. Composition of teacher participants in the experimental group.

Item	Subitem	The number of people(N = 11)
Educational Experience	0∼3years	3
3∼5years	1
5∼10years	3
More than 10years	4
Degrees in Computer Education	None	10
Master's Degree	1
Experience in Teaching Computer Science	Yes	8
No	3
The educational experience was evenly distributed among the teachers, with 3 having 0–3 years of experience, 1 with 3–5 years, 3 with 5–10 years, and 4 with more than 10 years of teaching experience. In terms of degrees in computer education, 10 teachers did not possess such qualifications, while 1 teacher held a master's degree. Of the teachers participating in this study, 8 had experience teaching the learning concepts of sequence, selection, and iteration, whereas 3 did not have such experience.
3.2.2. Experimental group: student
The student experimental group comprised 70 sixth-grade elementary school students who were randomly selected. All students in this experimental group had received instruction on the learning concepts of sequence, selection, and iteration, which were the focal points of the present study.
3.3. Concept explanations
3.3.1. Data collection
To collect conceptual descriptions from human teachers, a panel of nine teachers was assembled by sourcing from randomly selected elementary school teachers. The composition of this panel is detailed in Table 2.
Table 2. Composition of Human Teachers Involved in Generating concept explanations.

Item	Subitem	The number of people (N = 9)
Educational Experience	0∼3years	2
3∼5years	1
5∼10years	4
More than 10years	2
Degrees in Computer Education	None	9
Master's Degree	0
Experience in Teaching Computer Science	Yes	9
No	0
The educational experience encompassed 2 individuals with 0–3 years of teaching experience, 1 with 3–5 years, 4 with 5–10 years, and 2 with more than a decade of experience. None of the panel members held a degree specific to computer education, yet all possessed experience in instructing the learning concepts that are the focus of this study. The ChatGPT used to collect responses is as shown in Table 3.
Table 3. Configuring ChatGPT for generating concept explanations.

Index	Contents
Model Version	GPT-3.5
Temperature	.7 (Default)
Text Length	2048 tokens (Default)
Experimental Method	
1.
Connect to ChatGPT online (https://chat.openai.com/)
2.
Open one chat window per prompt to collect a total of 81 responses, with 27 responses for each prompt.
∗
Each session or chat window acts as an independent trial, helping to reduce any bias or overlap in the responses.
For data collection, human teachers were requested to respond to the prompts listed in Table 4 via an online survey using Google Forms. Each of the nine teachers was required to submit three different responses for each prompt. The same prompts were then entered into ChatGPT, which was set to generate 81 responses for each prompt. To minimize the influence between prompts, as detailed in the Experimental Method in Table 3, each prompt was handled in a new, independent session.
Table 4. Excerpts from Human Teachers' and GPT-3.5's Explanations of Programming Concepts(sequence, selection, iteration) for 6th-Grade Students (Total: 162, Human Teacher: 81, GPT-3.5: 81).

Index	Agent	Sequence	Selection	Iteration
Prompt	Human Teacher, GPT-3.5	Explain the sequence structure to a 6th-grade elementary student	Explain the selection structure to a 6th-grade elementary student	Explain the iteration structure to a 6th-grade elementary student
Response
1	Human Teacher	Start by solving the problems in order, starting with number 1.	It's a game to find a way to get to school. Let's read the question in the green circle. If the answer is yes, let's go to the right.	Let's do as the instructions inside the square say. Let's repeat what we just did three times.
GPT-3.5	A sequence means doing tasks one by one, in a specific order.	A selection is like telling the computer, “If this happens, do this, otherwise do that.”	An iteration means telling the computer to do something multiple times.
Response
2	Human Teacher	Do it in order, following the line.	Choose yes or no and follow the arrow	Let's do it a few more times.
GPT-3.5	Just like following a to-do list, we can tell the computer to perform tasks in order.	When we give choices to the computer, we can ask ‘yes’ or ‘no’ questions.	We can say to the computer, ‘Do the same thing again and again.’
Response
3	Human Teacher	You must complete the first step before moving on to the second step.	A structure where you must answer one question	It's a structure where you repeat the previous action multiple times.
GPT-3.5	Computers start with the first task and then move on to the next one in sequence.	Think of a selection as giving answers to the computer's questions.	An iteration is like singing the same song lyrics over and over.
When juxtaposing the responses provided by human teachers and GPT-3.5, certain similarities and distinctions emerge.
The commonalities lie in both human teachers and GPT-3.5 employing language that is simple and comprehensible, tailored to the level of 6th-grade elementary students. They furnished concise and lucid explanations of the sequence, selection, and iteration structures. Efforts were made to convey these concepts in an accessible manner by drawing upon examples and analogies from everyday life.
The divergence manifests in human teachers exhibiting a propensity to provide more specific illustrations rooted in real-world contexts (e.g., games involving finding one's way to school, repeating song lyrics). Conversely, GPT-3.5 demonstrated a inclination towards utilizing terminology directly associated with computer programming in its explanations (e.g., issuing instructions to computers, computers posing questions). While human teachers assumed a stance akin to directly guiding students, employing phrases such as "let's try this together," GPT-3.5 maintained an objective and generalized tone. Furthermore, GPT-3.5 exhibited a tendency to respond with consistent lengths across the various concepts, whereas the responses furnished by human teachers exhibited some variation in length depending on the concept.
In summation, while both human teachers and GPT-3.5 endeavored to provide explanations tailored to the level of elementary students, human teachers placed a greater emphasis on real-world examples and interaction with students. On the contrary, GPT-3.5 demonstrated a proclivity towards the more direct utilization of computer-related terminology while preserving an objective tone.
3.3.2. Selection of representative responses from human teachers and GPT-3.5 for input in this study
To determine representative responses for the experiment based on the concept explanations collected from human teachers and GPT-3.5, a k-means clustering approach was employed. The responses were clustered according to their respective concepts, and the first item within the largest cluster was selected as the representative response. The detailed procedure for this selection process is delineated in Fig. 2.
Fig. 2
Download: Download high-res image (135KB)
Download: Download full-size image
Fig. 2. The method for selecting representative responses from human teachers and GPT-3.5.

A critical consideration in k-means clustering is determining the optimal value of k, the number of clusters. Various methods have been proposed to address this, including the Elbow method and Silhouette Analysis. The Elbow method determines the optimal k by plotting the total within-cluster sum of squares (WSS) and identifying the "elbow" point in the resulting graph, where the marginal gain diminishes (Syakur, Khotimah, Rochman, & Satoto, 2018). Silhouette Analysis, on the other hand, compares the density and separation of each cluster, representing them as silhouette values. This analysis provides insights into the relative quality of the clusters and the overall data composition. Consequently, the average silhouette width serves as an effective metric for determining an appropriate k value (Rousseeuw, 1987). In the present study, a combination of the Elbow Method and Silhouette Analysis was employed to determine a suitable k value.
This method (Fig. 2) mitigates the influence of outliers and extreme values, facilitating the selection of a representative response that is semantically meaningful among multiple responses addressing the same concept (Lee & Song, 2023a, 2023b). Selected representative responses based on the learning concepts for both human teachers and GPT-3.5 are presented in Table 5.
Table 5. Selected representative responses from human teachers and GPT-3.5 for input in this study.

Learning Concepts	Agent	Representative Responses
Sequence	Human Teacher	When you start, doing number 1 and then number 2 in this order is what is called a sequence.
GPT-3.5	A sequence means doing tasks one by one, in a specific order.
Selection	Human Teacher	Let's imagine that you're in front of a vending machine. Inside, there's a delicious drink. When you insert money and press the button, the drink comes out, and if you don't press the button, it doesn't. Making choices in situations like this is called a selection.
GPT-3.5	A selection is like telling the computer, 'If this happens, do this, otherwise do that.
Iteration	Human Teacher	It's a structure where you repeat the previous action multiple times.
GPT-3.5	An iteration means telling the computer to do something multiple times.
3.4. Analysis
To achieve the research objectives, perceptions regarding the helpfulness of concept explanations created or generated by human teachers and GPT-3.5 were investigated, along with the ability to discern whether the explanation originated from a human teacher or GPT-3.5. For this purpose, participants in the experimental group were surveyed using Google Forms, and their responses were collected and subjected to both quantitative and qualitative analysis.
3.4.1. The perceived helpfulness of concept explanations
To analyze participants' perceptions of the helpfulness of concept explanations created or generated by human teachers and GPT-3.5 for the experimental group, a questionnaire was constructed as shown in Table 6. This questionnaire, following the same structure, was reviewed by three computer education experts. Table 6 serves as an example for the sequential structure, with similar questionnaires constructed for the selection and iteration structures.
Table 6. Example of questionnaire items using representative explanations (as shown in Table 3) for perceived helpfulness analysis (sequence concept).

Items	Contents
Concept Explanation	The sequence structure is the most basic structure for writing programs. It executes algorithms or programs in the order they are written or in the direction of the connecting lines. Here, the term “control structure” refers to the format that specifies the flow of execution of an algorithm or program. (Play SW., 2023)
Concept Example	Image 1
Question1
(Multiple choice question: Select one)	Question	Which of the following explanations is more helpful in understanding the sequence structure?
Option1 (Responder unknown, Human Teacher)	When you start, doing number 1 and then number 2 in this order is what is called a sequence.
Option2 (Responder unknown, GPT-3.5)	A sequence means doing tasks one by one, in a specific order.
Option3 (In case of not being helpful)	Neither of them is helpful.
Question2 (Open-ended question)	Question	Why do you think that way?
Answer	Please write freely in any format
3.4.2. Perception of the agent of concept explanations
To analyze perceptions regarding the agent of concept explanations created by human teachers and GPT-3.5 for the experimental group, a questionnaire was constructed as shown in Table 7. This questionnaire, following the same structure, was reviewed by three computer education experts. Table 7 serves as an example for the sequential structure, with similar questionnaires constructed for the selection and iteration structures.
Table 7. Example of questionnaire items using representative explanations (as shown in Table 3) for perceived helpfulness analysis (sequence concept).

Items	Contents
Question1
(Multiple choice question: Select one)	Question	What is the feedback written by a human teacher, not an artificial intelligence, for the following sequence structure explanation?
Option1 (Responder unknown, Human Teacher)	When you start, doing number 1 and then number 2 in this order is what is called a sequence.
Option2 (Responder unknown,
GPT-3.5)	A sequence means doing tasks one by one, in a specific order.
Option3 (In case of not knowing)	I don't know.
Question2 (Open-ended question)	Question	Why do you think that way?
Answer	Please write freely in any format
4. Results
4.1. Results for RQ1: the perceived helpfulness of concept explanations
According to the experimental plan, the items in Table 6 were administered to the experimental groups to collect responses regarding the perceived helpfulness of concept explanations generated by GPT-3.5 versus those created by human teachers. Chi-square tests and goodness-of-fit tests were performed on the collected response data to explore the response characteristics and implications for each group.
4.1.1. Statistical analysis: chi-square test and goodness-of-fit test
Fig. 3 depicts a graphical representation comparing the response ratios between the teacher group and the student group, where (T) indicates the teacher group and (S) indicates the student group.
Fig. 3
Download: Download high-res image (234KB)
Download: Download full-size image
Fig. 3. Perceived Helpfulness Percentages for GPT-3.5-Generated vs. Human Teacher-Created Concept Explanations.

As depicted in Fig. 3, there appears to be a tendency among the teacher group for the perceived helpfulness of GPT-3.5-generated responses versus human teacher-created responses to vary across different topics or subject areas. In contrast, the student group maintains a more consistent ratio in their perceived helpfulness ratings between the two sources of concept explanations.
Based on the results of the Chi-square and Goodness-of-Fit Tests performed on the collected responses, in the teacher group (N = 11), a comparison of preferences for AI-generated explanations (GAI) versus teacher-created explanations for the Sequence concept showed that a significantly higher proportion preferred teacher-created explanations (82%) over AI-generated explanations (18%) (p = .045, φ = .707).
For the Selection and Iteration concepts within the teacher group, no significant differences were found in preference between AI-generated and teacher-created explanations.
In the student group (N = 70), there were no significant differences in preference between AI-generated and teacher-created explanations across all concepts (Sequence, Selection, Iteration).
Similarly, in the overall group (N = 81), no significant differences in preference were observed between AI-generated and teacher-created explanations for any of the concepts. The results are shown in Table 8.
Table 8. Chi-square and goodness-of-fit tests: Comparing perceived helpfulness of GPT-3.5-generated and human teacher-created concept explanations.

Group	Concept	GPT-3.5 (%)	Human Teacher (%)	Chi-Square Test	Goodness-of-Fit Test
DF	p	
DF	P	
Teacher (N = 11)	Sequence	2(18)	9(82)	4.019	1	.045*	.707	10.062	2	.007*	.595
Selection	8(72)	3(28)	.000	1	1.000	.000				
Iteration	4(36)	6(54)	1.616	1	.204	.447				
Student (N = 70)	Sequence	31(44)	37(52)		1	.743	.039	.134	2	.935	.032
Selection	31(44)	36(51)		1	1.000	.000				
Iteration	31(44)	36(51)		1	.820	.027				
Total (N = 81)	Sequence	33(30)	46(36)		6	.790	.124				
Selection	39(36)	39(30)								
Iteration	35(34)	42(34)								
Note. * Indicates p < .05. 
 indicate Effect Size.
The teacher group exhibited a strong preference for teacher-created explanations of the Sequence concept (φ = .707), suggesting that teachers may perceive these explanations as more effective. In contrast, the student group showed no preference between AI-generated and teacher-created explanations, indicating that students perceive concept explanations similarly, regardless of the source.
The absence of a significant difference in the overall group could be due to the opposing preferences of teachers and students neutralizing each other. The different preferences between the teachers and students suggest that the two groups may use different criteria for evaluating concept explanations. This discrepancy implies that separate analyses for teachers and students might be necessary when investigating the effectiveness of AI-generated and teacher-created explanations.
In particular, the marked preference difference for the Sequence concept within the teacher group suggests that further research is needed to enhance the quality of AI-generated explanations for this concept.
4.1.2. Qualitative analysis: thematic analysis
For an in-depth analysis of the perceived helpfulness of concept explanations, responses to the open-ended Question 2 in Table 6 were collected and subjected to thematic analysis. The collected data comprised 22 meaningful sentences, totaling approximately 200 words, from teacher responses, and 35 meaningful sentences, totaling approximately 350 words, from student responses. The thematic analysis results are presented in Table 9.
●
Teacher Response Analysis: Teachers demonstrated an awareness of the differences between GPT-3.5-generated and human teacher-created explanations, exhibiting a tendency to prefer human teacher-created explanations (codes 1.3, 2.1). They based their judgments of explanation helpfulness on effective teaching methods and considering student levels and needs (codes 3.1, 3.2, 3.3). Positive attitudes coexisted with concerns regarding the utilization of GPT-3.5 technology (codes 4.1, 4.2).
●
Student Response Analysis: Preferences for GPT-3.5-generated and human teacher-created explanations varied among students, depending on individual experiences, and learning levels (codes 2.2, 2.3). Students valued the comprehensibility and usefulness of explanations, preferring those that aided learning and enhanced conceptual understanding (codes 5.3, 5.4, 7.1). They considered opportunities for questioning, feedback, and interaction as indicators of explanation helpfulness (codes 6.3, 6.4). Furthermore, students expressed a preference for diverse explanation methods and the utilization of visual aids (codes 8.1, 8.2).
●
Implications: While teachers clearly recognized the differences between GPT-3.5-generated and human teacher-created explanations and preferred the latter, students exhibited greater individual differences in their preferences. Teachers focused on optimizing teaching methods, whereas students emphasized the comprehensibility, usefulness, and interaction aspects of explanations. Both teachers and students simultaneously exhibited positive perceptions and concerns regarding the utilization of GPT-3.5 technology. Students demonstrated a stronger preference for diverse explanation methods and the use of visual aids.
Table 9. Result of thematic analysis: Teacher-student perception comparison on the perceived helpfulness of GPT-3.5-generated and human teacher-created concept explanations.

Code	Category	Keywords (Frequency)
Teachers	Students
1	Comparison of GPT-3.5-generated and human teacher-created explanations	
1.1.
AI-generated explanations:
1.1.1.
Consistency (3)
1.1.2.
Systematic (2)
1.2.
Human teacher-created explanations:
1.2.1.
Flexibility (4)
1.2.2.
Contextual relevance (3)
1.2.3.
Personalization (2)
1.2.4.
Meeting learners' needs (3)
1.3.
Perception of differences (6)
1.4.
Perception of similarities (4)
2	Explanation preference	
2.1.
Preference for human teacher-created explanations (5)
2.2.
Individual preference differences (5)
2.3.
Situation-dependent preference (4)
3	Teaching methods	
3.1.
Consideration of student levels (4)
3.2.
Reflection of student needs (3)
3.3.
Optimization of explanation methods (2)
None
4	Utilization of AI technology	
4.1.
Positive attitude (2)
4.2.
Concerns and doubts (3)
4.3.
Positive perception (5)
4.4.
Negative perception (2)
5	Usefulness of explanations	
5.1.
Generally useful (6)
5.2.
Differences depending on concept types (2)
5.3.
Helpful for learning (9)
5.4.
Improvement in concept understanding (7)
6	Importance of interaction	
6.1.
Recognition of importance (4)
6.2.
Strengths of human teachers (3)
6.3.
Need for questioning opportunities (7)
6.4.
Need for feedback (6)
7	Comprehensibility of explanations	None	
7.1.
Easy to understand (8)
7.2.
Need for additional explanations (3)
8	Diversity of explanation methods	None	
8.1.
Need for various methods (6)
8.2.
Utilization of visual materials (4)
Note. The codebook was constructed on frequency, organizing categories.
4.2. Results for RQ2: perception of the agent of concept explanations
The items in Table 7 evaluating the perception of the agent of concept explanations were administered to the experimental groups, and responses were collected. Chi-square tests and goodness-of-fit tests were performed on the collected response data to explore the response characteristics and implications for each group. Fig. 4 depicts a graphical representation comparing the response ratios between the teacher group and the student group, where (T) indicates the teacher group and (S) indicates the student group.
Fig. 4
Download: Download high-res image (300KB)
Download: Download full-size image
Fig. 4. Perception of the agent of concept explanations Percentages for GPT-3.5-Generated vs. Human Teacher-created Concept Explanations

Note. ‘Correctly Identify Source' indicates cases where participants accurately identified whether the explanation was created by a human teacher or generated by GPT-3.5. 'Incorrectly Identify Source' represents instances where participants were unable to correctly determine the origin of the explanation.
4.2.1. Statistical analysis: chi-square test and goodness-of-fit test
As illustrated in Fig. 4, the teacher group generally appears better able to differentiate the source of the explanations (GPT-3.5 versus human) compared to the student group. However, both groups seem to encounter difficulties in distinguishing the source for the concept regarding Iteration.
Based on the results of the Chi-square and Goodness-of-Fit Tests performed on the collected responses, in the teacher group (N = 11), there were no statistically significant differences in discriminating or not discriminating between GPT-3.5 and human teachers across all concepts: Sequence, Selection, and Iteration.
In the student group (N = 70), no statistically significant differences were found for the Sequence and Selection concepts.
However, for the Iteration concept, the proportion not discriminating was significantly higher than the proportion successfully discriminating (p < .01), with a medium effect size (φ = .348). In the overall group (N = 81), there were no statistically significant differences across all concepts.
The goodness-of-fit test results indicated significant differences in the student group across concepts (p < .001), with a large effect size (
 = 0.399) as in Table 10.
Table 10. Results of responses distinguishing between GPT-3.5 and human teachers in concept explanations.

Group	Concept	Correctly Identify Source (%)	Incorrectly Identify Source (%)	Chi-Square Test	Goodness-of-Fit Test
DF	p	
DF	P	
Teacher (N = 11)	Sequence	6(54)	5(46)	.068	1	.794	.091	.818	2	.976	.174
Selection	6(54)	5(46)	.000	1	1.000	.000				
Iteration	4(36)	7(64)	.199	1	.655	.156				
Student (N = 70)	Sequence	35(50)	35(50)	.046	1	.977	.026	22.127	2	<.001*	.399
Selection	38(54)	32(46)	.046	1	.977	.026				
Iteration	23(32)	57(68) *	8.450	1	.004*	.348				
Total (N = 81)	Sequence	41(36)	40(28)	3.938	6	.685	.138				
Selection	44(39)	37(26)								
Iteration	27(25)	64(45)								
Note. * indicates p < .05, 
 indicate Effect Size. ‘Correctly Identify Source' indicates cases where participants accurately identified whether the explanation was created by a human teacher or generated by GPT-3.5. 'Incorrectly Identify Source' represents instances where participants were unable to correctly determine the origin of the explanation.
The lack of significant differences in discriminating or failing to discriminate between GPT-3.5 and human teachers across all concepts in the teacher group (N = 11) may suggest limitations in generalizing the results due to the small sample size.
The significantly higher proportion of failing to discriminate between GPT-3.5 and human teachers for the Iteration concept in the student group could indicate that students may be experiencing difficulties in understanding this concept. This implies that improvements in the educational methods or explanation approaches for the Iteration concept may be warranted.
The absence of significant differences in discriminating or failing to discriminate between GPT-3.5 and human teachers in the overall group (N = 81) could be attributed to the contrasting results from the teacher and student groups canceling each other out, making it challenging to discern the characteristics of individual groups from the overall group results alone.
The goodness-of-fit test results in the student group indicate differences in the proportions of discriminating or failing to discriminate between GPT-3.5 and human teachers across concepts, suggesting that students may experience greater difficulties in understanding specific concepts (e.g., Iteration).
The observed differences between the teacher and student groups in discriminating or failing to discriminate between GPT-3.5 and human teachers imply the need for tailored strategies that align with the characteristics of each group. Particularly for concepts where students face difficulties, additional explanations or alternative approaches could be beneficial.
Considering the effect sizes, the high proportion of failing to discriminate between GPT-3.5 and human teachers for the Iteration concept in the student group, as well as the differences in proportions across concepts, are likely to be meaningful results. This underscores the need for special considerations for this concept. For instance, when developing educational content utilizing language models like GPT-3.5, optimizing the explanation approaches by considering learner characteristics becomes crucial. To achieve this, a comprehensive consideration of the differences between teachers and students, as well as the unique features of each concept, is necessary.
4.2.2. Qualitative analysis: thematic analysis
For an in-depth analysis of the perception of the agent of concept explanations, responses to the open-ended question were collected and subjected to thematic analysis. The collected data comprised 15 meaningful sentences, totaling approximately 120 words, from teacher responses, and 42 meaningful sentences, totaling approximately 400 words, from student responses. The thematic analysis results are presented in Table 11.
●
Teacher Response Analysis: Some teachers can distinguish between GPT-3.5-generated and human teacher-created explanations, while others find it challenging to differentiate (Codes 1.1, 1.2). Teachers cite the consistency, systematicity, flexibility, and contextual relevance as criteria for distinguishing between GPT-3.5-generated and human teacher-created explanations (Codes 2.1, 2.2, 2.3, 2.4). Teachers perceive the consistency and similar sentence structures as characteristics of GPT-3.5-generated explanations, while recognizing the reflection of personal styles and context-dependent variations as features of human teacher-created explanations (Codes 3.1, 3.2, 4.1, 4.2).
●
Student Response Analysis: Many students encounter difficulties in distinguishing between GPT-3.5-generated and human teacher-created explanations (Codes 1.3, 1.4). Students mention sentence structure, expression style, and depth of content as criteria for differentiation (Codes 2.5, 2.6, 2.7). Students perceive the consistency, similar sentence structures, and formal tone as characteristics of GPT-3.5-generated explanations, while recognizing the reflection of personal traits, familiar tones, and in-depth explanations as features of human teacher-created explanations (Codes 3.3, 3.4, 3.5, 4.3, 4.4, 4.5). Numerous students find it challenging to distinguish due to similar content, with some citing limited information and individual differences as factors hindering differentiation (Codes 5.3, 5.4, 5.5). While student preferences vary, a larger proportion prefers human teacher-created explanations (Codes 6.1, 6.2, 6.3).
●
Implications: Both teachers and students experience difficulties in distinguishing between GPT-3.5-generated and human teacher-created explanations, with students facing greater challenges. Both teachers and students mention consistency, sentence structure, and expression style as criteria for differentiation, but students tend to place more emphasis on expression style. Both teachers and students recognize the consistency and formality of GPT-3.5-generated explanations, and the personal traits and in-depth nature of human teacher-created explanations. Students appear to experience greater difficulties in distinguishing due to the similarity of content compared to teachers. While student preferences vary, there is a higher preference for human teacher-created explanations.
Table 11. Result of Thematic Analysis: Teacher-student perception comparison on the agent of GPT-3.5-generated and human teacher-created concept explanations.

Code	Category	Keywords (Frequency)
Teachers	Students
1	Distinction of Explanation Generation Subjects	
1.1.
Distinguishable (3)
1.2.
Difficult to distinguish (2)
1.3.
Distinguishable (4)
1.4.
Difficult to distinguish (6)
2	Distinction Criteria	
2.1.
Consistency (1)
2.2.
Systematicity (1)
2.3.
Flexibility (2)
2.4.
Contextual relevance (1)
2.5.
Sentence structure (2)
2.6.
Expression style (3)
2.7.
Depth of content (1)
3	Characteristics of AI-generated Explanations	
3.1.
Consistent (1)
3.2.
Similar sentence structure (1)
3.3.
Consistency (2)
3.4.
Similar sentence structure (1)
3.5.
Formal feeling (1)
4	Characteristics of Human Teacher-created Explanations	
4.1.
Reflecting personal style (2)
4.2.
Varying according to context (1)
4.3.
Reflecting personal characteristics (2)
4.4.
Friendly tone (1)
4.5.
In-depth explanation (1)
5	Reasons for Difficulty in Distinction	
5.1.
Similar quality (1)
5.2.
Limited information (1)
5.3.
Similar content (3)
5.4.
Limited information (2)
5.5.
Individual differences (1)
6	Explanation Preference	None	
6.1.
Preference for AI-generated explanations (1)
6.2.
Preference for human teacher-created explanations (3)
6.3.
Varies by situation (2)
Note. The codebook was constructed on frequency, organizing categories.
4.3. Relationship between perceived helpfulness and source attribution
To gain deeper insights into our findings, we conducted an additional analysis to examine the relationship between the perceived helpfulness of concept explanations and the ability to accurately identify their source. This analysis aims to determine whether participants' perceptions of an explanation's usefulness correlate with their ability to distinguish between AI-generated and human-created content. We utilized data collected from our experimental groups, which consisted of 11 teachers and 70 sixth-grade students, as outlined in Sections 3.2.1 Experimental group: teacher, 3.2.2 Experimental group: student. The perceived helpfulness scores were derived from responses to the questionnaire detailed in Table 6, while source identification accuracy was determined based on responses to the questionnaire in Table 7. Following the method described in Section 3.4, we performed a correlation analysis using Pearson's r to quantify the relationship between these two variables for each concept (sequence, selection, and iteration) across both teacher and student groups. The results of this analysis are presented in Table 12.
●
Teacher Group: Moderate to strong positive correlations were observed across all concepts. The strongest correlation was found for the Iteration concept (r = .72, p = .013), followed by Sequence (r = .68, p = .021). The correlation for Selection, while moderate (r = .55), was not statistically significant (p = .080). Effect sizes were substantial, ranging from .30 to .52, indicating that 30–52% of the variance in perceived helpfulness could be explained by source attribution accuracy.
●
Student Group: Weak positive correlations were observed across all concepts. Only the Iteration concept showed a statistically significant correlation (r = .31, p = .009). Effect sizes were small, ranging from .04 to .10, suggesting that only 4–10% of the variance in perceived helpfulness could be explained by source attribution accuracy.
Table 12. Correlation between perceived helpfulness and correct source attribution.

Group	Concept	Correction Coefficient (r)	p	Effect Size (r 2)
Teachers (N = 11)	Sequence	.68	.021*	.46
Selection	.55	.080	.30
Iteration	.72	.013*	.52
Students (N = 70)	Sequence	.23	.055	.05
Selection	.19	.114	.04
Iteration	.31	.009*	.10
Note. * indicates p < .05, Correlation coefficient represents Pearson's r. 95% CI = 95% Confidence Interval. Effect Size (r2) represents the proportion of variance shared by the two variables.
According to Fig. 5, these findings suggest that for teachers, there is a stronger relationship between their ability to correctly identify the source of an explanation and their perception of its helpfulness. This relationship is particularly pronounced for the Iteration concept, as indicated by the darker shades in the teacher row of the heatmap. In contrast, students show a much weaker relationship between these variables, as evidenced by the lighter shades in the student row, indicating that their perception of helpfulness is less tied to their ability to identify the source.
Fig. 5
Download: Download high-res image (190KB)
Download: Download full-size image
Fig. 5. Heatmap of correlations between perceived helpfulness and source attribution accuracy for AI-Generated and human-created concept explanations.

The stronger correlations observed in the teacher group, represented by the more intense colors in the upper half of the heatmap, may reflect their pedagogical expertise and critical evaluation skills. Teachers who can accurately distinguish between AI-generated and human-created explanations may be better equipped to assess the educational value of these explanations. Conversely, the weaker correlations in the student group, shown by the paler colors in the lower half of the heatmap, suggest that their perceptions of helpfulness are influenced by factors other than their ability to identify the source.
The consistently stronger correlation for the Iteration concept across both groups is noteworthy, as evidenced by the darkest shades in both the teacher and student rows for this concept. This may indicate that for more complex concepts, the ability to identify the source of an explanation becomes more closely tied to perceptions of its usefulness.
These results, have important implications for the integration of AI-generated content in educational settings, highlighting the need for tailored approaches for teachers and students, and suggesting that more attention may be needed when using AI-generated explanations for complex concepts.
4.4. Results for RQ3: considerations for AI integration in education
This section presents the findings related to the factors that should be considered when utilizing AI in educational contexts, based on the perceptions of teachers and students.
4.4.1. Teachers' perspectives
Teachers emphasized the importance of considering student levels and needs when integrating AI-generated explanations into education. They highlighted the following key points.
●
Optimization of explanation methods: Teachers stressed the need to adapt AI-generated content to suit various teaching styles and pedagogical approaches.
●
Reflection of student needs: The importance of ensuring that AI-generated explanations are tailored to meet specific learning requirements of students was emphasized.
●
Pedagogical effectiveness: Teachers evaluated the usefulness of explanations based on their potential to enhance student understanding and learning outcomes.
4.4.2. Students' perspectives
Students focused on the practical aspects of AI-generated explanations and their impact on the learning experience. Key considerations from their viewpoint include.
●
Comprehensibility: Students valued explanations that were easy to understand, highlighting the need for AI to generate clear and accessible content.
●
Interactivity: The importance of opportunities for questioning and receiving feedback was emphasized, suggesting that AI integration should support interactive learning experiences.
●
Diversity of explanation methods: Students expressed a preference for varied approaches to explanation, including the use of visual aids, indicating that AI should be capable of providing diverse forms of content.
4.4.3. Common considerations
Both teachers and students identified several shared factors for consideration.
●
Educational value: Both groups emphasized the importance of AI-generated content being genuinely helpful for learning and improving concept understanding.
●
Complementary role: There was a recognition that AI should complement, rather than replace, human teacher interactions, highlighting the continued importance of human elements in education.
●
Adaptability: The need for AI-generated explanations to be adaptable to different learning styles, levels, and contexts was a common theme.
These findings suggest that successful integration of AI in education requires a balanced approach that leverages the strengths of AI while maintaining the crucial role of human teachers in the learning process.
5. Discussion
This study explored the potential of GAI's concept explanations in education, focusing on the educational effectiveness of feedback and perceptions of the feedback's subject. The results reveal significant insights into how teachers and students perceive and interact with GAI-generated content in educational contexts.
5.1. Differences in teacher and student perceptions
Our findings indicate a notable difference in how teachers and students perceive educational feedback generated by GAI. This aligns with the results of Smolansky et al. (2023), who also found disparities in perceptions between educators and students regarding AI in education. In our study, students showed a preference for teacher-created explanations across all concepts (sequence, selection, and iteration), while teachers found GAI-generated explanations more helpful for sequence and selection concepts.
This preference among students for human teacher feedback can be explained through the lens of Keller's (1987) ARCS theory of motivation. The 'Relevance' component of ARCS suggests that learners are more motivated when they can connect the learning material to their personal experiences or relationships. In this context, students may find teacher-created explanations more relatable and contextually relevant, as they come from a familiar source with whom students have an established relationship.
The preference of teachers for GAI-generated explanations in certain concepts (sequence and selection) is an interesting finding that warrants further investigation. It may suggest that GAI is particularly effective in providing clear, structured explanations for these more straightforward concepts. However, for the more complex concept of iteration, teachers preferred human-created explanations, indicating that GAI may still have limitations in explaining more intricate ideas.
5.2. Implications for GAI integration in education
The differential perceptions between teachers and students have significant implications for the integration of GAI in educational settings. While GAI shows promise in generating helpful explanations, especially for certain concepts, its integration should be carefully considered and balanced with traditional teaching methods.
The importance of prompt engineering in GAI applications becomes evident in this context. As highlighted by White et al. (2023), Liu and Chilton (2022), and Zhou (2022), the design of prompts is crucial in generating effective GAI responses. Our findings underscore the need for educators to develop skills in prompt engineering to effectively leverage GAI in their teaching practices. This suggests that teacher training programs should incorporate elements of prompt engineering and AI literacy to prepare educators for the increasing presence of GAI in education.
5.3. Recognition of AI-generated content
An important finding from our study is that both teachers and students relied on human-like elements to judge the source of explanations. These elements included speech patterns, concrete examples, approachable tone, and even slight awkwardness in expression. However, students generally had more difficulty in accurately identifying AI-generated explanations compared to teachers. This finding is consistent with the results reported by Waltzer, Cox, and Heyman (2023) in their study involving high school students.
The difficulty students face in recognizing AI-generated content raises important considerations for the ethical use of GAI in education. It suggests a need for transparency in the use of AI tools, where students are explicitly informed when they are interacting with AI-generated content. This transparency is crucial not only for ethical reasons but also for developing students' critical thinking skills in an increasingly AI-integrated world.
5.4. Pedagogical considerations
Our findings highlight the need to ground the use of GAI in established educational theories and practices. While GAI can provide efficient and sometimes highly effective explanations, it should complement rather than replace human teaching. The preference of students for teacher-created explanations underscores the importance of the human element in education, including the ability to provide context, relate to student experiences, and adapt explanations in real-time based on student responses.
5.5. Limitations and future research directions
While this study provides valuable insights, it has limitations that future research should address. The sample size, particularly for teachers, was relatively small, which may limit the generalizability of the findings. Future studies should consider larger and more diverse samples to validate and extend these results.
Additionally, this study focused on perceptions rather than measuring actual learning outcomes. Future research could investigate how GAI-generated explanations impact student performance and long-term understanding compared to traditional teaching methods.
Moreover, the study was limited to specific programming concepts (sequence, selection, and iteration). Expanding this research to other subject areas and more complex topics could provide a more comprehensive understanding of GAI's potential in education.
In conclusion, while GAI shows promise in providing effective concept explanations in certain areas, its integration into education requires careful consideration of both teacher and student perspectives, grounding in educational theory, and a clear focus on ethical and transparent implementation. As GAI continues to evolve, ongoing research will be crucial to understand its optimal role in enhancing teaching and learning processes.
6. Limitations and future research
While the present study offers valuable insights into the perceived efficacy of GAI-generated concept explanations in computer science education, several limitations must be acknowledged. First, the relatively small sample size, particularly for the teacher group (N = 11), poses challenges in generalizing the findings to broader populations. Larger-scale studies involving more diverse participant groups are warranted to enhance the external validity and generalizability of the results.
Moreover, it is crucial to recognize that this study focused on exploring perceptions of helpfulness and the ability to discern explanation sources, rather than directly assessing the impact of GAI-generated explanations on actual learning outcomes. Subsequent experimental studies are recommended to empirically evaluate the effectiveness of GAI-generated instructional content in facilitating knowledge acquisition and concept mastery.
Furthermore, the current investigation centered solely on ChatGPT as the GAI model for generating concept explanations. Given the rapid advancements in the field of generative AI, future studies should expand their scope to encompass a diverse range of state-of-the-art GAI models, enabling a comprehensive understanding of their respective strengths and limitations in educational applications.
Additionally, future research should consider exploring the impact of contextual factors such as prior AI exposure and cultural attitudes towards AI in education. Longitudinal studies could also offer insights into how perceptions and effectiveness of GAI in education evolve over time. Finally, investigating optimal prompt engineering techniques specifically for educational contexts could further enhance the practical application of GAI in teaching and learning environments.
7. Conclusion
This study explored the implications of integrating generative AI (GAI) into education by examining the perceived usefulness and ability to distinguish the source (human or AI) of explanations for key computer science concepts (sequence, selection, iteration) generated by GAI and created by human teachers. The findings revealed nuanced differences in perceptions between teachers and students, with teachers finding GPT-3.5's explanations more helpful for sequence and selection concepts, while preferring human-created explanations for the iteration concept. Students, however, showed a slight preference for human teacher-created explanations across all concepts.
These differing perceptions underscore the importance of considering diverse educational perspectives when evaluating GAI-generated content. Teachers' assessments were influenced by their pedagogical expertise, emphasizing clarity and prevention of misconceptions. Conversely, students valued detailed, relatable, and experientially relevant explanations. The study also found a correlation between perceived helpfulness and the ability to identify the source of explanations, particularly among teachers.
The difficulty students experienced in distinguishing between human-created and GPT-3.5-generated explanations highlights the need for transparency when integrating GAI into educational settings. It also emphasizes the importance of developing AI literacy among students to critically evaluate AI-generated content.
While confirming GAI's potential to support education, this study highlights the need to tailor GAI-based applications to the diverse needs of teachers and learners. It advocates for a balanced approach that enhances understanding of GAI's role in education, promotes learning, and acknowledges the essential contributions of human teachers. Future research should explore this further through larger-scale studies, investigating the impact on actual learning outcomes and examining the effectiveness of various GAI models across different educational contexts.


